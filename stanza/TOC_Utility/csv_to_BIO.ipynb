{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in synthetic data\n",
    "df = pd.read_csv('../../data/data/generated/data_230124-172021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>organization</th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>unit</th>\n",
       "      <th>weight1</th>\n",
       "      <th>item1</th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "      <th>weight2</th>\n",
       "      <th>item2</th>\n",
       "      <th>text_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams Rocks</td>\n",
       "      <td>Take 3</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>kilograms</td>\n",
       "      <td>200</td>\n",
       "      <td>spread tubs</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>It was inspiring to witness so many people com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>[It, was, inspiring, to, witness, so, many, pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grahams Beach</td>\n",
       "      <td>Global Alliance Against Marine Pollution</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2018-09-04</td>\n",
       "      <td>pounds</td>\n",
       "      <td>100</td>\n",
       "      <td>trash</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>We just made a huge difference at Grahams Beac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>[We, just, made, a, huge, difference, at, Grah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Norfolk Island</td>\n",
       "      <td>Plastic Pollution Coalition Australia</td>\n",
       "      <td>press release</td>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>kilograms</td>\n",
       "      <td>386</td>\n",
       "      <td>glass bottles</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>\\nPlastic Pollution Coalition Australia (PPCA)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Plastic, Pollution, Coalition, Australia, (, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Playa Grande de Saboga</td>\n",
       "      <td>Take 3</td>\n",
       "      <td>press release</td>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>kgs</td>\n",
       "      <td>332</td>\n",
       "      <td>tupperwares</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>Take 3 Celebrates a Successful Beach Cleanup i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Take, 3, Celebrates, a, Successful, Beach, Cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Playa de Chachalacas</td>\n",
       "      <td>Rameau Project</td>\n",
       "      <td>press release</td>\n",
       "      <td>2022-04-15</td>\n",
       "      <td>lbs</td>\n",
       "      <td>168</td>\n",
       "      <td>plastic</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE\\n\\nThe Rameau Project Ce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>[FOR, IMMEDIATE, RELEASE, The, Rameau, Project...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 location                              organization  \\\n",
       "0             Adams Rocks                                    Take 3   \n",
       "1           Grahams Beach  Global Alliance Against Marine Pollution   \n",
       "2          Norfolk Island     Plastic Pollution Coalition Australia   \n",
       "3  Playa Grande de Saboga                                    Take 3   \n",
       "4    Playa de Chachalacas                            Rameau Project   \n",
       "\n",
       "                type        date       unit  weight1          item1  \\\n",
       "0  instagram caption  2017-04-07  kilograms      200    spread tubs   \n",
       "1  instagram caption  2018-09-04     pounds      100          trash   \n",
       "2      press release  2021-05-25  kilograms      386  glass bottles   \n",
       "3      press release  2016-12-30        kgs      332    tupperwares   \n",
       "4      press release  2022-04-15        lbs      168        plastic   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Generate an instagram caption for a beach clea...   \n",
       "1  Generate an instagram caption for a beach clea...   \n",
       "2  Generate a press release for a beach cleanup w...   \n",
       "3  Generate a press release for a beach cleanup w...   \n",
       "4  Generate a press release for a beach cleanup w...   \n",
       "\n",
       "                                                text  weight2 item2  \\\n",
       "0  It was inspiring to witness so many people com...      NaN   nan   \n",
       "1  We just made a huge difference at Grahams Beac...      NaN   nan   \n",
       "2  \\nPlastic Pollution Coalition Australia (PPCA)...      NaN   nan   \n",
       "3  Take 3 Celebrates a Successful Beach Cleanup i...      NaN   nan   \n",
       "4  FOR IMMEDIATE RELEASE\\n\\nThe Rameau Project Ce...      NaN   nan   \n",
       "\n",
       "                                          text_split  \n",
       "0  [It, was, inspiring, to, witness, so, many, pe...  \n",
       "1  [We, just, made, a, huge, difference, at, Grah...  \n",
       "2  [Plastic, Pollution, Coalition, Australia, (, ...  \n",
       "3  [Take, 3, Celebrates, a, Successful, Beach, Cl...  \n",
       "4  [FOR, IMMEDIATE, RELEASE, The, Rameau, Project...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_split'] = df['text'].apply(lambda x: x.strip())\n",
    "df['text_split'] = df['text_split'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "string_cols = [\"item1\", \"item2\", ]\n",
    "df[string_cols] = df[string_cols].astype(str)\n",
    "df.head()\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into list of words with associated 'B - entity', 'I - entity' or 'O'\n",
    "# Look at other preprocessing steps in read_datasets function in convert_bn_daffodil\n",
    "\n",
    "units = [\"kilograms\", \"kilogram\", \"kgs\", \"kg\", \"lb\", \"lbs\", \"pounds\", \"pound\"]\n",
    "\n",
    "def assign_entity_types(row):\n",
    "    sentence = row['text_split']\n",
    "    new_tags = []\n",
    "    prev_org_tag = False\n",
    "    prev_loc_tag = False\n",
    "    prev_unit_tag = False\n",
    "    prev_weight_tag = False\n",
    "    prev_item_tag = False\n",
    "    for i in range(len(sentence)):\n",
    "        # Assign location labels\n",
    "        if any(sentence[i] == word for word in row['location'].split()): #Problem: only searches for specific location that was \n",
    "                                        # given in the prompt, not other locations that GPT-3 produces in text\n",
    "                                        # Solution: implement another check for all international countries?\n",
    "            if prev_loc_tag:\n",
    "                new_tags.append(\"I-LOC\")\n",
    "            else:\n",
    "                new_tags.append(\"B-LOC\")\n",
    "                prev_loc_tag = True\n",
    "        # Assign organization labels\n",
    "        elif any(sentence[i] == word for word in row['organization'].split()):\n",
    "            if prev_org_tag:                    # consider capitalization - should i lowercase everything when checking orgs?\n",
    "                new_tags.append(\"I-ORG\")\n",
    "            else:\n",
    "                new_tags.append(\"B-ORG\")\n",
    "                prev_org_tag = True\n",
    "        # Assign unit labels\n",
    "        elif any(sentence[i] == word for word in units):\n",
    "            if prev_unit_tag: \n",
    "                new_tags.append(\"I-UNT\")\n",
    "            else:\n",
    "                new_tags.append(\"B-UNT\")\n",
    "                prev_unit_tag = True\n",
    "        # Assign weight labels \n",
    "        elif (sentence[i] == str(row['weight1']) or (row['weight2'] != None and sentence[i] == str(row['weight2']))):\n",
    "            if prev_weight_tag: \n",
    "                new_tags.append(\"I-WEI\")\n",
    "            else:\n",
    "                new_tags.append(\"B-WEI\")\n",
    "                prev_weight_tag = True\n",
    "        # Assign item labels\n",
    "        elif (any(sentence[i] == word for word in row['item1'].split()) or \n",
    "                                (row['item2'] != None and any(sentence[i] == word for word in row['item2'].split()))):\n",
    "            if prev_item_tag: \n",
    "                new_tags.append(\"I-ITM\")\n",
    "            else:\n",
    "                new_tags.append(\"B-ITM\")\n",
    "                prev_item_tag = True\n",
    "        # Open question: How to assign dates? Need to capture all possible date formats?\n",
    "        else:\n",
    "            new_tags.append(\"O\")\n",
    "            prev_org_tag = False\n",
    "            prev_loc_tag = False\n",
    "            prev_unit_tag = False\n",
    "            prev_weight_tag = False\n",
    "            prev_item_tag = False\n",
    "    return list(zip(sentence, new_tags))\n",
    "\n",
    "df['tagged_entities'] = df.apply(lambda row : assign_entity_types(row), axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plastic B-ORG\n",
      "Pollution I-ORG\n",
      "Coalition I-ORG\n",
      "Australia I-ORG\n",
      "Norfolk B-LOC\n",
      "Island I-LOC\n",
      "386 B-WEI\n",
      "kilograms B-UNT\n",
      "glass B-ITM\n",
      "bottles I-ITM\n",
      "Australia B-ORG\n",
      "glass B-ITM\n",
      "bottles I-ITM\n",
      "Australia B-ORG\n",
      "[('Plastic', 'B-ORG'), ('Pollution', 'I-ORG'), ('Coalition', 'I-ORG'), ('Australia', 'I-ORG'), ('(', 'O'), ('PPCA', 'O'), (')', 'O'), ('is', 'O'), ('proud', 'O'), ('to', 'O'), ('announce', 'O'), ('the', 'O'), ('success', 'O'), ('of', 'O'), ('its', 'O'), ('latest', 'O'), ('beach', 'O'), ('cleanup', 'O'), ('on', 'O'), ('Norfolk', 'B-LOC'), ('Island', 'I-LOC'), ('.', 'O'), ('On', 'O'), ('May', 'O'), ('25th', 'O'), (',', 'O'), ('2021', 'O'), (',', 'O'), ('PPCA', 'O'), ('and', 'O'), ('its', 'O'), ('volunteers', 'O'), ('collected', 'O'), ('a', 'O'), ('whopping', 'O'), ('386', 'B-WEI'), ('kilograms', 'B-UNT'), ('of', 'O'), ('glass', 'B-ITM'), ('bottles', 'I-ITM'), ('from', 'O'), ('the', 'O'), ('island', 'O'), (\"'s\", 'O'), ('coastline', 'O'), ('.', 'O'), ('This', 'O'), ('is', 'O'), ('a', 'O'), ('remarkable', 'O'), ('achievement', 'O'), ('for', 'O'), ('the', 'O'), ('organization', 'O'), (',', 'O'), ('which', 'O'), ('has', 'O'), ('made', 'O'), ('it', 'O'), ('its', 'O'), ('mission', 'O'), ('to', 'O'), ('rid', 'O'), ('Australia', 'B-ORG'), (\"'s\", 'O'), ('oceans', 'O'), ('and', 'O'), ('beaches', 'O'), ('of', 'O'), ('plastic', 'O'), ('pollution', 'O'), ('.', 'O'), ('``', 'O'), ('It', 'O'), ('is', 'O'), ('extremely', 'O'), ('gratifying', 'O'), ('for', 'O'), ('us', 'O'), ('to', 'O'), ('see', 'O'), ('the', 'O'), ('tangible', 'O'), ('results', 'O'), ('of', 'O'), ('our', 'O'), ('work', 'O'), ('in', 'O'), ('beach', 'O'), ('cleanups', 'O'), ('like', 'O'), ('this', 'O'), ('one', 'O'), (',', 'O'), (\"''\", 'O'), ('said', 'O'), ('PPCA', 'O'), ('CEO', 'O'), ('John', 'O'), ('Smith', 'O'), ('.', 'O'), ('``', 'O'), ('Our', 'O'), ('volunteers', 'O'), ('worked', 'O'), ('hard', 'O'), ('and', 'O'), ('achieved', 'O'), ('a', 'O'), ('great', 'O'), ('result', 'O'), (',', 'O'), ('and', 'O'), ('we', 'O'), ('are', 'O'), ('thrilled', 'O'), ('that', 'O'), ('we', 'O'), ('were', 'O'), ('able', 'O'), ('to', 'O'), ('make', 'O'), ('such', 'O'), ('a', 'O'), ('big', 'O'), ('difference', 'O'), ('in', 'O'), ('the', 'O'), ('local', 'O'), ('environment', 'O'), ('.', 'O'), (\"''\", 'O'), ('The', 'O'), ('glass', 'B-ITM'), ('bottles', 'I-ITM'), ('collected', 'O'), ('during', 'O'), ('the', 'O'), ('cleanup', 'O'), ('will', 'O'), ('be', 'O'), ('recycled', 'O'), ('and', 'O'), ('reused', 'O'), ('.', 'O'), ('This', 'O'), ('not', 'O'), ('only', 'O'), ('helps', 'O'), ('to', 'O'), ('reduce', 'O'), ('the', 'O'), ('amount', 'O'), ('of', 'O'), ('plastic', 'O'), ('waste', 'O'), ('in', 'O'), ('the', 'O'), ('ocean', 'O'), (',', 'O'), ('but', 'O'), ('also', 'O'), ('supports', 'O'), ('the', 'O'), ('local', 'O'), ('economy', 'O'), ('by', 'O'), ('creating', 'O'), ('jobs', 'O'), ('and', 'O'), ('generating', 'O'), ('revenue', 'O'), ('.', 'O'), ('``', 'O'), ('We', 'O'), ('are', 'O'), ('committed', 'O'), ('to', 'O'), ('continuing', 'O'), ('our', 'O'), ('efforts', 'O'), ('to', 'O'), ('make', 'O'), ('Australia', 'B-ORG'), (\"'s\", 'O'), ('beaches', 'O'), ('and', 'O'), ('oceans', 'O'), ('cleaner', 'O'), (',', 'O'), ('healthier', 'O'), (',', 'O'), ('and', 'O'), ('safer', 'O'), ('for', 'O'), ('all', 'O'), (',', 'O'), (\"''\", 'O'), ('said', 'O'), ('Smith', 'O'), ('.', 'O'), ('``', 'O'), ('We', 'O'), ('are', 'O'), ('calling', 'O'), ('on', 'O'), ('all', 'O'), ('Australians', 'O'), ('to', 'O'), ('join', 'O'), ('us', 'O'), ('in', 'O'), ('this', 'O'), ('effort', 'O'), ('and', 'O'), ('help', 'O'), ('us', 'O'), ('make', 'O'), ('a', 'O'), ('real', 'O'), ('difference', 'O'), ('.', 'O'), (\"''\", 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Review newly assigned non-\"O\" tags\n",
    "for i in df.iloc[2]['tagged_entities']:\n",
    "    if i[1] != \"O\":\n",
    "        print(i[0], i[1])\n",
    "\n",
    "print(df['tagged_entities'][2])\n",
    "# df['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into datasets = (train_sentences, dev_sentences, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to JSON file needed by Stanza model\n",
    "# There is a conversion script called several times in prepare_ner_dataset.py which converts IOB format to our internal NER format:\n",
    "# import stanza.utils.datasets.ner.prepare_ner_file as prepare_ner_file\n",
    "\n",
    "# prepare_ner_file.process_dataset(input_iob, output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or call write_dataset function from stanza.utils.datasets.ner.utils (docstring below)?\n",
    "def write_dataset(datasets, output_dir, short_name, suffix=\"bio\"):\n",
    "    \"\"\"\n",
    "    write all three pieces of a dataset to output_dir\n",
    "\n",
    "    datasets should be 3 lists: train, dev, test\n",
    "    each list should be a list of sentences\n",
    "    each sentence is a list of pairs: word, tag\n",
    "\n",
    "    after writing to .bio files, the files will be converted to .json\n",
    "    \"\"\"\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
