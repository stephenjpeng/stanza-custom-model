{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "from stanza.utils.datasets.ner.utils import write_dataset\n",
    "from transform_weight_date import number_to_words, date_to_formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in synthetic data\n",
    "df = pd.read_csv('../../data/data/generated/data_230124-172021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use re to replace any instances of \"####kg\" with \"#### kg\" where #### is any continuous \n",
    "# sequence of numbers and unit is one of those listed below\n",
    "def separate_weight_unit(row):\n",
    "    return re.sub(r'([0-9]+)(kgs|kg|lbs|lb|pounds|kilograms)', r\"\\1 \\2\", row)\n",
    "\n",
    "# Function to remove spaces (e.g. \"Take 3\" -> \"Take3\")\n",
    "def remove_spaces(text):\n",
    "    return text.replace(\" \", \"\")\n",
    "\n",
    "# Function to replace long hyphen ASCII code with short hyphen '-' ASCII code\n",
    "def character_norm(text):\n",
    "    return text.replace(chr(8211), \"-\")\n",
    "\n",
    "# Word tokenizer splits ',' into separate token, so we have this function to do the same\n",
    "def add_comma_token(text):\n",
    "    return text.replace(\",\", \" ,\")\n",
    "\n",
    "# Word tokenizer splits ',' into separate token, so we have this function to do the same for our dates list\n",
    "def add_date_var_comma_token(list):\n",
    "    new_list = []\n",
    "    for i in list:\n",
    "        new_list.append(add_comma_token(i))\n",
    "    return new_list\n",
    "\n",
    "# Gets the first token of each date variation, to allow for faster downstream computation \n",
    "def get_first_token_set(list):\n",
    "    new_set = set()\n",
    "    for i in list:\n",
    "        new_set.add(i.split()[0])\n",
    "    return new_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>organization</th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>unit</th>\n",
       "      <th>weight1</th>\n",
       "      <th>item1</th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "      <th>weight2</th>\n",
       "      <th>item2</th>\n",
       "      <th>text_split</th>\n",
       "      <th>org_no_space</th>\n",
       "      <th>loc_no_space</th>\n",
       "      <th>date_vars</th>\n",
       "      <th>weight1_text</th>\n",
       "      <th>weight2_text</th>\n",
       "      <th>date_vars_first_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adams rocks</td>\n",
       "      <td>take 3</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>kilograms</td>\n",
       "      <td>200</td>\n",
       "      <td>spread tubs</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>It was inspiring to witness so many people com...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[It, was, inspiring, to, witness, so, many, pe...</td>\n",
       "      <td>take3</td>\n",
       "      <td>adamsrocks</td>\n",
       "      <td>[2017-04-07, April 7 , 2017, april 7 , 2017, A...</td>\n",
       "      <td>two hundred</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "      <td>{April, 04/07/2017, 04/07/17, 7th, 7, 2017-04-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grahams beach</td>\n",
       "      <td>global alliance against marine pollution</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2018-09-04</td>\n",
       "      <td>pounds</td>\n",
       "      <td>100</td>\n",
       "      <td>trash</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>We just made a huge difference at Grahams Beac...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[We, just, made, a, huge, difference, at, Grah...</td>\n",
       "      <td>globalallianceagainstmarinepollution</td>\n",
       "      <td>grahamsbeach</td>\n",
       "      <td>[2018-09-04, September 4 , 2018, september 4 ,...</td>\n",
       "      <td>one hundred</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "      <td>{2018-09-04, 09/04/2018, Sep, 4th, sep, 4, Sep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>norfolk island</td>\n",
       "      <td>plastic pollution coalition australia</td>\n",
       "      <td>press release</td>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>kilograms</td>\n",
       "      <td>386</td>\n",
       "      <td>glass bottles</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>Plastic Pollution Coalition Australia (PPCA) i...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Plastic, Pollution, Coalition, Australia, (, ...</td>\n",
       "      <td>plasticpollutioncoalitionaustralia</td>\n",
       "      <td>norfolkisland</td>\n",
       "      <td>[2021-05-25, May 25 , 2021, may 25 , 2021, May...</td>\n",
       "      <td>three hundred and eighty-six</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "      <td>{05-25-2021, 25th, 25, 2021-05-25, may, May, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>playa grande de saboga</td>\n",
       "      <td>take 3</td>\n",
       "      <td>press release</td>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>kgs</td>\n",
       "      <td>332</td>\n",
       "      <td>tupperwares</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>Take 3 Celebrates a Successful Beach Cleanup i...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Take, 3, Celebrates, a, Successful, Beach, Cl...</td>\n",
       "      <td>take3</td>\n",
       "      <td>playagrandedesaboga</td>\n",
       "      <td>[2016-12-30, December 30 , 2016, december 30 ,...</td>\n",
       "      <td>three hundred and thirty-two</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "      <td>{30, 12/30/16, 12/30/2016, 12-30-2016, decembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>playa de chachalacas</td>\n",
       "      <td>rameau project</td>\n",
       "      <td>press release</td>\n",
       "      <td>2022-04-15</td>\n",
       "      <td>lbs</td>\n",
       "      <td>168</td>\n",
       "      <td>plastic</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE\\n\\nThe Rameau Project Ce...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[FOR, IMMEDIATE, RELEASE, The, Rameau, Project...</td>\n",
       "      <td>rameauproject</td>\n",
       "      <td>playadechachalacas</td>\n",
       "      <td>[2022-04-15, April 15 , 2022, april 15 , 2022,...</td>\n",
       "      <td>one hundred and sixty-eight</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "      <td>{April, 04/15/2022, 04-15-2022, 04/15/22, 15, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 location                              organization  \\\n",
       "0             adams rocks                                    take 3   \n",
       "1           grahams beach  global alliance against marine pollution   \n",
       "2          norfolk island     plastic pollution coalition australia   \n",
       "3  playa grande de saboga                                    take 3   \n",
       "4    playa de chachalacas                            rameau project   \n",
       "\n",
       "                type        date       unit  weight1          item1  \\\n",
       "0  instagram caption  2017-04-07  kilograms      200    spread tubs   \n",
       "1  instagram caption  2018-09-04     pounds      100          trash   \n",
       "2      press release  2021-05-25  kilograms      386  glass bottles   \n",
       "3      press release  2016-12-30        kgs      332    tupperwares   \n",
       "4      press release  2022-04-15        lbs      168        plastic   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Generate an instagram caption for a beach clea...   \n",
       "1  Generate an instagram caption for a beach clea...   \n",
       "2  Generate a press release for a beach cleanup w...   \n",
       "3  Generate a press release for a beach cleanup w...   \n",
       "4  Generate a press release for a beach cleanup w...   \n",
       "\n",
       "                                                text  weight2 item2  \\\n",
       "0  It was inspiring to witness so many people com...     <NA>   nan   \n",
       "1  We just made a huge difference at Grahams Beac...     <NA>   nan   \n",
       "2  Plastic Pollution Coalition Australia (PPCA) i...     <NA>   nan   \n",
       "3  Take 3 Celebrates a Successful Beach Cleanup i...     <NA>   nan   \n",
       "4  FOR IMMEDIATE RELEASE\\n\\nThe Rameau Project Ce...     <NA>   nan   \n",
       "\n",
       "                                          text_split  \\\n",
       "0  [It, was, inspiring, to, witness, so, many, pe...   \n",
       "1  [We, just, made, a, huge, difference, at, Grah...   \n",
       "2  [Plastic, Pollution, Coalition, Australia, (, ...   \n",
       "3  [Take, 3, Celebrates, a, Successful, Beach, Cl...   \n",
       "4  [FOR, IMMEDIATE, RELEASE, The, Rameau, Project...   \n",
       "\n",
       "                           org_no_space         loc_no_space  \\\n",
       "0                                 take3           adamsrocks   \n",
       "1  globalallianceagainstmarinepollution         grahamsbeach   \n",
       "2    plasticpollutioncoalitionaustralia        norfolkisland   \n",
       "3                                 take3  playagrandedesaboga   \n",
       "4                         rameauproject   playadechachalacas   \n",
       "\n",
       "                                           date_vars  \\\n",
       "0  [2017-04-07, April 7 , 2017, april 7 , 2017, A...   \n",
       "1  [2018-09-04, September 4 , 2018, september 4 ,...   \n",
       "2  [2021-05-25, May 25 , 2021, may 25 , 2021, May...   \n",
       "3  [2016-12-30, December 30 , 2016, december 30 ,...   \n",
       "4  [2022-04-15, April 15 , 2022, april 15 , 2022,...   \n",
       "\n",
       "                   weight1_text weight2_text  \\\n",
       "0                   two hundred         <na>   \n",
       "1                   one hundred         <na>   \n",
       "2  three hundred and eighty-six         <na>   \n",
       "3  three hundred and thirty-two         <na>   \n",
       "4   one hundred and sixty-eight         <na>   \n",
       "\n",
       "                               date_vars_first_token  \n",
       "0  {April, 04/07/2017, 04/07/17, 7th, 7, 2017-04-...  \n",
       "1  {2018-09-04, 09/04/2018, Sep, 4th, sep, 4, Sep...  \n",
       "2  {05-25-2021, 25th, 25, 2021-05-25, may, May, 0...  \n",
       "3  {30, 12/30/16, 12/30/2016, 12-30-2016, decembe...  \n",
       "4  {April, 04/15/2022, 04-15-2022, 04/15/22, 15, ...  "
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign appropriate types\n",
    "string_cols = [\"item1\", \"item2\", \"location\"]\n",
    "df[string_cols] = df[string_cols].astype(str)\n",
    "df['weight2'] = df['weight2'].astype('Int64')\n",
    "\n",
    "# Normalize text columns to match tokenizer \n",
    "df['text'] = df['text'].apply(lambda x: separate_weight_unit(x))\n",
    "text_cols = ['text', 'organization', \"item1\", \"item2\", \"location\", \"date\"]\n",
    "for i in text_cols:\n",
    "    df[i] = df[i].apply(lambda x: character_norm(x))\n",
    "df['text'] = df['text'].apply(lambda x: x.strip())\n",
    "\n",
    "# Tokenize text\n",
    "df['text_split'] = df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Preprocess orgs and locations \n",
    "df['org_no_space'] = df['organization'].apply(lambda x: remove_spaces(x))\n",
    "df['loc_no_space'] = df['location'].apply(lambda x: remove_spaces(x))\n",
    "for i in string_cols:\n",
    "    df[i] = df[i].apply(lambda x: add_comma_token(x))\n",
    "\n",
    "# Compute variations of date and weight formats and preprocess into desired formats\n",
    "df['date_vars'] = df['date'].apply(lambda x: date_to_formats(x))\n",
    "df['weight1_text'] = df['weight1'].apply(lambda x: number_to_words(x)[1])\n",
    "df['weight2_text'] = df['weight2'].apply(lambda x: number_to_words(x)[1] if pd.notnull(x) else str(x))\n",
    "df['date_vars'] = df['date_vars'].apply(lambda x: add_date_var_comma_token(x))\n",
    "df['date_vars_first_token'] = df['date_vars'].apply(lambda x: get_first_token_set(x))\n",
    "\n",
    "# Make string columns lowercase for downstream comparisons\n",
    "lowercase_cols = string_cols + ['organization', 'org_no_space', 'loc_no_space', 'weight1_text', 'weight2_text']\n",
    "for i in lowercase_cols:\n",
    "    df[i] = df[i].apply(lambda x: x.lower())\n",
    "\n",
    "df.head()\n",
    "# df.info()\n",
    "# print(df.iloc[12])\n",
    "# df['prompt'][13]\n",
    "# df['date_vars_first_token'][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into list of words with associated 'B - entity', 'I - entity' or 'O'\n",
    "\n",
    "units = set([\"kilograms\", \"kilogram\", \"kgs\", \"kg\", \"lb\", \"lbs\", \"pounds\", \"pound\"])\n",
    "filler_words = set([\"and\", \"the\", \"a\", \"an\", \",\"])\n",
    "\n",
    "def assign_entity_types(row):\n",
    "    words = row['text_split']\n",
    "    new_tags = []\n",
    "    prev_item_tag = False\n",
    "\n",
    "    idx = 0\n",
    "    while (idx < len(words)):\n",
    "        loc_length = len(row['location'].split())\n",
    "        org_length = len(row['organization'].split())\n",
    "        weight1_text_length = len(row['weight1_text'].split())\n",
    "        if row['weight2_text'] != None:\n",
    "            weight2_text_length = len(row['weight2_text'].split())\n",
    "        else:\n",
    "            weight2_text_length = -1\n",
    "        \n",
    "        # Assign location labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        # Does not handle extraneous locations not provided in prompt!\n",
    "        if ((idx <= len(words) - loc_length) and \n",
    "            [x.lower() for x in words[idx : idx + loc_length]] == row['location'].split()):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "            for i in range(1, loc_length):\n",
    "                new_tags.append(\"I-LOC\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['loc_no_space']):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "\n",
    "        # Assign organization labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        elif ((idx <= len(words) - org_length) and \n",
    "            [x.lower() for x in words[idx : idx + org_length]] == (row['organization'].lower().split())):\n",
    "            new_tags.append(\"B-ORG\")            # idea for later: tag acronyms for Orgs?\n",
    "            idx += 1                            \n",
    "            for i in range(1, org_length):\n",
    "                new_tags.append(\"I-ORG\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['org_no_space']):\n",
    "            new_tags.append(\"B-ORG\")      \n",
    "            idx += 1\n",
    "            \n",
    "        # Assign unit labels\n",
    "        elif words[idx] in units:   \n",
    "            new_tags.append(\"B-UNT\")\n",
    "            idx += 1\n",
    "        \n",
    "        # Assign weight labels for numeric and text numbers (consider '-' and non- '-' versions of written numbers?)\n",
    "        elif (words[idx] == str(row['weight1']) or (row['weight2'] != None and words[idx] == str(row['weight2']))): \n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "        elif ((idx <= len(words) - weight1_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight1_text_length]] == row['weight1_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "        elif ((weight2_text_length > 0) and (idx <= len(words) - weight2_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight2_text_length]] == row['weight2_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "\n",
    "        # Assign item labels (dont look for consecutive matches here)\n",
    "        # Does not handle extraneous trash items not provided in prompt!\n",
    "        elif ((any(words[idx] == word for word in row['item1'].split()) or \n",
    "             (row['item2'] != None and any(words[idx] == word for word in row['item2'].split()))) and\n",
    "             words[idx] not in filler_words):\n",
    "            if prev_item_tag: \n",
    "                new_tags.append(\"I-ITM\")\n",
    "            else:\n",
    "                new_tags.append(\"B-ITM\")\n",
    "                prev_item_tag = True\n",
    "            idx += 1\n",
    "        \n",
    "        # Assign date labels (check only first token to minimize computation on each word)\n",
    "        elif (words[idx] in row['date_vars_first_token']):\n",
    "            # Check for complete consecutive match with any of the possible date variations \n",
    "            date_found = False\n",
    "            for date_var in row['date_vars']:\n",
    "                if ((idx <= len(words) - len(date_var.split())) and \n",
    "                    [x.lower() for x in words[idx : idx + len(date_var.split())]] == date_var.lower().split()):\n",
    "                    new_tags.append(\"B-DAT\")\n",
    "                    idx += 1\n",
    "                    for i in range(1, len(date_var.split())):\n",
    "                        new_tags.append(\"I-DAT\")\n",
    "                        idx += 1\n",
    "                    date_found = True\n",
    "                    break\n",
    "            # If the text matches with none of the date_vars, we need to append \"O\"\n",
    "            if not date_found:\n",
    "                new_tags.append(\"O\")\n",
    "                prev_item_tag = False\n",
    "                idx += 1\n",
    "        \n",
    "        else:\n",
    "            new_tags.append(\"O\")\n",
    "            prev_item_tag = False\n",
    "            idx += 1\n",
    "\n",
    "    return list(zip(words, new_tags))\n",
    "\n",
    "df['tagged_entities'] = df.apply(assign_entity_types, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences\n",
    "# df['text_split'][13] = ['I', 'have', 'two', 'hundred', 'and', 'twenty', 'dogs', 'and', 'two', 'hundred', 'and', 'fifty-seven', \n",
    "#                         'cats', 'ugly', 'two', 'hundred', 'and', 'twenty-one', 'on', '11', 'Jan', '2020', '01-11-2020']\n",
    "# assign_entity_types(df.iloc[13])\n",
    "# print(df['text_split'][13][22] in df['date_vars_first_token'][13])\n",
    "# print('01-11-2020'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adopt B-ORG\n",
      "a I-ORG\n",
      "Beach I-ORG\n",
      "284 B-WEI\n",
      "kgs B-UNT\n",
      "Thessaloníki B-LOC\n",
      "Thessaloníki B-LOC\n",
      "April B-DAT\n",
      "20 I-DAT\n",
      ", I-DAT\n",
      "2015 I-DAT\n",
      "Adopt B-ORG\n",
      "a I-ORG\n",
      "Beach I-ORG\n",
      "plastic B-ITM\n",
      "Thessaloníki B-LOC\n",
      "284 B-WEI\n",
      "kgs B-UNT\n",
      "aluminium B-ITM\n",
      "blister I-ITM\n",
      "packs I-ITM\n",
      "disposable B-ITM\n",
      "food I-ITM\n",
      "containers I-ITM\n",
      "plastic B-ITM\n",
      "bottle I-ITM\n",
      "caps I-ITM\n",
      "Adopt B-ORG\n",
      "a I-ORG\n",
      "Beach I-ORG\n",
      "plastic B-ITM\n",
      "Adopt B-ORG\n",
      "a I-ORG\n",
      "Beach I-ORG\n",
      "plastic B-ITM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Adopt a Beach Cleanup results in 284 kgs of Waste Removed from Thessaloníki Beach\\n\\nThessaloníki, April 20, 2015 - Adopt a Beach, an initiative to reduce the amount of plastic waste in the ocean, is proud to report that a recent cleanup in Thessaloníki successfully removed 284 kgs of waste, including aluminium blister packs, disposable food containers, and plastic bottle caps.\\n\\nThis achievement was made possible through the dedication of over 200 volunteers and the generous support of several local businesses. In addition, the local community provided valuable support by collecting and disposing of the waste in a safe and sustainable manner.\\n\\n\"This is an important milestone for our initiative,\" said Adopt a Beach spokesperson John Doe. \"We are proud to have made a tangible difference in the amount of plastic waste in the ocean. We hope that this will serve as an example of the positive impact that can be made when the community comes together to tackle a common problem.\"\\n\\nAdopt a Beach is committed to continuing its efforts to reduce plastic waste in the ocean and is actively seeking additional volunteers and sponsors. For more information about how to get involved, visit www.adoptabeach.org.'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review newly assigned non-\"O\" tags\n",
    "SAMPLE_NO = 8\n",
    "for i in df.iloc[SAMPLE_NO]['tagged_entities']:\n",
    "    if i[1] != \"O\":\n",
    "        print(i[0], i[1])\n",
    "\n",
    "# print(df['tagged_entities'][SAMPLE_NO])\n",
    "df['text'][SAMPLE_NO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 13:40:57 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1a22a6e31f44ef865a7de10f0b2cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 13:40:57 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2023-02-23 13:40:57 INFO: Use device: cpu\n",
      "2023-02-23 13:40:57 INFO: Loading: tokenize\n",
      "2023-02-23 13:40:57 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use stanza tokenizer to determine sentence chunks \n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "df['text_stanza_tokenize'] = df['text'].apply(lambda x: nlp(x))\n",
    "\n",
    "# Compiles all sentences into a single list of lists (sentences) of word-pairs (word, NER tag)\n",
    "def get_all_sentences(df):\n",
    "    all_sentences = []\n",
    "    for i in range(len(df)):\n",
    "        idx = 0\n",
    "        for sentence in df.iloc[i]['text_stanza_tokenize'].sentences:\n",
    "            # Check for first word in stanza-tokenized sentence and adjust index within small range \n",
    "            # accordingly (Problem: may result in 1 or 2 tokens being truncated from front or end of sentences)\n",
    "            first_word = sentence.tokens[0].text\n",
    "            if (first_word != df.iloc[i]['tagged_entities'][idx][0]):\n",
    "                for adj in [-2, -1, 1, 2]:\n",
    "                    if (first_word == df.iloc[i]['tagged_entities'][idx + adj][0]):\n",
    "                        idx = idx + adj\n",
    "            \n",
    "            end_sentence_limit = min(idx+len(sentence.words), len(df.iloc[i]['tagged_entities'])-1)\n",
    "            new_sentence = list(df.iloc[i]['tagged_entities'][idx:end_sentence_limit])\n",
    "            all_sentences.append(new_sentence)\n",
    "            idx += len(sentence.words)\n",
    "    return all_sentences\n",
    "\n",
    "all_sentences = get_all_sentences(df)\n",
    "# print(len(all_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into datasets = (train_sentences, dev_sentences, test_sentences)\n",
    "\n",
    "DEV_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "random.seed(1234)\n",
    "random.shuffle(all_sentences)\n",
    "\n",
    "train_sentences = all_sentences[ : int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT))]\n",
    "dev_sentences = all_sentences[int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT)) : int(len(all_sentences)*(1-TEST_SPLIT))]\n",
    "test_sentences = all_sentences[int(len(all_sentences)*(1-TEST_SPLIT)) : ]\n",
    "\n",
    "# print(len(train_sentences))\n",
    "# print(len(dev_sentences))\n",
    "# print(len(test_sentences))\n",
    "# print(len(all_sentences))\n",
    "\n",
    "datasets = (train_sentences, dev_sentences, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.train.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.train.json\n",
      "95 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.train.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.train.json\n",
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.dev.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.dev.json\n",
      "12 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.dev.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.dev.json\n",
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.test.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.test.json\n",
      "12 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.test.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.test.json\n"
     ]
    }
   ],
   "source": [
    "# Convert file and write to JSON file needed for Stanza modelling\n",
    "out_directory = os.getcwd()\n",
    "write_dataset(datasets, out_directory, \"TOC_Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to JSON file needed by Stanza model\n",
    "# There is a conversion script called several times in prepare_ner_dataset.py which converts IOB format to our internal NER format:\n",
    "# import stanza.utils.datasets.ner.prepare_ner_file as prepare_ner_file\n",
    "\n",
    "# prepare_ner_file.process_dataset(input_iob, output_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
