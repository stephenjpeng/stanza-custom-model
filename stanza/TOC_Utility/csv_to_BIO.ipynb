{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import stanza\n",
    "from stanza.utils.datasets.ner.utils import write_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in synthetic data\n",
    "df = pd.read_csv('../../data/data/generated/data_230124-172021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use re to replace any instances of \"####kg\" with \"#### kg\" where #### is any continuous \n",
    "# sequence of numbers and unit is one of those listed below\n",
    "def separate_weight_unit(row):\n",
    "    return re.sub(r'([0-9]+)(kgs|kg|lbs|lb|pounds|kilograms)', r\"\\1 \\2\", row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>organization</th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>unit</th>\n",
       "      <th>weight1</th>\n",
       "      <th>item1</th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "      <th>weight2</th>\n",
       "      <th>item2</th>\n",
       "      <th>text_split</th>\n",
       "      <th>tagged_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams Rocks</td>\n",
       "      <td>Take 3</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>kilograms</td>\n",
       "      <td>200</td>\n",
       "      <td>spread tubs</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>It was inspiring to witness so many people com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>[It, was, inspiring, to, witness, so, many, pe...</td>\n",
       "      <td>[(It, O), (was, O), (inspiring, O), (to, O), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grahams Beach</td>\n",
       "      <td>Global Alliance Against Marine Pollution</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2018-09-04</td>\n",
       "      <td>pounds</td>\n",
       "      <td>100</td>\n",
       "      <td>trash</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>We just made a huge difference at Grahams Beac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>[We, just, made, a, huge, difference, at, Grah...</td>\n",
       "      <td>[(We, O), (just, O), (made, O), (a, O), (huge,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Norfolk Island</td>\n",
       "      <td>Plastic Pollution Coalition Australia</td>\n",
       "      <td>press release</td>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>kilograms</td>\n",
       "      <td>386</td>\n",
       "      <td>glass bottles</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>\\nPlastic Pollution Coalition Australia (PPCA)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Plastic, Pollution, Coalition, Australia, (, ...</td>\n",
       "      <td>[(Plastic, B-ORG), (Pollution, I-ORG), (Coalit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Playa Grande de Saboga</td>\n",
       "      <td>Take 3</td>\n",
       "      <td>press release</td>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>kgs</td>\n",
       "      <td>332</td>\n",
       "      <td>tupperwares</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>Take 3 Celebrates a Successful Beach Cleanup i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Take, 3, Celebrates, a, Successful, Beach, Cl...</td>\n",
       "      <td>[(Take, B-ORG), (3, I-ORG), (Celebrates, O), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Playa de Chachalacas</td>\n",
       "      <td>Rameau Project</td>\n",
       "      <td>press release</td>\n",
       "      <td>2022-04-15</td>\n",
       "      <td>lbs</td>\n",
       "      <td>168</td>\n",
       "      <td>plastic</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE\\n\\nThe Rameau Project Ce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>[FOR, IMMEDIATE, RELEASE, The, Rameau, Project...</td>\n",
       "      <td>[(FOR, O), (IMMEDIATE, O), (RELEASE, O), (The,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 location                              organization  \\\n",
       "0             Adams Rocks                                    Take 3   \n",
       "1           Grahams Beach  Global Alliance Against Marine Pollution   \n",
       "2          Norfolk Island     Plastic Pollution Coalition Australia   \n",
       "3  Playa Grande de Saboga                                    Take 3   \n",
       "4    Playa de Chachalacas                            Rameau Project   \n",
       "\n",
       "                type        date       unit  weight1          item1  \\\n",
       "0  instagram caption  2017-04-07  kilograms      200    spread tubs   \n",
       "1  instagram caption  2018-09-04     pounds      100          trash   \n",
       "2      press release  2021-05-25  kilograms      386  glass bottles   \n",
       "3      press release  2016-12-30        kgs      332    tupperwares   \n",
       "4      press release  2022-04-15        lbs      168        plastic   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Generate an instagram caption for a beach clea...   \n",
       "1  Generate an instagram caption for a beach clea...   \n",
       "2  Generate a press release for a beach cleanup w...   \n",
       "3  Generate a press release for a beach cleanup w...   \n",
       "4  Generate a press release for a beach cleanup w...   \n",
       "\n",
       "                                                text  weight2 item2  \\\n",
       "0  It was inspiring to witness so many people com...      NaN   nan   \n",
       "1  We just made a huge difference at Grahams Beac...      NaN   nan   \n",
       "2  \\nPlastic Pollution Coalition Australia (PPCA)...      NaN   nan   \n",
       "3  Take 3 Celebrates a Successful Beach Cleanup i...      NaN   nan   \n",
       "4  FOR IMMEDIATE RELEASE\\n\\nThe Rameau Project Ce...      NaN   nan   \n",
       "\n",
       "                                          text_split  \\\n",
       "0  [It, was, inspiring, to, witness, so, many, pe...   \n",
       "1  [We, just, made, a, huge, difference, at, Grah...   \n",
       "2  [Plastic, Pollution, Coalition, Australia, (, ...   \n",
       "3  [Take, 3, Celebrates, a, Successful, Beach, Cl...   \n",
       "4  [FOR, IMMEDIATE, RELEASE, The, Rameau, Project...   \n",
       "\n",
       "                                     tagged_entities  \n",
       "0  [(It, O), (was, O), (inspiring, O), (to, O), (...  \n",
       "1  [(We, O), (just, O), (made, O), (a, O), (huge,...  \n",
       "2  [(Plastic, B-ORG), (Pollution, I-ORG), (Coalit...  \n",
       "3  [(Take, B-ORG), (3, I-ORG), (Celebrates, O), (...  \n",
       "4  [(FOR, O), (IMMEDIATE, O), (RELEASE, O), (The,...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider using Stanza sentence tokenizer instead of nltk word_tokenizer? See csv_to_BIO_stanzaTokenize.ipynb file\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: separate_weight_unit(x))\n",
    "df['text_split'] = df['text'].apply(lambda x: x.strip())\n",
    "df['text_split'] = df['text_split'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "\n",
    "string_cols = [\"item1\", \"item2\", ]\n",
    "df[string_cols] = df[string_cols].astype(str)\n",
    "df.head()\n",
    "# df.info()\n",
    "# df['text_split'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into list of words with associated 'B - entity', 'I - entity' or 'O'\n",
    "# Look at other preprocessing steps in read_datasets function in convert_bn_daffodil\n",
    "\n",
    "units = [\"kilograms\", \"kilogram\", \"kgs\", \"kg\", \"lb\", \"lbs\", \"pounds\", \"pound\"]\n",
    "\n",
    "def assign_entity_types(row):\n",
    "    words = row['text_split']\n",
    "    new_tags = []\n",
    "    prev_org_tag = False\n",
    "    prev_loc_tag = False\n",
    "    prev_unit_tag = False\n",
    "    prev_weight_tag = False\n",
    "    prev_item_tag = False\n",
    "    \n",
    "    idx = 0\n",
    "    while (idx < len(words)):\n",
    "        # Assign location labels\n",
    "        if any(words[idx] == word for word in row['location'].split()): #Problem: only searches for specific location that was \n",
    "                                        # given in the prompt, not other locations that GPT-3 produces in text\n",
    "                                        # Solution: ignore these extraneous locations, they're not our targets\n",
    "                                        # Need to assign only consecutive matches for locations (calc len(location) and check if\n",
    "                                        # subsequent words all equal the location. If they do, append all the words with appropriate \n",
    "                                        # B and I tags. Convert to while loop instead of for and add length of\n",
    "                                        # location at end of while?)\n",
    "            if prev_loc_tag:\n",
    "                new_tags.append(\"I-LOC\")\n",
    "            else:\n",
    "                new_tags.append(\"B-LOC\")\n",
    "                prev_loc_tag = True\n",
    "            idx += 1\n",
    "        # Assign organization labels\n",
    "        elif any(words[idx] == word for word in row['organization'].split()):\n",
    "            if prev_org_tag:                    # I should lowercase everything when checking orgs\n",
    "                                                # Need to assign only consecutive matches for orgs\n",
    "                                                # Check for edge case of orgs without spaces (e.g. @take3 should get tagged)\n",
    "                                                # idea for later: acronyms for Orgs?\n",
    "                new_tags.append(\"I-ORG\")\n",
    "            else:\n",
    "                new_tags.append(\"B-ORG\")\n",
    "                prev_org_tag = True\n",
    "            idx += 1\n",
    "        # Assign unit labels\n",
    "        elif any(words[idx] == word for word in units):   #Problem: some texts might include \"two hundred\" instead of 200 \n",
    "                                                        #Solution: word2num and num2word packages\n",
    "            if prev_unit_tag: \n",
    "                new_tags.append(\"I-UNT\")\n",
    "            else:\n",
    "                new_tags.append(\"B-UNT\")\n",
    "                prev_unit_tag = True\n",
    "            idx += 1\n",
    "        # Assign weight labels \n",
    "        elif (words[idx] == str(row['weight1']) or (row['weight2'] != None and words[idx] == str(row['weight2']))):\n",
    "            if prev_weight_tag: \n",
    "                new_tags.append(\"I-WEI\")\n",
    "            else:\n",
    "                new_tags.append(\"B-WEI\")\n",
    "                prev_weight_tag = True\n",
    "            idx += 1\n",
    "        # Assign item labels (dont look for consecutive matches here)\n",
    "        elif (any(words[idx] == word for word in row['item1'].split()) or \n",
    "                                (row['item2'] != None and any(words[idx] == word for word in row['item2'].split()))):\n",
    "            if prev_item_tag: \n",
    "                new_tags.append(\"I-ITM\")\n",
    "            else:\n",
    "                new_tags.append(\"B-ITM\")\n",
    "                prev_item_tag = True\n",
    "            idx += 1\n",
    "        # Open question: How to assign dates? Need to capture all possible date formats?\n",
    "        # Solution: convert golden value dates to datetime objects then use strftime package to generate \n",
    "        # possible text versions of it\n",
    "        else:\n",
    "            new_tags.append(\"O\")\n",
    "            prev_org_tag = False\n",
    "            prev_loc_tag = False\n",
    "            prev_unit_tag = False\n",
    "            prev_weight_tag = False\n",
    "            prev_item_tag = False\n",
    "            idx += 1\n",
    "    return list(zip(words, new_tags))\n",
    "\n",
    "df['tagged_entities'] = df.apply(lambda row : assign_entity_types(row), axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plastic B-ORG\n",
      "Pollution I-ORG\n",
      "Coalition I-ORG\n",
      "Australia I-ORG\n",
      "Norfolk B-LOC\n",
      "Island I-LOC\n",
      "386 B-WEI\n",
      "kilograms B-UNT\n",
      "glass B-ITM\n",
      "bottles I-ITM\n",
      "Australia B-ORG\n",
      "glass B-ITM\n",
      "bottles I-ITM\n",
      "Australia B-ORG\n",
      "[('Plastic', 'B-ORG'), ('Pollution', 'I-ORG'), ('Coalition', 'I-ORG'), ('Australia', 'I-ORG'), ('(', 'O'), ('PPCA', 'O'), (')', 'O'), ('is', 'O'), ('proud', 'O'), ('to', 'O'), ('announce', 'O'), ('the', 'O'), ('success', 'O'), ('of', 'O'), ('its', 'O'), ('latest', 'O'), ('beach', 'O'), ('cleanup', 'O'), ('on', 'O'), ('Norfolk', 'B-LOC'), ('Island', 'I-LOC'), ('.', 'O'), ('On', 'O'), ('May', 'O'), ('25th', 'O'), (',', 'O'), ('2021', 'O'), (',', 'O'), ('PPCA', 'O'), ('and', 'O'), ('its', 'O'), ('volunteers', 'O'), ('collected', 'O'), ('a', 'O'), ('whopping', 'O'), ('386', 'B-WEI'), ('kilograms', 'B-UNT'), ('of', 'O'), ('glass', 'B-ITM'), ('bottles', 'I-ITM'), ('from', 'O'), ('the', 'O'), ('island', 'O'), (\"'s\", 'O'), ('coastline', 'O'), ('.', 'O'), ('This', 'O'), ('is', 'O'), ('a', 'O'), ('remarkable', 'O'), ('achievement', 'O'), ('for', 'O'), ('the', 'O'), ('organization', 'O'), (',', 'O'), ('which', 'O'), ('has', 'O'), ('made', 'O'), ('it', 'O'), ('its', 'O'), ('mission', 'O'), ('to', 'O'), ('rid', 'O'), ('Australia', 'B-ORG'), (\"'s\", 'O'), ('oceans', 'O'), ('and', 'O'), ('beaches', 'O'), ('of', 'O'), ('plastic', 'O'), ('pollution', 'O'), ('.', 'O'), ('``', 'O'), ('It', 'O'), ('is', 'O'), ('extremely', 'O'), ('gratifying', 'O'), ('for', 'O'), ('us', 'O'), ('to', 'O'), ('see', 'O'), ('the', 'O'), ('tangible', 'O'), ('results', 'O'), ('of', 'O'), ('our', 'O'), ('work', 'O'), ('in', 'O'), ('beach', 'O'), ('cleanups', 'O'), ('like', 'O'), ('this', 'O'), ('one', 'O'), (',', 'O'), (\"''\", 'O'), ('said', 'O'), ('PPCA', 'O'), ('CEO', 'O'), ('John', 'O'), ('Smith', 'O'), ('.', 'O'), ('``', 'O'), ('Our', 'O'), ('volunteers', 'O'), ('worked', 'O'), ('hard', 'O'), ('and', 'O'), ('achieved', 'O'), ('a', 'O'), ('great', 'O'), ('result', 'O'), (',', 'O'), ('and', 'O'), ('we', 'O'), ('are', 'O'), ('thrilled', 'O'), ('that', 'O'), ('we', 'O'), ('were', 'O'), ('able', 'O'), ('to', 'O'), ('make', 'O'), ('such', 'O'), ('a', 'O'), ('big', 'O'), ('difference', 'O'), ('in', 'O'), ('the', 'O'), ('local', 'O'), ('environment', 'O'), ('.', 'O'), (\"''\", 'O'), ('The', 'O'), ('glass', 'B-ITM'), ('bottles', 'I-ITM'), ('collected', 'O'), ('during', 'O'), ('the', 'O'), ('cleanup', 'O'), ('will', 'O'), ('be', 'O'), ('recycled', 'O'), ('and', 'O'), ('reused', 'O'), ('.', 'O'), ('This', 'O'), ('not', 'O'), ('only', 'O'), ('helps', 'O'), ('to', 'O'), ('reduce', 'O'), ('the', 'O'), ('amount', 'O'), ('of', 'O'), ('plastic', 'O'), ('waste', 'O'), ('in', 'O'), ('the', 'O'), ('ocean', 'O'), (',', 'O'), ('but', 'O'), ('also', 'O'), ('supports', 'O'), ('the', 'O'), ('local', 'O'), ('economy', 'O'), ('by', 'O'), ('creating', 'O'), ('jobs', 'O'), ('and', 'O'), ('generating', 'O'), ('revenue', 'O'), ('.', 'O'), ('``', 'O'), ('We', 'O'), ('are', 'O'), ('committed', 'O'), ('to', 'O'), ('continuing', 'O'), ('our', 'O'), ('efforts', 'O'), ('to', 'O'), ('make', 'O'), ('Australia', 'B-ORG'), (\"'s\", 'O'), ('beaches', 'O'), ('and', 'O'), ('oceans', 'O'), ('cleaner', 'O'), (',', 'O'), ('healthier', 'O'), (',', 'O'), ('and', 'O'), ('safer', 'O'), ('for', 'O'), ('all', 'O'), (',', 'O'), (\"''\", 'O'), ('said', 'O'), ('Smith', 'O'), ('.', 'O'), ('``', 'O'), ('We', 'O'), ('are', 'O'), ('calling', 'O'), ('on', 'O'), ('all', 'O'), ('Australians', 'O'), ('to', 'O'), ('join', 'O'), ('us', 'O'), ('in', 'O'), ('this', 'O'), ('effort', 'O'), ('and', 'O'), ('help', 'O'), ('us', 'O'), ('make', 'O'), ('a', 'O'), ('real', 'O'), ('difference', 'O'), ('.', 'O'), (\"''\", 'O')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPlastic Pollution Coalition Australia (PPCA) is proud to announce the success of its latest beach cleanup on Norfolk Island. On May 25th, 2021, PPCA and its volunteers collected a whopping 386 kilograms of glass bottles from the island\\'s coastline.\\n\\nThis is a remarkable achievement for the organization, which has made it its mission to rid Australia\\'s oceans and beaches of plastic pollution.\\n\\n\"It is extremely gratifying for us to see the tangible results of our work in beach cleanups like this one,\" said PPCA CEO John Smith. \"Our volunteers worked hard and achieved a great result, and we are thrilled that we were able to make such a big difference in the local environment.\"\\n\\nThe glass bottles collected during the cleanup will be recycled and reused. This not only helps to reduce the amount of plastic waste in the ocean, but also supports the local economy by creating jobs and generating revenue.\\n\\n\"We are committed to continuing our efforts to make Australia\\'s beaches and oceans cleaner, healthier, and safer for all,\" said Smith. \"We are calling on all Australians to join us in this effort and help us make a real difference.\"'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review newly assigned non-\"O\" tags\n",
    "SAMPLE_NO = 2\n",
    "for i in df.iloc[SAMPLE_NO]['tagged_entities']:\n",
    "    if i[1] != \"O\":\n",
    "        print(i[0], i[1])\n",
    "\n",
    "print(df['tagged_entities'][SAMPLE_NO])\n",
    "df['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 16:34:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d634a93d9add42c19eb906f4f79b0782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 16:34:04 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2023-02-20 16:34:04 INFO: Use device: cpu\n",
      "2023-02-20 16:34:04 INFO: Loading: tokenize\n",
      "2023-02-20 16:34:04 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Compile all sentences into a single list of lists (sentences) of word-pairs (word, NER tag)\n",
    "\n",
    "# Open question: this method is not very robust (cross-references stanza tokenizer sentence lengths \n",
    "# against list of original sentence text words, which might not be 1-1).\n",
    "#   Solution: use a find() to search for first word in each sentence, not just blind indexing into paragraph text\n",
    "\n",
    "# Method is not efficient. Maybe could be vectorized (?), but we only have to run this script once\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "df['text_stanza_tokenize'] = df['text'].apply(lambda x: nlp(x))\n",
    "\n",
    "all_sentences = []\n",
    "for i in range(len(df)):\n",
    "    idx = 0\n",
    "    for sentence in df.iloc[i]['text_stanza_tokenize'].sentences:\n",
    "        new_sentence = list(df.iloc[i]['tagged_entities'][idx:idx+len(sentence.words)])\n",
    "        all_sentences.append(new_sentence)\n",
    "        idx += len(sentence.words)\n",
    "\n",
    "# print(all_sentences[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into datasets = (train_sentences, dev_sentences, test_sentences)\n",
    "\n",
    "DEV_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "random.seed(1234)\n",
    "random.shuffle(all_sentences)\n",
    "\n",
    "train_sentences = all_sentences[ : int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT))]\n",
    "dev_sentences = all_sentences[int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT)) : int(len(all_sentences)*(1-TEST_SPLIT))]\n",
    "test_sentences = all_sentences[int(len(all_sentences)*(1-TEST_SPLIT)) : ]\n",
    "\n",
    "# print(len(train_sentences))\n",
    "# print(len(dev_sentences))\n",
    "# print(len(test_sentences))\n",
    "# print(len(all_sentences))\n",
    "\n",
    "datasets = (train_sentences, dev_sentences, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.train.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.train.json\n",
      "95 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.train.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.train.json\n",
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.dev.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.dev.json\n",
      "12 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.dev.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.dev.json\n",
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.test.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.test.json\n",
      "12 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.test.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.test.json\n"
     ]
    }
   ],
   "source": [
    "# Convert file and write to JSON file needed for Stanza modelling\n",
    "out_directory = os.getcwd()\n",
    "write_dataset(datasets, out_directory, \"TOC_Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to JSON file needed by Stanza model\n",
    "# There is a conversion script called several times in prepare_ner_dataset.py which converts IOB format to our internal NER format:\n",
    "# import stanza.utils.datasets.ner.prepare_ner_file as prepare_ner_file\n",
    "\n",
    "# prepare_ner_file.process_dataset(input_iob, output_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
