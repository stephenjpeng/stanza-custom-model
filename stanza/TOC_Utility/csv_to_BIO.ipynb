{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "from stanza.utils.datasets.ner.utils import write_dataset\n",
    "from transform_weight_date import number_to_words, date_to_formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in synthetic data\n",
    "df = pd.read_csv('../../data/data/generated/data_230124-172021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use re to replace any instances of \"####kg\" with \"#### kg\" where #### is any continuous \n",
    "# sequence of numbers and unit is one of those listed below\n",
    "def separate_weight_unit(row):\n",
    "    return re.sub(r'([0-9]+)(kgs|kg|lbs|lb|pounds|kilograms)', r\"\\1 \\2\", row)\n",
    "\n",
    "# Function to remove spaces (e.g. \"Take 3\" -> \"Take3\")\n",
    "def remove_spaces(row):\n",
    "    return row.replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>organization</th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>unit</th>\n",
       "      <th>weight1</th>\n",
       "      <th>item1</th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "      <th>weight2</th>\n",
       "      <th>item2</th>\n",
       "      <th>text_split</th>\n",
       "      <th>org_no_space</th>\n",
       "      <th>loc_no_space</th>\n",
       "      <th>date_vars</th>\n",
       "      <th>weight1_text</th>\n",
       "      <th>weight2_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adams rocks</td>\n",
       "      <td>take 3</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>kilograms</td>\n",
       "      <td>200</td>\n",
       "      <td>spread tubs</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>It was inspiring to witness so many people com...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[It, was, inspiring, to, witness, so, many, pe...</td>\n",
       "      <td>take3</td>\n",
       "      <td>adamsrocks</td>\n",
       "      <td>[2017-04-07, April  7, 2017,  7 Apr 2017, 04/0...</td>\n",
       "      <td>two hundred</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grahams beach</td>\n",
       "      <td>global alliance against marine pollution</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2018-09-04</td>\n",
       "      <td>pounds</td>\n",
       "      <td>100</td>\n",
       "      <td>trash</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>We just made a huge difference at Grahams Beac...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[We, just, made, a, huge, difference, at, Grah...</td>\n",
       "      <td>globalallianceagainstmarinepollution</td>\n",
       "      <td>grahamsbeach</td>\n",
       "      <td>[2018-09-04, September  4, 2018,  4 Sep 2018, ...</td>\n",
       "      <td>one hundred</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>norfolk island</td>\n",
       "      <td>plastic pollution coalition australia</td>\n",
       "      <td>press release</td>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>kilograms</td>\n",
       "      <td>386</td>\n",
       "      <td>glass bottles</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>\\nPlastic Pollution Coalition Australia (PPCA)...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Plastic, Pollution, Coalition, Australia, (, ...</td>\n",
       "      <td>plasticpollutioncoalitionaustralia</td>\n",
       "      <td>norfolkisland</td>\n",
       "      <td>[2021-05-25, May 25, 2021, 25 May 2021, 05/25/...</td>\n",
       "      <td>three hundred and eighty-six</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>playa grande de saboga</td>\n",
       "      <td>take 3</td>\n",
       "      <td>press release</td>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>kgs</td>\n",
       "      <td>332</td>\n",
       "      <td>tupperwares</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>Take 3 Celebrates a Successful Beach Cleanup i...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Take, 3, Celebrates, a, Successful, Beach, Cl...</td>\n",
       "      <td>take3</td>\n",
       "      <td>playagrandedesaboga</td>\n",
       "      <td>[2016-12-30, December 30, 2016, 30 Dec 2016, 1...</td>\n",
       "      <td>three hundred and thirty-two</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>playa de chachalacas</td>\n",
       "      <td>rameau project</td>\n",
       "      <td>press release</td>\n",
       "      <td>2022-04-15</td>\n",
       "      <td>lbs</td>\n",
       "      <td>168</td>\n",
       "      <td>plastic</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE\\n\\nThe Rameau Project Ce...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[FOR, IMMEDIATE, RELEASE, The, Rameau, Project...</td>\n",
       "      <td>rameauproject</td>\n",
       "      <td>playadechachalacas</td>\n",
       "      <td>[2022-04-15, April 15, 2022, 15 Apr 2022, 04/1...</td>\n",
       "      <td>one hundred and sixty-eight</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>choke canyon state park - south shore unit</td>\n",
       "      <td>world surfing reserves</td>\n",
       "      <td>press release</td>\n",
       "      <td>2019-03-12</td>\n",
       "      <td>kgs</td>\n",
       "      <td>49</td>\n",
       "      <td>other plastic bottles</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>\\nFOR IMMEDIATE RELEASE\\n\\nWORLD SURFING RESER...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[FOR, IMMEDIATE, RELEASE, WORLD, SURFING, RESE...</td>\n",
       "      <td>worldsurfingreserves</td>\n",
       "      <td>chokecanyonstatepark-southshoreunit</td>\n",
       "      <td>[2019-03-12, March 12, 2019, 12 Mar 2019, 03/1...</td>\n",
       "      <td>forty-nine</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mayfield beach</td>\n",
       "      <td>upcycle the gyres</td>\n",
       "      <td>press release</td>\n",
       "      <td>2019-10-26</td>\n",
       "      <td>lbs</td>\n",
       "      <td>300</td>\n",
       "      <td>trash</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>Mayfield Beach Cleaned Up of 300 lbs of Trash ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Mayfield, Beach, Cleaned, Up, of, 300, lbs, o...</td>\n",
       "      <td>upcyclethegyres</td>\n",
       "      <td>mayfieldbeach</td>\n",
       "      <td>[2019-10-26, October 26, 2019, 26 Oct 2019, 10...</td>\n",
       "      <td>three hundred</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>prescott beach county park</td>\n",
       "      <td>plastic soup foundation</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>lbs</td>\n",
       "      <td>240</td>\n",
       "      <td>trash</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>Today, our @plasticsoupfoundation team collect...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Today, ,, our, @, plasticsoupfoundation, team...</td>\n",
       "      <td>plasticsoupfoundation</td>\n",
       "      <td>prescottbeachcountypark</td>\n",
       "      <td>[2022-03-31, March 31, 2022, 31 Mar 2022, 03/3...</td>\n",
       "      <td>two hundred and forty</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>thessaloníki</td>\n",
       "      <td>adopt a beach</td>\n",
       "      <td>press release</td>\n",
       "      <td>2015-04-20</td>\n",
       "      <td>kgs</td>\n",
       "      <td>284</td>\n",
       "      <td>aluminium blister packs, disposable food conta...</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>\\nAdopt a Beach Cleanup results in 284 kgs of ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Adopt, a, Beach, Cleanup, results, in, 284, k...</td>\n",
       "      <td>adoptabeach</td>\n",
       "      <td>thessaloníki</td>\n",
       "      <td>[2015-04-20, April 20, 2015, 20 Apr 2015, 04/2...</td>\n",
       "      <td>two hundred and eighty-four</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>south sandy beach</td>\n",
       "      <td>beach nourishment</td>\n",
       "      <td>press release</td>\n",
       "      <td>2017-10-07</td>\n",
       "      <td>kilograms</td>\n",
       "      <td>110</td>\n",
       "      <td>trash</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>SOUTH SANDY BEACH – On October 7, 2017, Beach ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[SOUTH, SANDY, BEACH, –, On, October, 7, ,, 20...</td>\n",
       "      <td>beachnourishment</td>\n",
       "      <td>southsandybeach</td>\n",
       "      <td>[2017-10-07, October  7, 2017,  7 Oct 2017, 10...</td>\n",
       "      <td>one hundred and ten</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mount hordern</td>\n",
       "      <td>sea change project</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>pounds</td>\n",
       "      <td>437</td>\n",
       "      <td>drink cartons</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>Today, Sea Change Project volunteers gathered ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Today, ,, Sea, Change, Project, volunteers, g...</td>\n",
       "      <td>seachangeproject</td>\n",
       "      <td>mounthordern</td>\n",
       "      <td>[2020-04-02, April  2, 2020,  2 Apr 2020, 04/0...</td>\n",
       "      <td>four hundred and thirty-seven</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>playa fluvial fuente del prior</td>\n",
       "      <td>coastal watch</td>\n",
       "      <td>press release</td>\n",
       "      <td>2016-09-18</td>\n",
       "      <td>kilograms</td>\n",
       "      <td>375</td>\n",
       "      <td>trash</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>Coastal Watch Celebrates Successful Beach Clea...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Coastal, Watch, Celebrates, Successful, Beach...</td>\n",
       "      <td>coastalwatch</td>\n",
       "      <td>playafluvialfuentedelprior</td>\n",
       "      <td>[2016-09-18, September 18, 2016, 18 Sep 2016, ...</td>\n",
       "      <td>three hundred and seventy-five</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vauréal, pico</td>\n",
       "      <td>unite bvi</td>\n",
       "      <td>press release</td>\n",
       "      <td>2022-12-11</td>\n",
       "      <td>lbs</td>\n",
       "      <td>100</td>\n",
       "      <td>plastic</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>Unite BVI Achieves Incredible 100-Pound Plasti...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Unite, BVI, Achieves, Incredible, 100-Pound, ...</td>\n",
       "      <td>unitebvi</td>\n",
       "      <td>vauréal,pico</td>\n",
       "      <td>[2022-12-11, December 11, 2022, 11 Dec 2022, 1...</td>\n",
       "      <td>one hundred</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>juno peaks</td>\n",
       "      <td>reef relief</td>\n",
       "      <td>press release</td>\n",
       "      <td>2020-01-11</td>\n",
       "      <td>pounds</td>\n",
       "      <td>220</td>\n",
       "      <td>spread tubs</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>\\nReef Relief Hosts Successful Beach Cleanup a...</td>\n",
       "      <td>257</td>\n",
       "      <td>pizza boxes</td>\n",
       "      <td>[Reef, Relief, Hosts, Successful, Beach, Clean...</td>\n",
       "      <td>reefrelief</td>\n",
       "      <td>junopeaks</td>\n",
       "      <td>[2020-01-11, January 11, 2020, 11 Jan 2020, 01...</td>\n",
       "      <td>two hundred and twenty</td>\n",
       "      <td>two hundred and fifty-seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hübl peak</td>\n",
       "      <td>rise above plastics</td>\n",
       "      <td>press release</td>\n",
       "      <td>2022-06-25</td>\n",
       "      <td>pounds</td>\n",
       "      <td>490</td>\n",
       "      <td>corrugated cartons</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>\\nFOR IMMEDIATE RELEASE\\n\\nRise Above Plastics...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[FOR, IMMEDIATE, RELEASE, Rise, Above, Plastic...</td>\n",
       "      <td>riseaboveplastics</td>\n",
       "      <td>hüblpeak</td>\n",
       "      <td>[2022-06-25, June 25, 2022, 25 Jun 2022, 06/25...</td>\n",
       "      <td>four hundred and ninety</td>\n",
       "      <td>&lt;na&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      location  \\\n",
       "0                                  adams rocks   \n",
       "1                                grahams beach   \n",
       "2                               norfolk island   \n",
       "3                       playa grande de saboga   \n",
       "4                         playa de chachalacas   \n",
       "5   choke canyon state park - south shore unit   \n",
       "6                               mayfield beach   \n",
       "7                   prescott beach county park   \n",
       "8                                 thessaloníki   \n",
       "9                            south sandy beach   \n",
       "10                               mount hordern   \n",
       "11              playa fluvial fuente del prior   \n",
       "12                               vauréal, pico   \n",
       "13                                  juno peaks   \n",
       "14                                   hübl peak   \n",
       "\n",
       "                                organization               type        date  \\\n",
       "0                                     take 3  instagram caption  2017-04-07   \n",
       "1   global alliance against marine pollution  instagram caption  2018-09-04   \n",
       "2      plastic pollution coalition australia      press release  2021-05-25   \n",
       "3                                     take 3      press release  2016-12-30   \n",
       "4                             rameau project      press release  2022-04-15   \n",
       "5                     world surfing reserves      press release  2019-03-12   \n",
       "6                          upcycle the gyres      press release  2019-10-26   \n",
       "7                    plastic soup foundation  instagram caption  2022-03-31   \n",
       "8                              adopt a beach      press release  2015-04-20   \n",
       "9                          beach nourishment      press release  2017-10-07   \n",
       "10                        sea change project  instagram caption  2020-04-02   \n",
       "11                             coastal watch      press release  2016-09-18   \n",
       "12                                 unite bvi      press release  2022-12-11   \n",
       "13                               reef relief      press release  2020-01-11   \n",
       "14                       rise above plastics      press release  2022-06-25   \n",
       "\n",
       "         unit  weight1                                              item1  \\\n",
       "0   kilograms      200                                        spread tubs   \n",
       "1      pounds      100                                              trash   \n",
       "2   kilograms      386                                      glass bottles   \n",
       "3         kgs      332                                        tupperwares   \n",
       "4         lbs      168                                            plastic   \n",
       "5         kgs       49                              other plastic bottles   \n",
       "6         lbs      300                                              trash   \n",
       "7         lbs      240                                              trash   \n",
       "8         kgs      284  aluminium blister packs, disposable food conta...   \n",
       "9   kilograms      110                                              trash   \n",
       "10     pounds      437                                      drink cartons   \n",
       "11  kilograms      375                                              trash   \n",
       "12        lbs      100                                            plastic   \n",
       "13     pounds      220                                        spread tubs   \n",
       "14     pounds      490                                 corrugated cartons   \n",
       "\n",
       "                                               prompt  \\\n",
       "0   Generate an instagram caption for a beach clea...   \n",
       "1   Generate an instagram caption for a beach clea...   \n",
       "2   Generate a press release for a beach cleanup w...   \n",
       "3   Generate a press release for a beach cleanup w...   \n",
       "4   Generate a press release for a beach cleanup w...   \n",
       "5   Generate a press release for a beach cleanup w...   \n",
       "6   Generate a press release for a beach cleanup w...   \n",
       "7   Generate an instagram caption for a beach clea...   \n",
       "8   Generate a press release for a beach cleanup w...   \n",
       "9   Generate a press release for a beach cleanup w...   \n",
       "10  Generate an instagram caption for a beach clea...   \n",
       "11  Generate a press release for a beach cleanup w...   \n",
       "12  Generate a press release for a beach cleanup w...   \n",
       "13  Generate a press release for a beach cleanup w...   \n",
       "14  Generate a press release for a beach cleanup w...   \n",
       "\n",
       "                                                 text  weight2        item2  \\\n",
       "0   It was inspiring to witness so many people com...     <NA>          nan   \n",
       "1   We just made a huge difference at Grahams Beac...     <NA>          nan   \n",
       "2   \\nPlastic Pollution Coalition Australia (PPCA)...     <NA>          nan   \n",
       "3   Take 3 Celebrates a Successful Beach Cleanup i...     <NA>          nan   \n",
       "4   FOR IMMEDIATE RELEASE\\n\\nThe Rameau Project Ce...     <NA>          nan   \n",
       "5   \\nFOR IMMEDIATE RELEASE\\n\\nWORLD SURFING RESER...     <NA>          nan   \n",
       "6   Mayfield Beach Cleaned Up of 300 lbs of Trash ...     <NA>          nan   \n",
       "7   Today, our @plasticsoupfoundation team collect...     <NA>          nan   \n",
       "8   \\nAdopt a Beach Cleanup results in 284 kgs of ...     <NA>          nan   \n",
       "9   SOUTH SANDY BEACH – On October 7, 2017, Beach ...     <NA>          nan   \n",
       "10  Today, Sea Change Project volunteers gathered ...     <NA>          nan   \n",
       "11  Coastal Watch Celebrates Successful Beach Clea...     <NA>          nan   \n",
       "12  Unite BVI Achieves Incredible 100-Pound Plasti...     <NA>          nan   \n",
       "13  \\nReef Relief Hosts Successful Beach Cleanup a...      257  pizza boxes   \n",
       "14  \\nFOR IMMEDIATE RELEASE\\n\\nRise Above Plastics...     <NA>          nan   \n",
       "\n",
       "                                           text_split  \\\n",
       "0   [It, was, inspiring, to, witness, so, many, pe...   \n",
       "1   [We, just, made, a, huge, difference, at, Grah...   \n",
       "2   [Plastic, Pollution, Coalition, Australia, (, ...   \n",
       "3   [Take, 3, Celebrates, a, Successful, Beach, Cl...   \n",
       "4   [FOR, IMMEDIATE, RELEASE, The, Rameau, Project...   \n",
       "5   [FOR, IMMEDIATE, RELEASE, WORLD, SURFING, RESE...   \n",
       "6   [Mayfield, Beach, Cleaned, Up, of, 300, lbs, o...   \n",
       "7   [Today, ,, our, @, plasticsoupfoundation, team...   \n",
       "8   [Adopt, a, Beach, Cleanup, results, in, 284, k...   \n",
       "9   [SOUTH, SANDY, BEACH, –, On, October, 7, ,, 20...   \n",
       "10  [Today, ,, Sea, Change, Project, volunteers, g...   \n",
       "11  [Coastal, Watch, Celebrates, Successful, Beach...   \n",
       "12  [Unite, BVI, Achieves, Incredible, 100-Pound, ...   \n",
       "13  [Reef, Relief, Hosts, Successful, Beach, Clean...   \n",
       "14  [FOR, IMMEDIATE, RELEASE, Rise, Above, Plastic...   \n",
       "\n",
       "                            org_no_space                         loc_no_space  \\\n",
       "0                                  take3                           adamsrocks   \n",
       "1   globalallianceagainstmarinepollution                         grahamsbeach   \n",
       "2     plasticpollutioncoalitionaustralia                        norfolkisland   \n",
       "3                                  take3                  playagrandedesaboga   \n",
       "4                          rameauproject                   playadechachalacas   \n",
       "5                   worldsurfingreserves  chokecanyonstatepark-southshoreunit   \n",
       "6                        upcyclethegyres                        mayfieldbeach   \n",
       "7                  plasticsoupfoundation              prescottbeachcountypark   \n",
       "8                            adoptabeach                         thessaloníki   \n",
       "9                       beachnourishment                      southsandybeach   \n",
       "10                      seachangeproject                         mounthordern   \n",
       "11                          coastalwatch           playafluvialfuentedelprior   \n",
       "12                              unitebvi                         vauréal,pico   \n",
       "13                            reefrelief                            junopeaks   \n",
       "14                     riseaboveplastics                             hüblpeak   \n",
       "\n",
       "                                            date_vars  \\\n",
       "0   [2017-04-07, April  7, 2017,  7 Apr 2017, 04/0...   \n",
       "1   [2018-09-04, September  4, 2018,  4 Sep 2018, ...   \n",
       "2   [2021-05-25, May 25, 2021, 25 May 2021, 05/25/...   \n",
       "3   [2016-12-30, December 30, 2016, 30 Dec 2016, 1...   \n",
       "4   [2022-04-15, April 15, 2022, 15 Apr 2022, 04/1...   \n",
       "5   [2019-03-12, March 12, 2019, 12 Mar 2019, 03/1...   \n",
       "6   [2019-10-26, October 26, 2019, 26 Oct 2019, 10...   \n",
       "7   [2022-03-31, March 31, 2022, 31 Mar 2022, 03/3...   \n",
       "8   [2015-04-20, April 20, 2015, 20 Apr 2015, 04/2...   \n",
       "9   [2017-10-07, October  7, 2017,  7 Oct 2017, 10...   \n",
       "10  [2020-04-02, April  2, 2020,  2 Apr 2020, 04/0...   \n",
       "11  [2016-09-18, September 18, 2016, 18 Sep 2016, ...   \n",
       "12  [2022-12-11, December 11, 2022, 11 Dec 2022, 1...   \n",
       "13  [2020-01-11, January 11, 2020, 11 Jan 2020, 01...   \n",
       "14  [2022-06-25, June 25, 2022, 25 Jun 2022, 06/25...   \n",
       "\n",
       "                      weight1_text                 weight2_text  \n",
       "0                      two hundred                         <na>  \n",
       "1                      one hundred                         <na>  \n",
       "2     three hundred and eighty-six                         <na>  \n",
       "3     three hundred and thirty-two                         <na>  \n",
       "4      one hundred and sixty-eight                         <na>  \n",
       "5                       forty-nine                         <na>  \n",
       "6                    three hundred                         <na>  \n",
       "7            two hundred and forty                         <na>  \n",
       "8      two hundred and eighty-four                         <na>  \n",
       "9              one hundred and ten                         <na>  \n",
       "10   four hundred and thirty-seven                         <na>  \n",
       "11  three hundred and seventy-five                         <na>  \n",
       "12                     one hundred                         <na>  \n",
       "13          two hundred and twenty  two hundred and fifty-seven  \n",
       "14         four hundred and ninety                         <na>  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider using Stanza sentence tokenizer instead of nltk word_tokenizer? See csv_to_BIO_stanzaTokenize.ipynb file\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: separate_weight_unit(x))\n",
    "df['text_split'] = df['text'].apply(lambda x: x.strip())\n",
    "df['text_split'] = df['text_split'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Remove spaces from orgs and locations \n",
    "df['org_no_space'] = df['organization'].apply(lambda x: remove_spaces(x))\n",
    "df['loc_no_space'] = df['location'].apply(lambda x: remove_spaces(x))\n",
    "\n",
    "# Assign appropriate types\n",
    "string_cols = [\"item1\", \"item2\", \"location\"]\n",
    "df[string_cols] = df[string_cols].astype(str)\n",
    "df['weight2'] = df['weight2'].astype('Int64')\n",
    "\n",
    "# Compute variations of date and weight formats\n",
    "df['date_vars'] = df['date'].apply(lambda x: date_to_formats(x))\n",
    "df['weight1_text'] = df['weight1'].apply(lambda x: number_to_words(x)[1])\n",
    "df['weight2_text'] = df['weight2'].apply(lambda x: number_to_words(x)[1] if pd.notnull(x) else str(x))\n",
    "\n",
    "# Make string columns lowercase for comparisons\n",
    "lowercase_cols = string_cols + ['organization', 'location', 'org_no_space', 'loc_no_space', 'weight1_text', 'weight2_text']\n",
    "for i in lowercase_cols:\n",
    "    df[i] = df[i].apply(lambda x: x.lower())\n",
    "\n",
    "df.head(15)\n",
    "# df.info()\n",
    "# print(df.iloc[12])\n",
    "# df['text'][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into list of words with associated 'B - entity', 'I - entity' or 'O'\n",
    "# Look at other preprocessing steps in read_datasets function in convert_bn_daffodil\n",
    "\n",
    "units = [\"kilograms\", \"kilogram\", \"kgs\", \"kg\", \"lb\", \"lbs\", \"pounds\", \"pound\"]\n",
    "\n",
    "def assign_entity_types(row):\n",
    "    words = row['text_split']\n",
    "    new_tags = []\n",
    "    prev_item_tag = False\n",
    "\n",
    "    idx = 0\n",
    "    while (idx < len(words)):\n",
    "        loc_length = len(row['location'].split())\n",
    "        org_length = len(row['organization'].split())\n",
    "        weight1_text_length = len(row['weight1_text'].split())\n",
    "        if row['weight2_text'] != None:\n",
    "            weight2_text_length = len(row['weight2_text'].split())\n",
    "        else:\n",
    "            weight2_text_length = -1\n",
    "        \n",
    "        # Assign location labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        # Does not handle extraneous locations not provided in prompt\n",
    "        if ((idx < len(words) - loc_length) and \n",
    "            [x.lower() for x in words[idx : idx + loc_length]] == row['location'].split()):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "            for i in range(1, loc_length):\n",
    "                new_tags.append(\"I-LOC\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['loc_no_space']):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "\n",
    "        # Assign organization labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        elif ((idx < len(words) - org_length) and \n",
    "            [x.lower() for x in words[idx : idx + org_length]] == (row['organization'].lower().split())):\n",
    "            new_tags.append(\"B-ORG\")            # idea for later: tag acronyms for Orgs?\n",
    "            idx += 1                            \n",
    "            for i in range(1, org_length):\n",
    "                new_tags.append(\"I-ORG\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['org_no_space']):\n",
    "            new_tags.append(\"B-ORG\")      \n",
    "            idx += 1\n",
    "            \n",
    "        # Assign unit labels\n",
    "        elif any(words[idx] == word for word in units):   \n",
    "            new_tags.append(\"B-UNT\")\n",
    "            idx += 1\n",
    "        \n",
    "        # Assign weight labels for numeric and text numbers\n",
    "        elif (words[idx] == str(row['weight1']) or (row['weight2'] != None and words[idx] == str(row['weight2']))): \n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "        elif ((idx < len(words) - weight1_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight1_text_length]] == row['weight1_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "        elif ((weight2_text_length > 0) and (idx < len(words) - weight2_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight2_text_length]] == row['weight2_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "\n",
    "        # Assign item labels (dont look for consecutive matches here)\n",
    "        elif (any(words[idx] == word for word in row['item1'].split()) or \n",
    "                                (row['item2'] != None and any(words[idx] == word for word in row['item2'].split()))):\n",
    "            if prev_item_tag: \n",
    "                new_tags.append(\"I-ITM\")\n",
    "            else:\n",
    "                new_tags.append(\"B-ITM\")\n",
    "                prev_item_tag = True\n",
    "            idx += 1\n",
    "        # Open question: How to assign dates? Need to capture all possible date formats?\n",
    "        # Solution: convert golden value dates to datetime objects then use strftime package to generate \n",
    "        # possible text versions of it\n",
    "        # Assign date labels\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            new_tags.append(\"O\")\n",
    "            prev_org_tag = False\n",
    "            prev_unit_tag = False\n",
    "            prev_weight_tag = False\n",
    "            prev_item_tag = False\n",
    "            idx += 1\n",
    "    return list(zip(words, new_tags))\n",
    "\n",
    "df['tagged_entities'] = df.apply(assign_entity_types, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences\n",
    "# df['text_split'][13] = ['I', 'have', 'two', 'hundred', 'and', 'twenty', 'dogs', 'and', 'two', 'hundred', 'and', 'fifty-seven', \n",
    "#                         'cats', 'ugly', 'two', 'hundred', 'and', 'twenty-one']\n",
    "# assign_entity_types(df.iloc[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reef B-ORG\n",
      "Relief I-ORG\n",
      "Juno B-LOC\n",
      "Peaks I-LOC\n",
      "Reef B-ORG\n",
      "Relief I-ORG\n",
      "Juno B-LOC\n",
      "Peaks I-LOC\n",
      "220 B-WEI\n",
      "pounds B-UNT\n",
      "257 B-WEI\n",
      "pounds B-UNT\n",
      "pizza B-ITM\n",
      "boxes I-ITM\n",
      "Reef B-ORG\n",
      "Relief I-ORG\n",
      "Reef B-ORG\n",
      "Relief I-ORG\n",
      "Reef B-ORG\n",
      "Relief I-ORG\n",
      "Reef B-ORG\n",
      "Relief I-ORG\n",
      "Reef B-ORG\n",
      "Relief I-ORG\n",
      "[('Reef', 'B-ORG'), ('Relief', 'I-ORG'), ('Hosts', 'O'), ('Successful', 'O'), ('Beach', 'O'), ('Cleanup', 'O'), ('at', 'O'), ('Juno', 'B-LOC'), ('Peaks', 'I-LOC'), ('Key', 'O'), ('West', 'O'), (',', 'O'), ('FL', 'O'), ('–', 'O'), ('On', 'O'), ('January', 'O'), ('11', 'O'), (',', 'O'), ('2020', 'O'), (',', 'O'), ('Reef', 'B-ORG'), ('Relief', 'I-ORG'), ('hosted', 'O'), ('a', 'O'), ('successful', 'O'), ('beach', 'O'), ('cleanup', 'O'), ('at', 'O'), ('Juno', 'B-LOC'), ('Peaks', 'I-LOC'), ('.', 'O'), ('After', 'O'), ('an', 'O'), ('hour', 'O'), ('and', 'O'), ('a', 'O'), ('half', 'O'), ('of', 'O'), ('cleaning', 'O'), (',', 'O'), ('a', 'O'), ('total', 'O'), ('of', 'O'), ('220', 'B-WEI'), ('pounds', 'B-UNT'), ('of', 'O'), ('Styrofoam', 'O'), ('containers', 'O'), ('and', 'O'), ('257', 'B-WEI'), ('pounds', 'B-UNT'), ('of', 'O'), ('pizza', 'B-ITM'), ('boxes', 'I-ITM'), ('were', 'O'), ('collected', 'O'), ('.', 'O'), ('“', 'O'), ('We', 'O'), ('’', 'O'), ('re', 'O'), ('so', 'O'), ('proud', 'O'), ('of', 'O'), ('the', 'O'), ('hard', 'O'), ('work', 'O'), ('and', 'O'), ('dedication', 'O'), ('that', 'O'), ('our', 'O'), ('volunteers', 'O'), ('put', 'O'), ('in', 'O'), ('to', 'O'), ('make', 'O'), ('this', 'O'), ('event', 'O'), ('a', 'O'), ('success', 'O'), (',', 'O'), ('”', 'O'), ('said', 'O'), ('Reef', 'B-ORG'), ('Relief', 'I-ORG'), ('Executive', 'O'), ('Director', 'O'), ('DeeVon', 'O'), ('Quirolo', 'O'), ('.', 'O'), ('“', 'O'), ('It', 'O'), ('’', 'O'), ('s', 'O'), ('inspiring', 'O'), ('to', 'O'), ('see', 'O'), ('the', 'O'), ('community', 'O'), ('come', 'O'), ('together', 'O'), ('and', 'O'), ('take', 'O'), ('action', 'O'), ('to', 'O'), ('protect', 'O'), ('and', 'O'), ('preserve', 'O'), ('our', 'O'), ('environment.', 'O'), ('”', 'O'), ('Reef', 'B-ORG'), ('Relief', 'I-ORG'), ('is', 'O'), ('a', 'O'), ('non-profit', 'O'), ('organization', 'O'), ('dedicated', 'O'), ('to', 'O'), ('the', 'O'), ('conservation', 'O'), ('and', 'O'), ('protection', 'O'), ('of', 'O'), ('coral', 'O'), ('reef', 'O'), ('ecosystems', 'O'), ('.', 'O'), ('Reef', 'B-ORG'), ('Relief', 'I-ORG'), ('’', 'O'), ('s', 'O'), ('mission', 'O'), ('is', 'O'), ('to', 'O'), ('be', 'O'), ('a', 'O'), ('voice', 'O'), ('for', 'O'), ('the', 'O'), ('reefs', 'O'), ('.', 'O'), ('The', 'O'), ('organization', 'O'), ('hosts', 'O'), ('beach', 'O'), ('cleanup', 'O'), ('events', 'O'), ('throughout', 'O'), ('the', 'O'), ('year', 'O'), ('to', 'O'), ('remove', 'O'), ('debris', 'O'), ('from', 'O'), ('the', 'O'), ('shoreline', 'O'), ('and', 'O'), ('keep', 'O'), ('the', 'O'), ('ocean', 'O'), ('clean', 'O'), ('.', 'O'), ('Reef', 'B-ORG'), ('Relief', 'I-ORG'), ('encourages', 'O'), ('the', 'O'), ('community', 'O'), ('to', 'O'), ('get', 'O'), ('involved', 'O'), ('and', 'O'), ('help', 'O'), ('preserve', 'O'), ('our', 'O'), ('oceans', 'O'), ('by', 'O'), ('participating', 'O'), ('in', 'O'), ('their', 'O'), ('beach', 'O'), ('cleanup', 'O'), ('events', 'O'), ('.', 'O'), ('To', 'O'), ('learn', 'O'), ('more', 'O'), ('about', 'O'), ('Reef', 'B-ORG'), ('Relief', 'I-ORG'), ('and', 'O'), ('their', 'O'), ('efforts', 'O'), (',', 'O'), ('please', 'O'), ('visit', 'O'), ('their', 'O'), ('website', 'O'), ('at', 'O'), ('www.reefrelief.org', 'O'), ('.', 'O')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nReef Relief Hosts Successful Beach Cleanup at Juno Peaks \\n\\nKey West, FL – On January 11, 2020, Reef Relief hosted a successful beach cleanup at Juno Peaks. After an hour and a half of cleaning, a total of 220 pounds of Styrofoam containers and 257 pounds of pizza boxes were collected.\\n\\n“We’re so proud of the hard work and dedication that our volunteers put in to make this event a success,” said Reef Relief Executive Director DeeVon Quirolo. “It’s inspiring to see the community come together and take action to protect and preserve our environment.”\\n\\nReef Relief is a non-profit organization dedicated to the conservation and protection of coral reef ecosystems. Reef Relief’s mission is to be a voice for the reefs.\\n\\nThe organization hosts beach cleanup events throughout the year to remove debris from the shoreline and keep the ocean clean.\\n\\nReef Relief encourages the community to get involved and help preserve our oceans by participating in their beach cleanup events. To learn more about Reef Relief and their efforts, please visit their website at www.reefrelief.org.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review newly assigned non-\"O\" tags\n",
    "SAMPLE_NO = 13\n",
    "for i in df.iloc[SAMPLE_NO]['tagged_entities']:\n",
    "    if i[1] != \"O\":\n",
    "        print(i[0], i[1])\n",
    "\n",
    "print(df['tagged_entities'][SAMPLE_NO])\n",
    "df['text'][SAMPLE_NO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all sentences into a single list of lists (sentences) of word-pairs (word, NER tag)\n",
    "\n",
    "# Open question: this method is not very robust (cross-references stanza tokenizer sentence lengths \n",
    "# against list of original sentence text words, which might not be 1-1).\n",
    "#   Solution: use a find() to search for first word in each sentence, not just blind indexing into paragraph text\n",
    "\n",
    "# Method is not efficient. Maybe could be vectorized (?), but we only have to run this script once\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "df['text_stanza_tokenize'] = df['text'].apply(lambda x: nlp(x))\n",
    "\n",
    "all_sentences = []\n",
    "for i in range(len(df)):\n",
    "    idx = 0\n",
    "    for sentence in df.iloc[i]['text_stanza_tokenize'].sentences:\n",
    "        new_sentence = list(df.iloc[i]['tagged_entities'][idx:idx+len(sentence.words)])\n",
    "        all_sentences.append(new_sentence)\n",
    "        idx += len(sentence.words)\n",
    "\n",
    "# print(all_sentences[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into datasets = (train_sentences, dev_sentences, test_sentences)\n",
    "\n",
    "DEV_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "random.seed(1234)\n",
    "random.shuffle(all_sentences)\n",
    "\n",
    "train_sentences = all_sentences[ : int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT))]\n",
    "dev_sentences = all_sentences[int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT)) : int(len(all_sentences)*(1-TEST_SPLIT))]\n",
    "test_sentences = all_sentences[int(len(all_sentences)*(1-TEST_SPLIT)) : ]\n",
    "\n",
    "# print(len(train_sentences))\n",
    "# print(len(dev_sentences))\n",
    "# print(len(test_sentences))\n",
    "# print(len(all_sentences))\n",
    "\n",
    "datasets = (train_sentences, dev_sentences, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert file and write to JSON file needed for Stanza modelling\n",
    "out_directory = os.getcwd()\n",
    "write_dataset(datasets, out_directory, \"TOC_Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to JSON file needed by Stanza model\n",
    "# There is a conversion script called several times in prepare_ner_dataset.py which converts IOB format to our internal NER format:\n",
    "# import stanza.utils.datasets.ner.prepare_ner_file as prepare_ner_file\n",
    "\n",
    "# prepare_ner_file.process_dataset(input_iob, output_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
