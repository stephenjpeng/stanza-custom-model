{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "from stanza.utils.datasets.ner.utils import write_dataset\n",
    "from transform_weight_date import number_to_words, date_to_formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data to be read in from CSV using output\n",
    "toy_data_path = '../../data/data/generated/data_230124-172021.csv'\n",
    "synthetic_data_1_path = '../../../xplore-the-ocean-cleanup/data-generation/data/generated/data_230221-205202.csv'\n",
    "synthetic_data_2_path = '../../../xplore-the-ocean-cleanup/data-generation/data/generated/data_230222-092039.csv'\n",
    "\n",
    "DATA_SELECTION = \"synth1\"\n",
    "\n",
    "if DATA_SELECTION == \"toy\":\n",
    "    data_path = toy_data_path\n",
    "if DATA_SELECTION == \"synth1\":\n",
    "    data_path = synthetic_data_1_path\n",
    "if DATA_SELECTION == \"synth2\":\n",
    "    data_path = synthetic_data_2_path\n",
    "\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use re to replace any instances of \"####kg\" with \"#### kg\" where #### is any continuous \n",
    "# sequence of numbers and unit is one of those listed below\n",
    "def separate_weight_unit(row):\n",
    "    return re.sub(r'([0-9]+)(kgs|kg|lbs|lb|pounds|kilograms)', r\"\\1 \\2\", row)\n",
    "\n",
    "# Function to remove spaces (e.g. \"Take 3\" -> \"Take3\")\n",
    "def remove_spaces(text):\n",
    "    return text.replace(\" \", \"\")\n",
    "\n",
    "# Function to replace long hyphen ASCII code with short hyphen '-' ASCII code\n",
    "def character_norm(text):\n",
    "    return text.replace(chr(8211), \"-\")\n",
    "\n",
    "# Word tokenizer splits ',' into separate token, so we have this function to do the same\n",
    "def add_comma_token(text):\n",
    "    return text.replace(\",\", \" ,\")\n",
    "\n",
    "# Split '/' into its own token   JOE TO UPDATE THIS TINY EDGE CASE\n",
    "def add_slash_token(text):\n",
    "    return text.replace(chr(47), \" / \")\n",
    "\n",
    "# Word tokenizer splits ',' into separate token, so we have this function to do the same for our dates list\n",
    "def add_date_var_comma_token(list):\n",
    "    new_list = []\n",
    "    for i in list:\n",
    "        new_list.append(add_comma_token(i))\n",
    "    return new_list\n",
    "\n",
    "# Gets the first token of each date variation, to allow for faster downstream computation \n",
    "def get_first_token_set(list):\n",
    "    new_set = set()\n",
    "    for i in list:\n",
    "        new_set.add(i.split()[0])\n",
    "    return new_set\n",
    "\n",
    "df['item1'][186][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>location</th>\n",
       "      <th>organization</th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>unit</th>\n",
       "      <th>weight1</th>\n",
       "      <th>item1</th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "      <th>weight2</th>\n",
       "      <th>item2</th>\n",
       "      <th>text_split</th>\n",
       "      <th>org_no_space</th>\n",
       "      <th>loc_no_space</th>\n",
       "      <th>date_vars</th>\n",
       "      <th>weight1_text</th>\n",
       "      <th>weight2_text</th>\n",
       "      <th>date_vars_first_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>smathers beach</td>\n",
       "      <td>industrial surplus foundation</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2016-10-22</td>\n",
       "      <td>pounds</td>\n",
       "      <td>381</td>\n",
       "      <td>bait bags/containers and foam cups</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>What a productive day at Smathers Beach! We co...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[What, a, productive, day, at, Smathers, Beach...</td>\n",
       "      <td>industrialsurplusfoundation</td>\n",
       "      <td>smathersbeach</td>\n",
       "      <td>[2016-10-22, October 22 , 2016, october 22 , 2...</td>\n",
       "      <td>three hundred and eighty-one</td>\n",
       "      <td></td>\n",
       "      <td>{10-22-2016, 2016-10-22, October, Oct, october...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>benedict beach</td>\n",
       "      <td>independent bakers association inc</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2015-12-15</td>\n",
       "      <td>kilograms</td>\n",
       "      <td>200</td>\n",
       "      <td>plastic</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>Today, the Independent Bakers Association Inc....</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Today, ,, the, Independent, Bakers, Associati...</td>\n",
       "      <td>independentbakersassociationinc</td>\n",
       "      <td>benedictbeach</td>\n",
       "      <td>[2015-12-15, December 15 , 2015, december 15 ,...</td>\n",
       "      <td>two hundred</td>\n",
       "      <td></td>\n",
       "      <td>{15, December, dec, 12-15-2015, 12/15/15, 2015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>wamberal beach</td>\n",
       "      <td>trout unlimited</td>\n",
       "      <td>press release</td>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>lbs</td>\n",
       "      <td>309</td>\n",
       "      <td>plastic</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>TROUT UNLIMITED ANNOUNCES SUCCESSFUL WAMBERAL ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[TROUT, UNLIMITED, ANNOUNCES, SUCCESSFUL, WAMB...</td>\n",
       "      <td>troutunlimited</td>\n",
       "      <td>wamberalbeach</td>\n",
       "      <td>[2015-02-25, February 25 , 2015, february 25 ,...</td>\n",
       "      <td>three hundred and nine</td>\n",
       "      <td></td>\n",
       "      <td>{feb, 02-25-2015, february, February, 02/25/15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>wagner spur</td>\n",
       "      <td>el rey de gloria mision</td>\n",
       "      <td>press release</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>pounds</td>\n",
       "      <td>20</td>\n",
       "      <td>dog poop bags , rope , and glass cups</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>El Rey De Gloria Mision Takes Local Beach Clea...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[El, Rey, De, Gloria, Mision, Takes, Local, Be...</td>\n",
       "      <td>elreydegloriamision</td>\n",
       "      <td>wagnerspur</td>\n",
       "      <td>[2017-06-06, June 6 , 2017, june 6 , 2017, Jun...</td>\n",
       "      <td>twenty</td>\n",
       "      <td></td>\n",
       "      <td>{June, jun, 06-06-2017, Jun, 06/06/17, 6th, 06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>owerri</td>\n",
       "      <td>friends of the upper delaware river</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2016-05-07</td>\n",
       "      <td>units</td>\n",
       "      <td>381</td>\n",
       "      <td>plastic films and foam fragments</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>Friends of the Upper Delaware River made a hug...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Friends, of, the, Upper, Delaware, River, mad...</td>\n",
       "      <td>friendsoftheupperdelawareriver</td>\n",
       "      <td>owerri</td>\n",
       "      <td>[2016-05-07, May 7 , 2016, may 7 , 2016, May 7...</td>\n",
       "      <td>three hundred and eighty-one</td>\n",
       "      <td></td>\n",
       "      <td>{05-07-2016, 7th, May, 2016-05-07, 7, may, 05/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        location                         organization  \\\n",
       "0           0  smathers beach        industrial surplus foundation   \n",
       "1           1  benedict beach   independent bakers association inc   \n",
       "2           2  wamberal beach                      trout unlimited   \n",
       "3           3     wagner spur              el rey de gloria mision   \n",
       "4           4          owerri  friends of the upper delaware river   \n",
       "\n",
       "                type        date       unit  weight1  \\\n",
       "0  instagram caption  2016-10-22     pounds      381   \n",
       "1  instagram caption  2015-12-15  kilograms      200   \n",
       "2      press release  2015-02-25        lbs      309   \n",
       "3      press release  2017-06-06     pounds       20   \n",
       "4  instagram caption  2016-05-07      units      381   \n",
       "\n",
       "                                   item1  \\\n",
       "0     bait bags/containers and foam cups   \n",
       "1                                plastic   \n",
       "2                                plastic   \n",
       "3  dog poop bags , rope , and glass cups   \n",
       "4       plastic films and foam fragments   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Generate an instagram caption for a beach clea...   \n",
       "1  Generate an instagram caption for a beach clea...   \n",
       "2  Generate a press release for a beach cleanup w...   \n",
       "3  Generate a press release for a beach cleanup w...   \n",
       "4  Generate an instagram caption for a beach clea...   \n",
       "\n",
       "                                                text  weight2 item2  \\\n",
       "0  What a productive day at Smathers Beach! We co...     <NA>   nan   \n",
       "1  Today, the Independent Bakers Association Inc....     <NA>   nan   \n",
       "2  TROUT UNLIMITED ANNOUNCES SUCCESSFUL WAMBERAL ...     <NA>   nan   \n",
       "3  El Rey De Gloria Mision Takes Local Beach Clea...     <NA>   nan   \n",
       "4  Friends of the Upper Delaware River made a hug...     <NA>   nan   \n",
       "\n",
       "                                          text_split  \\\n",
       "0  [What, a, productive, day, at, Smathers, Beach...   \n",
       "1  [Today, ,, the, Independent, Bakers, Associati...   \n",
       "2  [TROUT, UNLIMITED, ANNOUNCES, SUCCESSFUL, WAMB...   \n",
       "3  [El, Rey, De, Gloria, Mision, Takes, Local, Be...   \n",
       "4  [Friends, of, the, Upper, Delaware, River, mad...   \n",
       "\n",
       "                      org_no_space   loc_no_space  \\\n",
       "0      industrialsurplusfoundation  smathersbeach   \n",
       "1  independentbakersassociationinc  benedictbeach   \n",
       "2                   troutunlimited  wamberalbeach   \n",
       "3              elreydegloriamision     wagnerspur   \n",
       "4   friendsoftheupperdelawareriver         owerri   \n",
       "\n",
       "                                           date_vars  \\\n",
       "0  [2016-10-22, October 22 , 2016, october 22 , 2...   \n",
       "1  [2015-12-15, December 15 , 2015, december 15 ,...   \n",
       "2  [2015-02-25, February 25 , 2015, february 25 ,...   \n",
       "3  [2017-06-06, June 6 , 2017, june 6 , 2017, Jun...   \n",
       "4  [2016-05-07, May 7 , 2016, may 7 , 2016, May 7...   \n",
       "\n",
       "                   weight1_text weight2_text  \\\n",
       "0  three hundred and eighty-one                \n",
       "1                   two hundred                \n",
       "2        three hundred and nine                \n",
       "3                        twenty                \n",
       "4  three hundred and eighty-one                \n",
       "\n",
       "                               date_vars_first_token  \n",
       "0  {10-22-2016, 2016-10-22, October, Oct, october...  \n",
       "1  {15, December, dec, 12-15-2015, 12/15/15, 2015...  \n",
       "2  {feb, 02-25-2015, february, February, 02/25/15...  \n",
       "3  {June, jun, 06-06-2017, Jun, 06/06/17, 6th, 06...  \n",
       "4  {05-07-2016, 7th, May, 2016-05-07, 7, may, 05/...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign appropriate types\n",
    "string_cols = [\"item1\", \"item2\", \"location\", \"organization\", \"date\"]\n",
    "df[string_cols] = df[string_cols].astype(str)\n",
    "int_cols = [\"weight1\", \"weight2\"]\n",
    "for i in int_cols:\n",
    "    df[i] = df[i].astype('Int64')\n",
    "\n",
    "# Normalize text columns to match tokenizer \n",
    "df['text'] = df['text'].apply(lambda x: separate_weight_unit(x))\n",
    "for i in string_cols:\n",
    "    df[i] = df[i].apply(lambda x: character_norm(x))\n",
    "df['text'] = df['text'].apply(lambda x: x.strip())\n",
    "\n",
    "# Tokenize text\n",
    "df['text_split'] = df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Preprocess orgs and locations \n",
    "df['org_no_space'] = df['organization'].apply(lambda x: remove_spaces(x))\n",
    "df['loc_no_space'] = df['location'].apply(lambda x: remove_spaces(x))\n",
    "\n",
    "# Preprocess ',' and '/' tokens\n",
    "for i in string_cols:\n",
    "    df[i] = df[i].apply(lambda x: add_comma_token(x))\n",
    "    if i in ['item1', 'item2']:\n",
    "        df[i].apply(lambda x: add_slash_token(x))\n",
    "\n",
    "# Compute variations of date and weight formats and preprocess into desired formats\n",
    "df['date_vars'] = df['date'].apply(lambda x: date_to_formats(x) if x != 'nan' else str(x))\n",
    "df['weight1_text'] = df['weight1'].apply(lambda x: number_to_words(x)[1] if pd.notnull(x) else \"\")\n",
    "df['weight2_text'] = df['weight2'].apply(lambda x: number_to_words(x)[1] if pd.notnull(x) else \"\")\n",
    "df['date_vars'] = df['date_vars'].apply(lambda x: add_date_var_comma_token(x))\n",
    "df['date_vars_first_token'] = df['date_vars'].apply(lambda x: get_first_token_set(x))\n",
    "\n",
    "# Make string columns lowercase for downstream comparisons\n",
    "lowercase_cols = string_cols + ['organization', 'org_no_space', 'loc_no_space', 'weight1_text', 'weight2_text']\n",
    "for i in lowercase_cols:\n",
    "    df[i] = df[i].apply(lambda x: x.lower())\n",
    "\n",
    "df.head()\n",
    "# df.info()\n",
    "# print(df.iloc[12])\n",
    "# df['prompt'][13]\n",
    "# df['date_vars_first_token'][13]\n",
    "# df[df['weight2'] != \"\"].iloc[0:4]\n",
    "# df[df['weight1'] == 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into list of words with associated 'B - entity', 'I - entity' or 'O'\n",
    "\n",
    "units = set([\"kilograms\", \"kilogram\", \"kgs\", \"kg\", \"lb\", \"lbs\", \"pounds\", \"pound\"])\n",
    "filler_words = set([\"and\", \"the\", \"a\", \"an\", \",\", \"/\"])\n",
    "\n",
    "def assign_entity_types(row):\n",
    "    words = row['text_split']\n",
    "    new_tags = []\n",
    "    prev_item_tag = False\n",
    "\n",
    "    idx = 0\n",
    "    while (idx < len(words)):\n",
    "        loc_length = len(row['location'].split())\n",
    "        org_length = len(row['organization'].split())\n",
    "        weight1_text_length = len(row['weight1_text'].split())\n",
    "        if row['weight2_text'] != None:\n",
    "            weight2_text_length = len(row['weight2_text'].split())\n",
    "        else:\n",
    "            weight2_text_length = -1\n",
    "        \n",
    "        # Assign location labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        # Does not handle extraneous locations not provided in prompt!\n",
    "        if ((idx <= len(words) - loc_length) and \n",
    "            [x.lower() for x in words[idx : idx + loc_length]] == row['location'].split()):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "            for i in range(1, loc_length):\n",
    "                new_tags.append(\"I-LOC\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['loc_no_space']):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "\n",
    "        # Assign organization labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        elif ((idx <= len(words) - org_length) and \n",
    "            [x.lower() for x in words[idx : idx + org_length]] == (row['organization'].lower().split())):\n",
    "            new_tags.append(\"B-ORG\")            # idea for later: tag acronyms for Orgs?\n",
    "            idx += 1                            \n",
    "            for i in range(1, org_length):\n",
    "                new_tags.append(\"I-ORG\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['org_no_space']):\n",
    "            new_tags.append(\"B-ORG\")      \n",
    "            idx += 1\n",
    "            \n",
    "        # Assign unit labels\n",
    "        elif words[idx] in units:   \n",
    "            new_tags.append(\"B-UNT\")\n",
    "            idx += 1\n",
    "        \n",
    "        # Assign weight labels for numeric and text numbers (consider '-' and non- '-' versions of written numbers?)\n",
    "        elif (words[idx] == str(row['weight1']) or \n",
    "            (not pd.isna(row['weight2']) and words[idx] == str(row['weight2']))): \n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "        elif (not pd.isna(row['weight1']) and (idx <= len(words) - weight1_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight1_text_length]] == row['weight1_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "        elif ((weight2_text_length > 0) and (idx <= len(words) - weight2_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight2_text_length]] == row['weight2_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "\n",
    "        # Assign item labels (dont look for consecutive matches here)\n",
    "        # Does not handle extraneous trash items not provided in prompt!\n",
    "        elif ((any(words[idx] == word for word in row['item1'].split()) or \n",
    "             (row['item2'] != None and any(words[idx] == word for word in row['item2'].split()))) and\n",
    "             words[idx] not in filler_words):\n",
    "            if prev_item_tag: \n",
    "                new_tags.append(\"I-ITM\")\n",
    "            else:\n",
    "                new_tags.append(\"B-ITM\")\n",
    "                prev_item_tag = True\n",
    "            idx += 1\n",
    "        \n",
    "        # Assign date labels (check only first token to minimize computation on each word)\n",
    "        elif (words[idx] in row['date_vars_first_token']):\n",
    "            # Check for complete consecutive match with any of the possible date variations \n",
    "            date_found = False\n",
    "            for date_var in row['date_vars']:\n",
    "                if ((idx <= len(words) - len(date_var.split())) and \n",
    "                    [x.lower() for x in words[idx : idx + len(date_var.split())]] == date_var.lower().split()):\n",
    "                    new_tags.append(\"B-DAT\")\n",
    "                    idx += 1\n",
    "                    for i in range(1, len(date_var.split())):\n",
    "                        new_tags.append(\"I-DAT\")\n",
    "                        idx += 1\n",
    "                    date_found = True\n",
    "                    break\n",
    "            # If the text matches with none of the date_vars, we need to append \"O\"\n",
    "            if not date_found:\n",
    "                new_tags.append(\"O\")\n",
    "                prev_item_tag = False\n",
    "                idx += 1\n",
    "        \n",
    "        else:\n",
    "            new_tags.append(\"O\")\n",
    "            prev_item_tag = False\n",
    "            idx += 1\n",
    "\n",
    "    return list(zip(words, new_tags))\n",
    "\n",
    "df['tagged_entities'] = df.apply(assign_entity_types, axis =1)\n",
    "# assign_entity_types(df.iloc[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences\n",
    "# df['text_split'][13] = ['I', 'have', 'two', 'hundred', 'and', 'twenty', 'dogs', 'and', 'two', 'hundred', 'and', 'fifty-seven', \n",
    "#                         'cats', 'ugly', 'two', 'hundred', 'and', 'twenty-one', 'on', '11', 'Jan', '2020', '01-11-2020']\n",
    "# assign_entity_types(df.iloc[13])\n",
    "# print(df['text_split'][13][22] in df['date_vars_first_token'][13])\n",
    "# print('01-11-2020'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Al B-LOC\n",
      "Marj I-LOC\n",
      "70 B-WEI\n",
      "lbs B-UNT\n",
      "paper/wood fragments/pieces\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Today at Al Marj Beach, the Reuse Center of the Treasure Coast Inc. made a huge difference by cleaning up 70 lbs of paper, wood fragments, and pieces! #protectourplanet #cleanup #treasurecoast #reusecenter #beachcleanup'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review newly assigned non-\"O\" tags\n",
    "SAMPLE_NO = 186\n",
    "for i in df.iloc[SAMPLE_NO]['tagged_entities']:\n",
    "    if i[1] != \"O\":\n",
    "        print(i[0], i[1])\n",
    "\n",
    "# print(df['tagged_entities'][SAMPLE_NO])\n",
    "print(df['item1'][SAMPLE_NO])\n",
    "df['text'][SAMPLE_NO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 15:48:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be01ff8c3fe441782ba95d79ca3ad76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 15:48:59 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2023-02-23 15:48:59 INFO: Use device: cpu\n",
      "2023-02-23 15:48:59 INFO: Loading: tokenize\n",
      "2023-02-23 15:48:59 INFO: Done loading processors!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [59], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use stanza tokenizer to determine sentence chunks \u001b[39;00m\n\u001b[1;32m      2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m stanza\u001b[38;5;241m.\u001b[39mPipeline(lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_stanza_tokenize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1157\u001b[0m             values,\n\u001b[1;32m   1158\u001b[0m             f,\n\u001b[1;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn [59], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use stanza tokenizer to determine sentence chunks \u001b[39;00m\n\u001b[1;32m      2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m stanza\u001b[38;5;241m.\u001b[39mPipeline(lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_stanza_tokenize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stanza/pipeline/core.py:408\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, doc, processors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 408\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess(doc, processors)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stanza/pipeline/core.py:397\u001b[0m, in \u001b[0;36mPipeline.process\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors\u001b[39m.\u001b[39mget(processor_name):\n\u001b[1;32m    396\u001b[0m         process \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name]\u001b[39m.\u001b[39mbulk_process \u001b[39mif\u001b[39;00m bulk \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name]\u001b[39m.\u001b[39mprocess\n\u001b[0;32m--> 397\u001b[0m         doc \u001b[39m=\u001b[39m process(doc)\n\u001b[1;32m    398\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stanza/pipeline/tokenize_processor.py:87\u001b[0m, in \u001b[0;36mTokenizeProcessor.process\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     85\u001b[0m batches \u001b[39m=\u001b[39m TokenizationDataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, input_text\u001b[39m=\u001b[39mraw_text, vocab\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab, evaluation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, dictionary\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mdictionary)\n\u001b[1;32m     86\u001b[0m \u001b[39m# get dict data\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m _, _, _, document \u001b[39m=\u001b[39m output_predictions(\u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer, batches, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab, \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     88\u001b[0m                                        \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mmax_seqlen\u001b[39;49m\u001b[39m'\u001b[39;49m, TokenizeProcessor\u001b[39m.\u001b[39;49mMAX_SEQ_LENGTH_DEFAULT),\n\u001b[1;32m     89\u001b[0m                                        orig_text\u001b[39m=\u001b[39;49mraw_text,\n\u001b[1;32m     90\u001b[0m                                        no_ssplit\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mno_ssplit\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m     91\u001b[0m                                        num_workers \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mnum_workers\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m0\u001b[39;49m))\n\u001b[1;32m     92\u001b[0m \u001b[39mreturn\u001b[39;00m doc\u001b[39m.\u001b[39mDocument(document, raw_text)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stanza/models/tokenization/utils.py:264\u001b[0m, in \u001b[0;36moutput_predictions\u001b[0;34m(output_file, trainer, data_generator, vocab, mwt_dict, max_seqlen, orig_text, no_ssplit, use_regex_tokens, num_workers)\u001b[0m\n\u001b[1;32m    261\u001b[0m     all_raw\u001b[39m.\u001b[39mappend(\u001b[39mlist\u001b[39m(paragraph))\n\u001b[1;32m    263\u001b[0m \u001b[39mif\u001b[39;00m N \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m max_seqlen:\n\u001b[0;32m--> 264\u001b[0m     pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(trainer\u001b[39m.\u001b[39;49mpredict(batch), axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    265\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     \u001b[39m# TODO: we could shortcircuit some processing of\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     \u001b[39m# long strings of PAD by tracking which rows are finished\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     idx \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m num_sentences\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use stanza tokenizer to determine sentence chunks \n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "df['text_stanza_tokenize'] = df['text'].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences tagged:  4819\n"
     ]
    }
   ],
   "source": [
    "# Compiles all sentences into a single list of lists (sentences) of word-pairs (word, NER tag)\n",
    "def get_all_sentences(df):\n",
    "    all_sentences = []\n",
    "    for i in range(len(df)):\n",
    "        idx = 0\n",
    "        for sentence in df.iloc[i]['text_stanza_tokenize'].sentences:\n",
    "            # Check for first word in stanza-tokenized sentence and adjust index within small range to correct\n",
    "            # starting word (Problem: may result in 1 or 2 tokens being truncated from front or end of sentences, \n",
    "            # though this adjustment doesn't happen in every document and only 2-3 times per document when it does)\n",
    "            first_word = sentence.tokens[0].text\n",
    "            try:\n",
    "                if (first_word != df.iloc[i]['tagged_entities'][idx][0]):\n",
    "                    for adj in [-2, -1, 1, 2]:\n",
    "                        if (first_word == df.iloc[i]['tagged_entities'][idx + adj][0]):\n",
    "                            idx = idx + adj\n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "            end_sentence_limit = min(idx+len(sentence.words), len(df.iloc[i]['tagged_entities'])-1)\n",
    "            new_sentence = list(df.iloc[i]['tagged_entities'][idx:end_sentence_limit])\n",
    "            all_sentences.append(new_sentence)\n",
    "            idx += len(sentence.words)\n",
    "    return all_sentences\n",
    "\n",
    "# get_all_sentences(df.iloc[38])\n",
    "all_sentences = get_all_sentences(df)\n",
    "print(\"# of sentences tagged: \", len(all_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into datasets = (train_sentences, dev_sentences, test_sentences)\n",
    "\n",
    "DEV_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "random.seed(1234)\n",
    "random.shuffle(all_sentences)\n",
    "\n",
    "train_sentences = all_sentences[ : int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT))]\n",
    "dev_sentences = all_sentences[int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT)) : int(len(all_sentences)*(1-TEST_SPLIT))]\n",
    "test_sentences = all_sentences[int(len(all_sentences)*(1-TEST_SPLIT)) : ]\n",
    "\n",
    "# print(len(train_sentences))\n",
    "# print(len(dev_sentences))\n",
    "# print(len(test_sentences))\n",
    "# print(len(all_sentences))\n",
    "\n",
    "datasets = (train_sentences, dev_sentences, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.train.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.train.json\n",
      "3825 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.train.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.train.json\n",
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.dev.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.dev.json\n",
      "479 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.dev.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.dev.json\n",
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.test.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.test.json\n",
      "480 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.test.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.test.json\n"
     ]
    }
   ],
   "source": [
    "# Convert file and write to JSON file needed for Stanza modelling\n",
    "out_directory = os.getcwd() + '/Processed_Data'\n",
    "write_dataset(datasets, out_directory, DATA_SELECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
