{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import stanza\n",
    "from stanza.utils.datasets.ner.utils import write_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in synthetic data\n",
    "df = pd.read_csv('../../data/data/generated/data_230124-172021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use re to replace any instances of \"####kg\" with \"#### kg\" where #### is any continuous \n",
    "# sequence of numbers and unit is one of those listed below\n",
    "def separate_weight_unit(row):\n",
    "    return re.sub(r'([0-9]+)(kgs|kg|lbs|lb|pounds|kilograms)', r\"\\1 \\2\", row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>organization</th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>unit</th>\n",
       "      <th>weight1</th>\n",
       "      <th>item1</th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "      <th>weight2</th>\n",
       "      <th>item2</th>\n",
       "      <th>text_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams Rocks</td>\n",
       "      <td>Take 3</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2017-04-07</td>\n",
       "      <td>kilograms</td>\n",
       "      <td>200</td>\n",
       "      <td>spread tubs</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>It was inspiring to witness so many people com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>[It, was, inspiring, to, witness, so, many, pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grahams Beach</td>\n",
       "      <td>Global Alliance Against Marine Pollution</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2018-09-04</td>\n",
       "      <td>pounds</td>\n",
       "      <td>100</td>\n",
       "      <td>trash</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>We just made a huge difference at Grahams Beac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>[We, just, made, a, huge, difference, at, Grah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Norfolk Island</td>\n",
       "      <td>Plastic Pollution Coalition Australia</td>\n",
       "      <td>press release</td>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>kilograms</td>\n",
       "      <td>386</td>\n",
       "      <td>glass bottles</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>\\nPlastic Pollution Coalition Australia (PPCA)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Plastic, Pollution, Coalition, Australia, (, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Playa Grande de Saboga</td>\n",
       "      <td>Take 3</td>\n",
       "      <td>press release</td>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>kgs</td>\n",
       "      <td>332</td>\n",
       "      <td>tupperwares</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>Take 3 Celebrates a Successful Beach Cleanup i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Take, 3, Celebrates, a, Successful, Beach, Cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Playa de Chachalacas</td>\n",
       "      <td>Rameau Project</td>\n",
       "      <td>press release</td>\n",
       "      <td>2022-04-15</td>\n",
       "      <td>lbs</td>\n",
       "      <td>168</td>\n",
       "      <td>plastic</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE\\n\\nThe Rameau Project Ce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>[FOR, IMMEDIATE, RELEASE, The, Rameau, Project...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 location                              organization  \\\n",
       "0             Adams Rocks                                    Take 3   \n",
       "1           Grahams Beach  Global Alliance Against Marine Pollution   \n",
       "2          Norfolk Island     Plastic Pollution Coalition Australia   \n",
       "3  Playa Grande de Saboga                                    Take 3   \n",
       "4    Playa de Chachalacas                            Rameau Project   \n",
       "\n",
       "                type        date       unit  weight1          item1  \\\n",
       "0  instagram caption  2017-04-07  kilograms      200    spread tubs   \n",
       "1  instagram caption  2018-09-04     pounds      100          trash   \n",
       "2      press release  2021-05-25  kilograms      386  glass bottles   \n",
       "3      press release  2016-12-30        kgs      332    tupperwares   \n",
       "4      press release  2022-04-15        lbs      168        plastic   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Generate an instagram caption for a beach clea...   \n",
       "1  Generate an instagram caption for a beach clea...   \n",
       "2  Generate a press release for a beach cleanup w...   \n",
       "3  Generate a press release for a beach cleanup w...   \n",
       "4  Generate a press release for a beach cleanup w...   \n",
       "\n",
       "                                                text  weight2 item2  \\\n",
       "0  It was inspiring to witness so many people com...      NaN   nan   \n",
       "1  We just made a huge difference at Grahams Beac...      NaN   nan   \n",
       "2  \\nPlastic Pollution Coalition Australia (PPCA)...      NaN   nan   \n",
       "3  Take 3 Celebrates a Successful Beach Cleanup i...      NaN   nan   \n",
       "4  FOR IMMEDIATE RELEASE\\n\\nThe Rameau Project Ce...      NaN   nan   \n",
       "\n",
       "                                          text_split  \n",
       "0  [It, was, inspiring, to, witness, so, many, pe...  \n",
       "1  [We, just, made, a, huge, difference, at, Grah...  \n",
       "2  [Plastic, Pollution, Coalition, Australia, (, ...  \n",
       "3  [Take, 3, Celebrates, a, Successful, Beach, Cl...  \n",
       "4  [FOR, IMMEDIATE, RELEASE, The, Rameau, Project...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider using Stanza sentence tokenizer instead of nltk word_tokenizer? See csv_to_BIO_stanzaTokenize.ipynb file\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: separate_weight_unit(x))\n",
    "df['text_split'] = df['text'].apply(lambda x: x.strip())\n",
    "df['text_split'] = df['text_split'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "\n",
    "string_cols = [\"item1\", \"item2\", ]\n",
    "df[string_cols] = df[string_cols].astype(str)\n",
    "df.head()\n",
    "# df.info()\n",
    "# df['text_split'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into list of words with associated 'B - entity', 'I - entity' or 'O'\n",
    "# Look at other preprocessing steps in read_datasets function in convert_bn_daffodil\n",
    "\n",
    "units = [\"kilograms\", \"kilogram\", \"kgs\", \"kg\", \"lb\", \"lbs\", \"pounds\", \"pound\"]\n",
    "\n",
    "def assign_entity_types(row):\n",
    "    words = row['text_split']\n",
    "    new_tags = []\n",
    "    prev_org_tag = False\n",
    "    prev_loc_tag = False\n",
    "    prev_unit_tag = False\n",
    "    prev_weight_tag = False\n",
    "    prev_item_tag = False\n",
    "    \n",
    "    idx = 0\n",
    "    while (idx < len(words)):\n",
    "        # Assign location labels\n",
    "        \n",
    "        # Problem: only searches for specific location that was \n",
    "        # given in the prompt, not other locations that GPT-3 produces in text\n",
    "        # Solution: ignore these extraneous locations, they're not our targets\n",
    "        # Need to assign only consecutive matches for locations (calc len(location) and check if\n",
    "        # subsequent words all equal the location. If they do, append all the words with appropriate \n",
    "        # B and I tags. Convert to while loop instead of for and add length of\n",
    "        # location at end of while?)\n",
    "        # make the check for equality with all lower cases?\n",
    "        \n",
    "        loc_length = len(row['location'].split())\n",
    "        # Check for consecutive word matching for full location name\n",
    "        if (idx < len(words) - loc_length and words[idx : idx + loc_length] == row['location'].split()):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "            for i in range(1, loc_length):\n",
    "                new_tags.append(\"I-LOC\")\n",
    "                idx += 1\n",
    "        # Assign organization labels\n",
    "        elif any(words[idx] == word for word in row['organization'].split()):\n",
    "            if prev_org_tag:                    # I should lowercase everything when checking orgs\n",
    "                                                # Need to assign only consecutive matches for orgs\n",
    "                                                # Check for edge case of orgs without spaces (e.g. @take3 should get tagged)\n",
    "                                                # idea for later: acronyms for Orgs?\n",
    "                new_tags.append(\"I-ORG\")\n",
    "            else:\n",
    "                new_tags.append(\"B-ORG\")\n",
    "                prev_org_tag = True\n",
    "            idx += 1\n",
    "        # Assign unit labels\n",
    "        elif any(words[idx] == word for word in units):   #Problem: some texts might include \"two hundred\" instead of 200 \n",
    "                                                        #Solution: word2num and num2word packages\n",
    "            if prev_unit_tag: \n",
    "                new_tags.append(\"I-UNT\")\n",
    "            else:\n",
    "                new_tags.append(\"B-UNT\")\n",
    "                prev_unit_tag = True\n",
    "            idx += 1\n",
    "        # Assign weight labels \n",
    "        elif (words[idx] == str(row['weight1']) or (row['weight2'] != None and words[idx] == str(row['weight2']))):\n",
    "            if prev_weight_tag: \n",
    "                new_tags.append(\"I-WEI\")\n",
    "            else:\n",
    "                new_tags.append(\"B-WEI\")\n",
    "                prev_weight_tag = True\n",
    "            idx += 1\n",
    "        # Assign item labels (dont look for consecutive matches here)\n",
    "        elif (any(words[idx] == word for word in row['item1'].split()) or \n",
    "                                (row['item2'] != None and any(words[idx] == word for word in row['item2'].split()))):\n",
    "            if prev_item_tag: \n",
    "                new_tags.append(\"I-ITM\")\n",
    "            else:\n",
    "                new_tags.append(\"B-ITM\")\n",
    "                prev_item_tag = True\n",
    "            idx += 1\n",
    "        # Open question: How to assign dates? Need to capture all possible date formats?\n",
    "        # Solution: convert golden value dates to datetime objects then use strftime package to generate \n",
    "        # possible text versions of it\n",
    "        else:\n",
    "            new_tags.append(\"O\")\n",
    "            prev_org_tag = False\n",
    "            prev_loc_tag = False\n",
    "            prev_unit_tag = False\n",
    "            prev_weight_tag = False\n",
    "            prev_item_tag = False\n",
    "            idx += 1\n",
    "    return list(zip(words, new_tags))\n",
    "\n",
    "df['tagged_entities'] = df.apply(lambda row : assign_entity_types(row), axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "\n",
    "# df_test = df.iloc[2]\n",
    "# print(df_test)\n",
    "# df_test['tagged_entities'] = assign_entity_types(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adams B-LOC\n",
      "Rocks I-LOC\n",
      "200 B-WEI\n",
      "kgs B-UNT\n",
      "[('It', 'O'), ('was', 'O'), ('inspiring', 'O'), ('to', 'O'), ('witness', 'O'), ('so', 'O'), ('many', 'O'), ('people', 'O'), ('come', 'O'), ('together', 'O'), ('to', 'O'), ('clean', 'O'), ('up', 'O'), ('Adams', 'B-LOC'), ('Rocks', 'I-LOC'), ('beach', 'O'), ('!', 'O'), ('We', 'O'), ('removed', 'O'), ('200', 'B-WEI'), ('kgs', 'B-UNT'), ('of', 'O'), ('debris', 'O'), (',', 'O'), ('thanks', 'O'), ('to', 'O'), ('the', 'O'), ('incredible', 'O'), ('efforts', 'O'), ('of', 'O'), ('@', 'O'), ('Take3', 'O'), ('.', 'O'), ('#', 'O'), ('Take3ForTheSea', 'O'), ('#', 'O'), ('CleanBeaches', 'O'), ('#', 'O'), ('AdamsRocks', 'O')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It was inspiring to witness so many people come together to clean up Adams Rocks beach! We removed 200 kgs of debris, thanks to the incredible efforts of @Take3. #Take3ForTheSea #CleanBeaches #AdamsRocks'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review newly assigned non-\"O\" tags\n",
    "SAMPLE_NO = 0\n",
    "for i in df.iloc[SAMPLE_NO]['tagged_entities']:\n",
    "    if i[1] != \"O\":\n",
    "        print(i[0], i[1])\n",
    "\n",
    "print(df['tagged_entities'][SAMPLE_NO])\n",
    "df['text'][SAMPLE_NO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 16:34:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d634a93d9add42c19eb906f4f79b0782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 16:34:04 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2023-02-20 16:34:04 INFO: Use device: cpu\n",
      "2023-02-20 16:34:04 INFO: Loading: tokenize\n",
      "2023-02-20 16:34:04 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Compile all sentences into a single list of lists (sentences) of word-pairs (word, NER tag)\n",
    "\n",
    "# Open question: this method is not very robust (cross-references stanza tokenizer sentence lengths \n",
    "# against list of original sentence text words, which might not be 1-1).\n",
    "#   Solution: use a find() to search for first word in each sentence, not just blind indexing into paragraph text\n",
    "\n",
    "# Method is not efficient. Maybe could be vectorized (?), but we only have to run this script once\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "df['text_stanza_tokenize'] = df['text'].apply(lambda x: nlp(x))\n",
    "\n",
    "all_sentences = []\n",
    "for i in range(len(df)):\n",
    "    idx = 0\n",
    "    for sentence in df.iloc[i]['text_stanza_tokenize'].sentences:\n",
    "        new_sentence = list(df.iloc[i]['tagged_entities'][idx:idx+len(sentence.words)])\n",
    "        all_sentences.append(new_sentence)\n",
    "        idx += len(sentence.words)\n",
    "\n",
    "# print(all_sentences[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into datasets = (train_sentences, dev_sentences, test_sentences)\n",
    "\n",
    "DEV_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "random.seed(1234)\n",
    "random.shuffle(all_sentences)\n",
    "\n",
    "train_sentences = all_sentences[ : int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT))]\n",
    "dev_sentences = all_sentences[int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT)) : int(len(all_sentences)*(1-TEST_SPLIT))]\n",
    "test_sentences = all_sentences[int(len(all_sentences)*(1-TEST_SPLIT)) : ]\n",
    "\n",
    "# print(len(train_sentences))\n",
    "# print(len(dev_sentences))\n",
    "# print(len(test_sentences))\n",
    "# print(len(all_sentences))\n",
    "\n",
    "datasets = (train_sentences, dev_sentences, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.train.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.train.json\n",
      "95 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.train.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.train.json\n",
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.dev.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.dev.json\n",
      "12 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.dev.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.dev.json\n",
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.test.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.test.json\n",
      "12 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.test.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/TOC_Test.test.json\n"
     ]
    }
   ],
   "source": [
    "# Convert file and write to JSON file needed for Stanza modelling\n",
    "out_directory = os.getcwd()\n",
    "write_dataset(datasets, out_directory, \"TOC_Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to JSON file needed by Stanza model\n",
    "# There is a conversion script called several times in prepare_ner_dataset.py which converts IOB format to our internal NER format:\n",
    "# import stanza.utils.datasets.ner.prepare_ner_file as prepare_ner_file\n",
    "\n",
    "# prepare_ner_file.process_dataset(input_iob, output_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
