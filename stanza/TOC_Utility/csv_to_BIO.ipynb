{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "from stanza.utils.datasets.ner.utils import write_dataset\n",
    "from transform_weight_date import number_to_words, date_to_formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in synthetic data\n",
    "df = pd.read_csv('../../data/data/generated/data_230124-172021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use re to replace any instances of \"####kg\" with \"#### kg\" where #### is any continuous \n",
    "# sequence of numbers and unit is one of those listed below\n",
    "def separate_weight_unit(row):\n",
    "    return re.sub(r'([0-9]+)(kgs|kg|lbs|lb|pounds|kilograms)', r\"\\1 \\2\", row)\n",
    "\n",
    "# Function to remove spaces (e.g. \"Take 3\" -> \"Take3\")\n",
    "def remove_spaces(row):\n",
    "    return row.replace(\" \", \"\")\n",
    "\n",
    "def add_loc_comma_token(row):\n",
    "    return row.replace(\",\", \" ,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021-05-25',\n",
       " 'May 25, 2021',\n",
       " 'may 25, 2021',\n",
       " 'May 25th, 2021',\n",
       " 'may 25th, 2021',\n",
       " '25 May 2021',\n",
       " '25 may 2021',\n",
       " '25th May 2021',\n",
       " '25th may 2021',\n",
       " 'May 25 2021',\n",
       " 'may 25 2021',\n",
       " 'May 25th 2021',\n",
       " 'may 25th 2021',\n",
       " '05/25/21',\n",
       " '05/25/2021',\n",
       " '05-25-2021',\n",
       " '25 May 2021',\n",
       " '25 may 2021',\n",
       " '25th May 2021',\n",
       " '25th may 2021']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider using Stanza sentence tokenizer instead of nltk word_tokenizer? See csv_to_BIO_stanzaTokenize.ipynb file\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: separate_weight_unit(x))\n",
    "df['text_split'] = df['text'].apply(lambda x: x.strip())\n",
    "df['text_split'] = df['text_split'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Preprocess orgs and locations \n",
    "df['org_no_space'] = df['organization'].apply(lambda x: remove_spaces(x))\n",
    "df['loc_no_space'] = df['location'].apply(lambda x: remove_spaces(x))\n",
    "df['location'] = df['location'].apply(lambda x: add_loc_comma_token(x))\n",
    "\n",
    "# Assign appropriate types\n",
    "string_cols = [\"item1\", \"item2\", \"location\"]\n",
    "df[string_cols] = df[string_cols].astype(str)\n",
    "df['weight2'] = df['weight2'].astype('Int64')\n",
    "\n",
    "# Compute variations of date and weight formats\n",
    "df['date_vars'] = df['date'].apply(lambda x: date_to_formats(x))\n",
    "df['weight1_text'] = df['weight1'].apply(lambda x: number_to_words(x)[1])\n",
    "df['weight2_text'] = df['weight2'].apply(lambda x: number_to_words(x)[1] if pd.notnull(x) else str(x))\n",
    "\n",
    "# Make string columns lowercase for comparisons\n",
    "lowercase_cols = string_cols + ['organization', 'location', 'org_no_space', 'loc_no_space', 'weight1_text', 'weight2_text']\n",
    "for i in lowercase_cols:\n",
    "    df[i] = df[i].apply(lambda x: x.lower())\n",
    "\n",
    "# df.head(15)\n",
    "# df.info()\n",
    "# print(df.iloc[12])\n",
    "df['date_vars'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into list of words with associated 'B - entity', 'I - entity' or 'O'\n",
    "# Look at other preprocessing steps in read_datasets function in convert_bn_daffodil\n",
    "\n",
    "units = [\"kilograms\", \"kilogram\", \"kgs\", \"kg\", \"lb\", \"lbs\", \"pounds\", \"pound\"]\n",
    "\n",
    "def assign_entity_types(row):\n",
    "    words = row['text_split']\n",
    "    new_tags = []\n",
    "    prev_item_tag = False\n",
    "\n",
    "    idx = 0\n",
    "    while (idx < len(words)):\n",
    "        loc_length = len(row['location'].split())\n",
    "        org_length = len(row['organization'].split())\n",
    "        weight1_text_length = len(row['weight1_text'].split())\n",
    "        if row['weight2_text'] != None:\n",
    "            weight2_text_length = len(row['weight2_text'].split())\n",
    "        else:\n",
    "            weight2_text_length = -1\n",
    "        \n",
    "        # Assign location labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        # Does not handle extraneous locations not provided in prompt\n",
    "        if ((idx < len(words) - loc_length) and \n",
    "            [x.lower() for x in words[idx : idx + loc_length]] == row['location'].split()):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "            for i in range(1, loc_length):\n",
    "                new_tags.append(\"I-LOC\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['loc_no_space']):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "\n",
    "        # Assign organization labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        elif ((idx < len(words) - org_length) and \n",
    "            [x.lower() for x in words[idx : idx + org_length]] == (row['organization'].lower().split())):\n",
    "            new_tags.append(\"B-ORG\")            # idea for later: tag acronyms for Orgs?\n",
    "            idx += 1                            \n",
    "            for i in range(1, org_length):\n",
    "                new_tags.append(\"I-ORG\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['org_no_space']):\n",
    "            new_tags.append(\"B-ORG\")      \n",
    "            idx += 1\n",
    "            \n",
    "        # Assign unit labels\n",
    "        elif any(words[idx] == word for word in units):   \n",
    "            new_tags.append(\"B-UNT\")\n",
    "            idx += 1\n",
    "        \n",
    "        # Assign weight labels for numeric and text numbers (consider '-' and non- '-' versions of written numbers?)\n",
    "        elif (words[idx] == str(row['weight1']) or (row['weight2'] != None and words[idx] == str(row['weight2']))): \n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "        elif ((idx < len(words) - weight1_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight1_text_length]] == row['weight1_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "        elif ((weight2_text_length > 0) and (idx < len(words) - weight2_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight2_text_length]] == row['weight2_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "\n",
    "        # Assign item labels (dont look for consecutive matches here)\n",
    "        elif (any(words[idx] == word for word in row['item1'].split()) or \n",
    "                                (row['item2'] != None and any(words[idx] == word for word in row['item2'].split()))):\n",
    "            if prev_item_tag: \n",
    "                new_tags.append(\"I-ITM\")\n",
    "            else:\n",
    "                new_tags.append(\"B-ITM\")\n",
    "                prev_item_tag = True\n",
    "            idx += 1\n",
    "        # Open question: How to assign dates? Need to capture all possible date formats?\n",
    "        # Solution: convert golden value dates to datetime objects then use strftime package to generate \n",
    "        # possible text versions of it\n",
    "        # Assign date labels\n",
    "        # elif () UPDATE MEEEEEE\n",
    "        \n",
    "        else:\n",
    "            new_tags.append(\"O\")\n",
    "            prev_item_tag = False\n",
    "            idx += 1\n",
    "\n",
    "    return list(zip(words, new_tags))\n",
    "\n",
    "df['tagged_entities'] = df.apply(assign_entity_types, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences\n",
    "# df['text_split'][13] = ['I', 'have', 'two', 'hundred', 'and', 'twenty', 'dogs', 'and', 'two', 'hundred', 'and', 'fifty-seven', \n",
    "#                         'cats', 'ugly', 'two', 'hundred', 'and', 'twenty-one']\n",
    "# assign_entity_types(df.iloc[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review newly assigned non-\"O\" tags\n",
    "SAMPLE_NO = 12\n",
    "for i in df.iloc[SAMPLE_NO]['tagged_entities']:\n",
    "    if i[1] != \"O\":\n",
    "        print(i[0], i[1])\n",
    "\n",
    "print(df['tagged_entities'][SAMPLE_NO])\n",
    "df['text'][SAMPLE_NO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all sentences into a single list of lists (sentences) of word-pairs (word, NER tag)\n",
    "\n",
    "# Open question: this method is not very robust (cross-references stanza tokenizer sentence lengths \n",
    "# against list of original sentence text words, which might not be 1-1).\n",
    "#   Solution: use a find() to search for first word in each sentence, not just blind indexing into paragraph text\n",
    "\n",
    "# Method is not efficient. Maybe could be vectorized (?), but we only have to run this script once\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "df['text_stanza_tokenize'] = df['text'].apply(lambda x: nlp(x))\n",
    "\n",
    "all_sentences = []\n",
    "for i in range(len(df)):\n",
    "    idx = 0\n",
    "    for sentence in df.iloc[i]['text_stanza_tokenize'].sentences:\n",
    "        new_sentence = list(df.iloc[i]['tagged_entities'][idx:idx+len(sentence.words)])\n",
    "        all_sentences.append(new_sentence)\n",
    "        idx += len(sentence.words)\n",
    "\n",
    "# print(all_sentences[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into datasets = (train_sentences, dev_sentences, test_sentences)\n",
    "\n",
    "DEV_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "random.seed(1234)\n",
    "random.shuffle(all_sentences)\n",
    "\n",
    "train_sentences = all_sentences[ : int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT))]\n",
    "dev_sentences = all_sentences[int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT)) : int(len(all_sentences)*(1-TEST_SPLIT))]\n",
    "test_sentences = all_sentences[int(len(all_sentences)*(1-TEST_SPLIT)) : ]\n",
    "\n",
    "# print(len(train_sentences))\n",
    "# print(len(dev_sentences))\n",
    "# print(len(test_sentences))\n",
    "# print(len(all_sentences))\n",
    "\n",
    "datasets = (train_sentences, dev_sentences, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert file and write to JSON file needed for Stanza modelling\n",
    "out_directory = os.getcwd()\n",
    "write_dataset(datasets, out_directory, \"TOC_Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to JSON file needed by Stanza model\n",
    "# There is a conversion script called several times in prepare_ner_dataset.py which converts IOB format to our internal NER format:\n",
    "# import stanza.utils.datasets.ner.prepare_ner_file as prepare_ner_file\n",
    "\n",
    "# prepare_ner_file.process_dataset(input_iob, output_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
