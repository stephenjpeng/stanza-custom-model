{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "from stanza.utils.datasets.ner.utils import write_dataset\n",
    "from transform_weight_date import number_to_words, date_to_formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2462 entries, 0 to 2461\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   location      2324 non-null   object \n",
      " 1   organization  2343 non-null   object \n",
      " 2   type          2462 non-null   object \n",
      " 3   date          2338 non-null   object \n",
      " 4   unit          2332 non-null   object \n",
      " 5   weight1       2332 non-null   float64\n",
      " 6   item1         2462 non-null   object \n",
      " 7   prompt        2462 non-null   object \n",
      " 8   text          2462 non-null   object \n",
      " 9   weight2       117 non-null    float64\n",
      " 10  item2         120 non-null    object \n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 211.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Specify data to be read in from CSV using output\n",
    "toy_data_path = '../../data/data/generated/data_230124-172021.csv'\n",
    "synthetic_data_1_path = '../../../xplore-the-ocean-cleanup/data-generation/data/generated/data_230221-205202.csv'\n",
    "synthetic_data_2_path = '../../../xplore-the-ocean-cleanup/data-generation/data/generated/data_230222-092039.csv'\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "DATA_SELECTION = \"synth_combined\"\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "if DATA_SELECTION == \"toy\":\n",
    "    df = pd.read_csv(toy_data_path)\n",
    "if DATA_SELECTION == \"synth1\":\n",
    "    df = pd.read_csv(synthetic_data_1_path)\n",
    "if DATA_SELECTION == \"synth2\":\n",
    "    df = pd.read_csv(synthetic_data_2_path)\n",
    "\n",
    "if DATA_SELECTION == \"synth_combined\":\n",
    "    df = pd.concat([pd.read_csv(synthetic_data_1_path), pd.read_csv(synthetic_data_2_path)], \n",
    "                    axis= 0, ignore_index= True)\n",
    "    df = df.drop(columns = [\"Unnamed: 0\"])\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use re to replace any instances of \"####kg\" with \"#### kg\" where #### is any continuous \n",
    "# sequence of numbers and unit is one of those listed below\n",
    "def separate_weight_unit(row):\n",
    "    return re.sub(r'([0-9]+)(kgs|kg|lbs|lb|pounds|kilograms)', r\"\\1 \\2\", row)\n",
    "\n",
    "# Function to remove spaces (e.g. \"Take 3\" -> \"Take3\")\n",
    "def remove_spaces(text):\n",
    "    return text.replace(\" \", \"\")\n",
    "\n",
    "# Function to replace long hyphen ASCII code with short hyphen '-' ASCII code\n",
    "def character_norm(text):\n",
    "    return text.replace(chr(8211), \"-\")\n",
    "\n",
    "# Word tokenizer splits ',' into separate token, so we have this function to do the same\n",
    "def add_comma_token(text):\n",
    "    return text.replace(\",\", \" ,\")\n",
    "\n",
    "# Split '/' into its own token \n",
    "def add_slash_token(text):\n",
    "    return text.replace(chr(47), \" / \")\n",
    "\n",
    "# Word tokenizer splits ',' into separate token, so we have this function to do the same for our dates list\n",
    "def add_date_var_comma_token(list):\n",
    "    new_list = []\n",
    "    for i in list:\n",
    "        new_list.append(add_comma_token(i))\n",
    "    return new_list\n",
    "\n",
    "# Gets the first token of each date variation, to allow for faster downstream computation \n",
    "def get_first_token_set(list):\n",
    "    new_set = set()\n",
    "    for i in list:\n",
    "        new_set.add(i.split()[0])\n",
    "    return new_set\n",
    "\n",
    "def get_item_set(row):\n",
    "    item_set = set([])\n",
    "    for i in row['item1'].split():\n",
    "        item_set.add(i.lower())\n",
    "    for j in row['item2'].split():\n",
    "        item_set.add(j.lower())\n",
    "    if 'nan' in item_set:\n",
    "        item_set.remove('nan')\n",
    "    return item_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>organization</th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>unit</th>\n",
       "      <th>weight1</th>\n",
       "      <th>item1</th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "      <th>weight2</th>\n",
       "      <th>item2</th>\n",
       "      <th>text_split</th>\n",
       "      <th>org_no_space</th>\n",
       "      <th>loc_no_space</th>\n",
       "      <th>item_set</th>\n",
       "      <th>date_vars</th>\n",
       "      <th>weight1_text</th>\n",
       "      <th>weight2_text</th>\n",
       "      <th>date_vars_first_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smathers beach</td>\n",
       "      <td>industrial surplus foundation</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2016-10-22</td>\n",
       "      <td>pounds</td>\n",
       "      <td>381</td>\n",
       "      <td>bait bags / containers and foam cups</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>What a productive day at Smathers Beach! We co...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[What, a, productive, day, at, Smathers, Beach...</td>\n",
       "      <td>industrialsurplusfoundation</td>\n",
       "      <td>smathersbeach</td>\n",
       "      <td>{cups, foam, bait, containers, bags, and, /}</td>\n",
       "      <td>[2016-10-22, October 22 , 2016, october 22 , 2...</td>\n",
       "      <td>three hundred and eighty-one</td>\n",
       "      <td></td>\n",
       "      <td>{10/22/16, 10-22, October, 22, 22nd, oct, octo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>benedict beach</td>\n",
       "      <td>independent bakers association inc</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2015-12-15</td>\n",
       "      <td>kilograms</td>\n",
       "      <td>200</td>\n",
       "      <td>plastic</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>Today, the Independent Bakers Association Inc....</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Today, ,, the, Independent, Bakers, Associati...</td>\n",
       "      <td>independentbakersassociationinc</td>\n",
       "      <td>benedictbeach</td>\n",
       "      <td>{plastic}</td>\n",
       "      <td>[2015-12-15, December 15 , 2015, december 15 ,...</td>\n",
       "      <td>two hundred</td>\n",
       "      <td></td>\n",
       "      <td>{December, Dec, dec, 12-15, december, 12-15-20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wamberal beach</td>\n",
       "      <td>trout unlimited</td>\n",
       "      <td>press release</td>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>lbs</td>\n",
       "      <td>309</td>\n",
       "      <td>plastic</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>TROUT UNLIMITED ANNOUNCES SUCCESSFUL WAMBERAL ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[TROUT, UNLIMITED, ANNOUNCES, SUCCESSFUL, WAMB...</td>\n",
       "      <td>troutunlimited</td>\n",
       "      <td>wamberalbeach</td>\n",
       "      <td>{plastic}</td>\n",
       "      <td>[2015-02-25, February 25 , 2015, february 25 ,...</td>\n",
       "      <td>three hundred and nine</td>\n",
       "      <td></td>\n",
       "      <td>{Feb, 02-25, February, 2015-02-25, february, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wagner spur</td>\n",
       "      <td>el rey de gloria mision</td>\n",
       "      <td>press release</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>pounds</td>\n",
       "      <td>20</td>\n",
       "      <td>dog poop bags , rope , and glass cups</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>El Rey De Gloria Mision Takes Local Beach Clea...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[El, Rey, De, Gloria, Mision, Takes, Local, Be...</td>\n",
       "      <td>elreydegloriamision</td>\n",
       "      <td>wagnerspur</td>\n",
       "      <td>{glass, cups, dog, bags, ,, and, rope, poop}</td>\n",
       "      <td>[2017-06-06, June 6 , 2017, june 6 , 2017, Jun...</td>\n",
       "      <td>twenty</td>\n",
       "      <td></td>\n",
       "      <td>{6, June, 06/06/17, 06-06, 06/06, 6th, jun, 06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>owerri</td>\n",
       "      <td>friends of the upper delaware river</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2016-05-07</td>\n",
       "      <td>units</td>\n",
       "      <td>381</td>\n",
       "      <td>plastic films and foam fragments</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>Friends of the Upper Delaware River made a hug...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Friends, of, the, Upper, Delaware, River, mad...</td>\n",
       "      <td>friendsoftheupperdelawareriver</td>\n",
       "      <td>owerri</td>\n",
       "      <td>{foam, plastic, films, and, fragments}</td>\n",
       "      <td>[2016-05-07, May 7 , 2016, may 7 , 2016, May 7...</td>\n",
       "      <td>three hundred and eighty-one</td>\n",
       "      <td></td>\n",
       "      <td>{05/07, 05/07/16, 05-07, 05/07/2016, 2016-05-0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         location                         organization               type  \\\n",
       "0  smathers beach        industrial surplus foundation  instagram caption   \n",
       "1  benedict beach   independent bakers association inc  instagram caption   \n",
       "2  wamberal beach                      trout unlimited      press release   \n",
       "3     wagner spur              el rey de gloria mision      press release   \n",
       "4          owerri  friends of the upper delaware river  instagram caption   \n",
       "\n",
       "         date       unit  weight1                                  item1  \\\n",
       "0  2016-10-22     pounds      381   bait bags / containers and foam cups   \n",
       "1  2015-12-15  kilograms      200                                plastic   \n",
       "2  2015-02-25        lbs      309                                plastic   \n",
       "3  2017-06-06     pounds       20  dog poop bags , rope , and glass cups   \n",
       "4  2016-05-07      units      381       plastic films and foam fragments   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Generate an instagram caption for a beach clea...   \n",
       "1  Generate an instagram caption for a beach clea...   \n",
       "2  Generate a press release for a beach cleanup w...   \n",
       "3  Generate a press release for a beach cleanup w...   \n",
       "4  Generate an instagram caption for a beach clea...   \n",
       "\n",
       "                                                text  weight2 item2  \\\n",
       "0  What a productive day at Smathers Beach! We co...     <NA>   nan   \n",
       "1  Today, the Independent Bakers Association Inc....     <NA>   nan   \n",
       "2  TROUT UNLIMITED ANNOUNCES SUCCESSFUL WAMBERAL ...     <NA>   nan   \n",
       "3  El Rey De Gloria Mision Takes Local Beach Clea...     <NA>   nan   \n",
       "4  Friends of the Upper Delaware River made a hug...     <NA>   nan   \n",
       "\n",
       "                                          text_split  \\\n",
       "0  [What, a, productive, day, at, Smathers, Beach...   \n",
       "1  [Today, ,, the, Independent, Bakers, Associati...   \n",
       "2  [TROUT, UNLIMITED, ANNOUNCES, SUCCESSFUL, WAMB...   \n",
       "3  [El, Rey, De, Gloria, Mision, Takes, Local, Be...   \n",
       "4  [Friends, of, the, Upper, Delaware, River, mad...   \n",
       "\n",
       "                      org_no_space   loc_no_space  \\\n",
       "0      industrialsurplusfoundation  smathersbeach   \n",
       "1  independentbakersassociationinc  benedictbeach   \n",
       "2                   troutunlimited  wamberalbeach   \n",
       "3              elreydegloriamision     wagnerspur   \n",
       "4   friendsoftheupperdelawareriver         owerri   \n",
       "\n",
       "                                       item_set  \\\n",
       "0  {cups, foam, bait, containers, bags, and, /}   \n",
       "1                                     {plastic}   \n",
       "2                                     {plastic}   \n",
       "3  {glass, cups, dog, bags, ,, and, rope, poop}   \n",
       "4        {foam, plastic, films, and, fragments}   \n",
       "\n",
       "                                           date_vars  \\\n",
       "0  [2016-10-22, October 22 , 2016, october 22 , 2...   \n",
       "1  [2015-12-15, December 15 , 2015, december 15 ,...   \n",
       "2  [2015-02-25, February 25 , 2015, february 25 ,...   \n",
       "3  [2017-06-06, June 6 , 2017, june 6 , 2017, Jun...   \n",
       "4  [2016-05-07, May 7 , 2016, may 7 , 2016, May 7...   \n",
       "\n",
       "                   weight1_text weight2_text  \\\n",
       "0  three hundred and eighty-one                \n",
       "1                   two hundred                \n",
       "2        three hundred and nine                \n",
       "3                        twenty                \n",
       "4  three hundred and eighty-one                \n",
       "\n",
       "                               date_vars_first_token  \n",
       "0  {10/22/16, 10-22, October, 22, 22nd, oct, octo...  \n",
       "1  {December, Dec, dec, 12-15, december, 12-15-20...  \n",
       "2  {Feb, 02-25, February, 2015-02-25, february, 0...  \n",
       "3  {6, June, 06/06/17, 06-06, 06/06, 6th, jun, 06...  \n",
       "4  {05/07, 05/07/16, 05-07, 05/07/2016, 2016-05-0...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign appropriate types\n",
    "string_cols = [\"item1\", \"item2\", \"location\", \"organization\", \"date\"]\n",
    "df[string_cols] = df[string_cols].astype(str)\n",
    "int_cols = [\"weight1\", \"weight2\"]\n",
    "for i in int_cols:\n",
    "    df[i] = df[i].astype('Int64')\n",
    "\n",
    "# Normalize text columns to match tokenizer \n",
    "df['text'] = df['text'].apply(lambda x: separate_weight_unit(x))\n",
    "for i in string_cols:\n",
    "    df[i] = df[i].apply(lambda x: character_norm(x))\n",
    "df['text'] = df['text'].apply(lambda x: x.strip())\n",
    "\n",
    "# Tokenize text\n",
    "df['text_split'] = df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Preprocess orgs and locations \n",
    "df['org_no_space'] = df['organization'].apply(lambda x: remove_spaces(x))\n",
    "df['loc_no_space'] = df['location'].apply(lambda x: remove_spaces(x))\n",
    "\n",
    "# Preprocess ',' and '/' tokens\n",
    "for i in string_cols:\n",
    "    df[i] = df[i].apply(lambda x: add_comma_token(x))\n",
    "for i in [\"item1\", \"item2\"]:\n",
    "    df[i] = df[i].apply(lambda x: add_slash_token(x))\n",
    "\n",
    "# Create set of trash items of interest for each text\n",
    "df['item_set'] = df.apply(get_item_set, axis = 1)\n",
    "\n",
    "# Compute variations of date and weight formats and preprocess into desired formats\n",
    "df['date_vars'] = df['date'].apply(lambda x: date_to_formats(x) if x != 'nan' else str(x))\n",
    "df['weight1_text'] = df['weight1'].apply(lambda x: number_to_words(x)[1] if pd.notnull(x) else \"\")\n",
    "df['weight2_text'] = df['weight2'].apply(lambda x: number_to_words(x)[1] if pd.notnull(x) else \"\")\n",
    "df['date_vars'] = df['date_vars'].apply(lambda x: add_date_var_comma_token(x))\n",
    "df['date_vars_first_token'] = df['date_vars'].apply(lambda x: get_first_token_set(x))\n",
    "\n",
    "# Make string columns lowercase for downstream comparisons\n",
    "lowercase_cols = string_cols + ['organization', 'org_no_space', 'loc_no_space', 'weight1_text', 'weight2_text']\n",
    "for i in lowercase_cols:\n",
    "    df[i] = df[i].apply(lambda x: x.lower())\n",
    "\n",
    "df.head()\n",
    "# df.info()\n",
    "# print(df.iloc[2])\n",
    "# df[(df['organization'].str.contains(\"Coalition\"))]\n",
    "# df[(df['weight1']== 386)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into list of words with associated 'B - entity', 'I - entity' or 'O'\n",
    "\n",
    "units = set([\"kilograms\", \"kilogram\", \"kgs\", \"kg\", \"lb\", \"lbs\", \"pounds\", \"pound\"])\n",
    "filler_words = set([\"and\", \"the\", \"a\", \"an\", \",\", \"/\", \"with\"])\n",
    "\n",
    "def assign_entity_types(row):\n",
    "    words = row['text_split']\n",
    "    new_tags = []\n",
    "    prev_item_tag = False\n",
    "\n",
    "    idx = 0\n",
    "    while (idx < len(words)):\n",
    "        loc_length = len(row['location'].split())\n",
    "        org_length = len(row['organization'].split())\n",
    "        weight1_text_length = len(row['weight1_text'].split())\n",
    "        if row['weight2_text'] != None:\n",
    "            weight2_text_length = len(row['weight2_text'].split())\n",
    "        else:\n",
    "            weight2_text_length = -1\n",
    "        \n",
    "        # Assign location labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        # Does not handle extraneous locations not provided in prompt!\n",
    "        if ((idx <= len(words) - loc_length) and \n",
    "            [x.lower() for x in words[idx : idx + loc_length]] == row['location'].split()):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "            for i in range(1, loc_length):\n",
    "                new_tags.append(\"I-LOC\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['loc_no_space']):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "\n",
    "        # Assign organization labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        elif ((idx <= len(words) - org_length) and \n",
    "            [x.lower() for x in words[idx : idx + org_length]] == (row['organization'].lower().split())):\n",
    "            new_tags.append(\"B-ORG\")            # idea for later: tag acronyms for Orgs?\n",
    "            idx += 1                            \n",
    "            for i in range(1, org_length):\n",
    "                new_tags.append(\"I-ORG\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['org_no_space']):\n",
    "            new_tags.append(\"B-ORG\")      \n",
    "            idx += 1\n",
    "            \n",
    "        # Assign unit labels\n",
    "        elif words[idx].lower() in units:   \n",
    "            new_tags.append(\"B-UNT\")\n",
    "            idx += 1\n",
    "        \n",
    "        # Assign weight labels for numeric and text numbers (consider '-' and non- '-' versions of written numbers?)\n",
    "        elif (words[idx] == str(row['weight1']) or \n",
    "            (not pd.isna(row['weight2']) and words[idx] == str(row['weight2']))): \n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "        elif (not pd.isna(row['weight1']) and (idx <= len(words) - weight1_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight1_text_length]] == row['weight1_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "        elif ((weight2_text_length > 0) and (idx <= len(words) - weight2_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight2_text_length]] == row['weight2_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "\n",
    "        # Assign item labels (dont look for consecutive matches here)\n",
    "        # Does not handle extraneous trash items not provided in prompt!\n",
    "        elif (words[idx].lower() in row['item_set'] and words[idx].lower() not in filler_words):\n",
    "            if prev_item_tag: \n",
    "                new_tags.append(\"I-ITM\")\n",
    "            else:\n",
    "                new_tags.append(\"B-ITM\")\n",
    "                prev_item_tag = True\n",
    "            idx += 1\n",
    "        # Assign date labels (check only first token to minimize computation on each word)\n",
    "        elif (words[idx] in row['date_vars_first_token']):\n",
    "            # Check for complete consecutive match with any of the possible date variations \n",
    "            date_found = False\n",
    "            for date_var in row['date_vars']:\n",
    "                if ((idx <= len(words) - len(date_var.split())) and \n",
    "                    [x.lower() for x in words[idx : idx + len(date_var.split())]] == date_var.lower().split()):\n",
    "                    new_tags.append(\"B-DAT\")\n",
    "                    idx += 1\n",
    "                    for i in range(1, len(date_var.split())):\n",
    "                        new_tags.append(\"I-DAT\")\n",
    "                        idx += 1\n",
    "                    date_found = True\n",
    "                    break\n",
    "            # If the text matches with none of the date_vars, we need to append \"O\"\n",
    "            if not date_found:\n",
    "                new_tags.append(\"O\")\n",
    "                prev_item_tag = False\n",
    "                idx += 1\n",
    "        \n",
    "        else:\n",
    "            new_tags.append(\"O\")\n",
    "            prev_item_tag = False\n",
    "            idx += 1\n",
    "\n",
    "    return list(zip(words, new_tags))\n",
    "\n",
    "df['tagged_entities'] = df.apply(assign_entity_types, axis =1)\n",
    "# assign_entity_types(df.iloc[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences\n",
    "# df['text_split'][13] = ['I', 'have', 'two', 'hundred', 'and', 'twenty', 'dogs', 'and', 'two', 'hundred', 'and', 'fifty-seven', \n",
    "#                         'cats', 'ugly', 'two', 'hundred', 'and', 'twenty-one', 'on', '11', 'Jan', '2020', '01-11-2020']\n",
    "# assign_entity_types(df.iloc[13])\n",
    "# print(df['text_split'][13][22] in df['date_vars_first_token'][13])\n",
    "# print('01-11-2020'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TroutUnlimited B-ORG\n",
      "Greens B-LOC\n",
      "Beach I-LOC\n",
      "356 B-WEI\n",
      "pounds B-UNT\n",
      "plastic B-ITM\n",
      "bottle I-ITM\n",
      "caps I-ITM\n",
      "rings B-ITM\n",
      "TroutUnlimited B-ORG\n",
      "[('No', 'O'), ('job', 'O'), ('is', 'O'), ('too', 'O'), ('big', 'O'), ('or', 'O'), ('too', 'O'), ('small', 'O'), ('!', 'O'), ('We', 'O'), ('had', 'O'), ('a', 'O'), ('great', 'O'), ('beach', 'O'), ('clean-up', 'O'), ('with', 'O'), ('@', 'O'), ('TroutUnlimited', 'B-ORG'), ('at', 'O'), ('Greens', 'B-LOC'), ('Beach', 'I-LOC'), ('today', 'O'), (',', 'O'), ('collecting', 'O'), ('a', 'O'), ('whopping', 'O'), ('356', 'B-WEI'), ('pounds', 'B-UNT'), ('of', 'O'), ('plastic', 'B-ITM'), ('bottle', 'I-ITM'), ('caps', 'I-ITM'), ('and', 'O'), ('rings', 'B-ITM'), ('.', 'O'), ('#', 'O'), ('OurWatersOurWorld', 'O'), ('#', 'O'), ('BeachCleanup', 'O'), ('#', 'O'), ('TroutUnlimited', 'B-ORG'), ('#', 'O'), ('ProtectOurPlanet', 'O')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No job is too big or too small! We had a great beach clean-up with @TroutUnlimited at Greens Beach today, collecting a whopping 356 pounds of plastic bottle caps and rings. #OurWatersOurWorld #BeachCleanup #TroutUnlimited #ProtectOurPlanet'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review newly assigned non-\"O\" tags\n",
    "SAMPLE_NO = 8\n",
    "for i in df.iloc[SAMPLE_NO]['tagged_entities']:\n",
    "    if i[1] != \"O\":\n",
    "        print(i[0], i[1])\n",
    "\n",
    "print(df['tagged_entities'][SAMPLE_NO])\n",
    "# print(df['item1'][SAMPLE_NO])\n",
    "df['text'][SAMPLE_NO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use stanza tokenizer to determine sentence chunks \n",
    "\n",
    "# nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "# df['text_stanza_tokenize'] = df['text'].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences tagged:  9122\n"
     ]
    }
   ],
   "source": [
    "# Compiles all sentences into a single list of lists (sentences) of word-pairs (word, NER tag)\n",
    "# def get_all_sentences1(df):\n",
    "#     all_sentences = []\n",
    "#     for i in range(len(df)):\n",
    "#         idx = 0\n",
    "#         for sentence in df.iloc[i]['text_stanza_tokenize'].sentences:\n",
    "#             # Check for first word in stanza-tokenized sentence and adjust index within small range to correct\n",
    "#             # starting word (Problem: may result in 1 or 2 tokens being truncated from front or end of sentences, \n",
    "#             # though this adjustment doesn't happen in every document and only 2-3 times per document when it does)\n",
    "#             first_word = sentence.tokens[0].text\n",
    "#             try:\n",
    "#                 if (first_word != df.iloc[i]['tagged_entities'][idx][0]):\n",
    "#                     for adj in [-2, -1, 1, 2]:\n",
    "#                         if (first_word == df.iloc[i]['tagged_entities'][idx + adj][0]):\n",
    "#                             idx = idx + adj\n",
    "#             except IndexError:\n",
    "#                 pass\n",
    "            \n",
    "#             end_sentence_limit = min(idx+len(sentence.words), len(df.iloc[i]['tagged_entities'])-1)\n",
    "#             new_sentence = list(df.iloc[i]['tagged_entities'][idx:end_sentence_limit])\n",
    "#             all_sentences.append(new_sentence)\n",
    "#             idx += len(sentence.words)\n",
    "#     return all_sentences\n",
    "\n",
    "end_sentence = set(['.', '!', '?', '\\n'])\n",
    "\n",
    "# Method to split sentences based on punctuation marks, not based on Stanza-chunked sentences. \n",
    "# Splits text into fewer, longer sentences than get_all_sentences1, but is less likely to truncate a sentence.\n",
    "\n",
    "# Compiles all sentences into a single list of lists (sentences) of word-pairs (word, NER tag)\n",
    "def get_all_sentences2(df):\n",
    "    all_sentences = []\n",
    "    for i in range(len(df)):\n",
    "        idx = 0\n",
    "        text_length = len(df.iloc[i]['tagged_entities'])\n",
    "        # print(\"text length:\", text_length)\n",
    "        while idx < text_length:\n",
    "            end = text_length - 1\n",
    "            for j in range(idx, text_length):\n",
    "                if df.iloc[i]['tagged_entities'][j][0] in end_sentence:\n",
    "                    end = j\n",
    "                    # print(j)\n",
    "                    break\n",
    "            \n",
    "            # print(\"end\", end)\n",
    "            new_sentence = list(df.iloc[i]['tagged_entities'][idx : end + 1])\n",
    "            all_sentences.append(new_sentence)\n",
    "            idx = end + 1\n",
    "    return all_sentences\n",
    "\n",
    "# print(get_all_sentences2(df.iloc[5:7])[2])\n",
    "all_sentences = get_all_sentences2(df)\n",
    "\n",
    "print(\"# of sentences tagged: \", len(all_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into datasets = (train_sentences, dev_sentences, test_sentences)\n",
    "\n",
    "DEV_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "random.seed(1234)\n",
    "random.shuffle(all_sentences)\n",
    "\n",
    "train_sentences = all_sentences[ : int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT))]\n",
    "dev_sentences = all_sentences[int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT)) : int(len(all_sentences)*(1-TEST_SPLIT))]\n",
    "test_sentences = all_sentences[int(len(all_sentences)*(1-TEST_SPLIT)) : ]\n",
    "\n",
    "# print(len(train_sentences))\n",
    "# print(len(dev_sentences))\n",
    "# print(len(test_sentences))\n",
    "# print(len(all_sentences))\n",
    "\n",
    "datasets = (train_sentences, dev_sentences, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of 'O' tags:  14918\n",
      "ratio of 'O' tags to total:  0.8154140475539765\n"
     ]
    }
   ],
   "source": [
    "# Count number of 'O' tags in test set for dummy baseline prediction\n",
    "count = 0\n",
    "total = 0\n",
    "for i in test_sentences:\n",
    "    for token in i:\n",
    "        if token[1] == 'O':\n",
    "            count += 1\n",
    "        total +=1\n",
    "print(\"count of 'O' tags: \", count)\n",
    "print(\"ratio of 'O' tags to total: \", count / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth_combined.train.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth_combined.train.json\n",
      "7297 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth_combined.train.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth_combined.train.json\n",
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth_combined.dev.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth_combined.dev.json\n",
      "912 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth_combined.dev.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth_combined.dev.json\n",
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth_combined.test.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth_combined.test.json\n",
      "913 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth_combined.test.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth_combined.test.json\n"
     ]
    }
   ],
   "source": [
    "# Convert file and write to JSON file needed for Stanza modelling\n",
    "out_directory = os.getcwd() + '/Processed_Data'\n",
    "write_dataset(datasets, out_directory, DATA_SELECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
