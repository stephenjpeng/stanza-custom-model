{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "from stanza.utils.datasets.ner.utils import write_dataset\n",
    "from transform_weight_date import number_to_words, date_to_formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data to be read in from CSV using output\n",
    "toy_data_path = '../../data/data/generated/data_230124-172021.csv'\n",
    "synthetic_data_1_path = '../../../xplore-the-ocean-cleanup/data-generation/data/generated/data_230221-205202.csv'\n",
    "synthetic_data_2_path = '../../../xplore-the-ocean-cleanup/data-generation/data/generated/data_230222-092039.csv'\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "DATA_SELECTION = \"synth1\"\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "if DATA_SELECTION == \"toy\":\n",
    "    data_path = toy_data_path\n",
    "if DATA_SELECTION == \"synth1\":\n",
    "    data_path = synthetic_data_1_path\n",
    "if DATA_SELECTION == \"synth2\":\n",
    "    data_path = synthetic_data_2_path\n",
    "\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use re to replace any instances of \"####kg\" with \"#### kg\" where #### is any continuous \n",
    "# sequence of numbers and unit is one of those listed below\n",
    "def separate_weight_unit(row):\n",
    "    return re.sub(r'([0-9]+)(kgs|kg|lbs|lb|pounds|kilograms)', r\"\\1 \\2\", row)\n",
    "\n",
    "# Function to remove spaces (e.g. \"Take 3\" -> \"Take3\")\n",
    "def remove_spaces(text):\n",
    "    return text.replace(\" \", \"\")\n",
    "\n",
    "# Function to replace long hyphen ASCII code with short hyphen '-' ASCII code\n",
    "def character_norm(text):\n",
    "    return text.replace(chr(8211), \"-\")\n",
    "\n",
    "# Word tokenizer splits ',' into separate token, so we have this function to do the same\n",
    "def add_comma_token(text):\n",
    "    return text.replace(\",\", \" ,\")\n",
    "\n",
    "# Split '/' into its own token   JOE TO UPDATE THIS TINY EDGE CASE\n",
    "def add_slash_token(text):\n",
    "    return text.replace(chr(47), \" / \")\n",
    "\n",
    "# Word tokenizer splits ',' into separate token, so we have this function to do the same for our dates list\n",
    "def add_date_var_comma_token(list):\n",
    "    new_list = []\n",
    "    for i in list:\n",
    "        new_list.append(add_comma_token(i))\n",
    "    return new_list\n",
    "\n",
    "# Gets the first token of each date variation, to allow for faster downstream computation \n",
    "def get_first_token_set(list):\n",
    "    new_set = set()\n",
    "    for i in list:\n",
    "        new_set.add(i.split()[0])\n",
    "    return new_set\n",
    "\n",
    "def get_item_set(row):\n",
    "    item_set = set([])\n",
    "    for i in row['item1'].split():\n",
    "        item_set.add(i)\n",
    "    for j in row['item2'].split():\n",
    "        item_set.add(j)\n",
    "    if 'nan' in item_set:\n",
    "        item_set.remove('nan')\n",
    "    return item_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2015-10-10', 'October 10 , 2015', 'october 10 , 2015', 'October 10th , 2015', 'october 10th , 2015', '10 Oct 2015', '10 oct 2015', '10th Oct 2015', '10th oct 2015', 'Oct 10 2015', 'oct 10 2015', 'Oct 10th 2015', 'oct 10th 2015', '10/10/15', '10/10/2015', '10-10-2015', '10 October 2015', '10 october 2015', '10th October 2015', '10th october 2015', 'October 10', 'october 10', 'October 10th', 'october 10th', '10 Oct', '10 oct', '10th Oct', '10th oct', 'Oct 10', 'oct 10', 'Oct 10th', 'oct 10th', '10/10', '10-10', '10 October', '10 october', '10th October', '10th october']\n"
     ]
    }
   ],
   "source": [
    "# Assign appropriate types\n",
    "string_cols = [\"item1\", \"item2\", \"location\", \"organization\", \"date\"]\n",
    "df[string_cols] = df[string_cols].astype(str)\n",
    "int_cols = [\"weight1\", \"weight2\"]\n",
    "for i in int_cols:\n",
    "    df[i] = df[i].astype('Int64')\n",
    "\n",
    "# Normalize text columns to match tokenizer \n",
    "df['text'] = df['text'].apply(lambda x: separate_weight_unit(x))\n",
    "for i in string_cols:\n",
    "    df[i] = df[i].apply(lambda x: character_norm(x))\n",
    "df['text'] = df['text'].apply(lambda x: x.strip())\n",
    "\n",
    "# Tokenize text\n",
    "df['text_split'] = df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Preprocess orgs and locations \n",
    "df['org_no_space'] = df['organization'].apply(lambda x: remove_spaces(x))\n",
    "df['loc_no_space'] = df['location'].apply(lambda x: remove_spaces(x))\n",
    "\n",
    "# Preprocess ',' and '/' tokens\n",
    "for i in string_cols:\n",
    "    df[i] = df[i].apply(lambda x: add_comma_token(x))\n",
    "for i in [\"item1\", \"item2\"]:\n",
    "    df[i] = df[i].apply(lambda x: add_slash_token(x))\n",
    "\n",
    "# Create set of trash items of interest for each text\n",
    "df['item_set'] = df.apply(get_item_set, axis = 1)\n",
    "\n",
    "# Compute variations of date and weight formats and preprocess into desired formats\n",
    "df['date_vars'] = df['date'].apply(lambda x: date_to_formats(x) if x != 'nan' else str(x))\n",
    "df['weight1_text'] = df['weight1'].apply(lambda x: number_to_words(x)[1] if pd.notnull(x) else \"\")\n",
    "df['weight2_text'] = df['weight2'].apply(lambda x: number_to_words(x)[1] if pd.notnull(x) else \"\")\n",
    "df['date_vars'] = df['date_vars'].apply(lambda x: add_date_var_comma_token(x))\n",
    "df['date_vars_first_token'] = df['date_vars'].apply(lambda x: get_first_token_set(x))\n",
    "\n",
    "# Make string columns lowercase for downstream comparisons\n",
    "lowercase_cols = string_cols + ['organization', 'org_no_space', 'loc_no_space', 'weight1_text', 'weight2_text']\n",
    "for i in lowercase_cols:\n",
    "    df[i] = df[i].apply(lambda x: x.lower())\n",
    "\n",
    "df.head(10)\n",
    "# df.info()\n",
    "print(df.iloc[12]['date_vars'])\n",
    "# df[(df['item1'].str.contains(\"cigarette butts\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into list of words with associated 'B - entity', 'I - entity' or 'O'\n",
    "\n",
    "units = set([\"kilograms\", \"kilogram\", \"kgs\", \"kg\", \"lb\", \"lbs\", \"pounds\", \"pound\"])\n",
    "filler_words = set([\"and\", \"the\", \"a\", \"an\", \",\", \"/\"])\n",
    "\n",
    "def assign_entity_types(row):\n",
    "    words = row['text_split']\n",
    "    new_tags = []\n",
    "    prev_item_tag = False\n",
    "\n",
    "    idx = 0\n",
    "    while (idx < len(words)):\n",
    "        loc_length = len(row['location'].split())\n",
    "        org_length = len(row['organization'].split())\n",
    "        weight1_text_length = len(row['weight1_text'].split())\n",
    "        if row['weight2_text'] != None:\n",
    "            weight2_text_length = len(row['weight2_text'].split())\n",
    "        else:\n",
    "            weight2_text_length = -1\n",
    "        \n",
    "        # Assign location labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        # Does not handle extraneous locations not provided in prompt!\n",
    "        if ((idx <= len(words) - loc_length) and \n",
    "            [x.lower() for x in words[idx : idx + loc_length]] == row['location'].split()):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "            for i in range(1, loc_length):\n",
    "                new_tags.append(\"I-LOC\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['loc_no_space']):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "\n",
    "        # Assign organization labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        elif ((idx <= len(words) - org_length) and \n",
    "            [x.lower() for x in words[idx : idx + org_length]] == (row['organization'].lower().split())):\n",
    "            new_tags.append(\"B-ORG\")            # idea for later: tag acronyms for Orgs?\n",
    "            idx += 1                            \n",
    "            for i in range(1, org_length):\n",
    "                new_tags.append(\"I-ORG\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['org_no_space']):\n",
    "            new_tags.append(\"B-ORG\")      \n",
    "            idx += 1\n",
    "            \n",
    "        # Assign unit labels\n",
    "        elif words[idx] in units:   \n",
    "            new_tags.append(\"B-UNT\")\n",
    "            idx += 1\n",
    "        \n",
    "        # Assign weight labels for numeric and text numbers (consider '-' and non- '-' versions of written numbers?)\n",
    "        elif (words[idx] == str(row['weight1']) or \n",
    "            (not pd.isna(row['weight2']) and words[idx] == str(row['weight2']))): \n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "        elif (not pd.isna(row['weight1']) and (idx <= len(words) - weight1_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight1_text_length]] == row['weight1_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "        elif ((weight2_text_length > 0) and (idx <= len(words) - weight2_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight2_text_length]] == row['weight2_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "\n",
    "        # Assign item labels (dont look for consecutive matches here)\n",
    "        # Does not handle extraneous trash items not provided in prompt!\n",
    "        elif (words[idx] in row['item_set'] and words[idx] not in filler_words):\n",
    "            if prev_item_tag: \n",
    "                new_tags.append(\"I-ITM\")\n",
    "            else:\n",
    "                new_tags.append(\"B-ITM\")\n",
    "                prev_item_tag = True\n",
    "            idx += 1\n",
    "        # Assign date labels (check only first token to minimize computation on each word)\n",
    "        elif (words[idx] in row['date_vars_first_token']):\n",
    "            # Check for complete consecutive match with any of the possible date variations \n",
    "            date_found = False\n",
    "            for date_var in row['date_vars']:\n",
    "                if ((idx <= len(words) - len(date_var.split())) and \n",
    "                    [x.lower() for x in words[idx : idx + len(date_var.split())]] == date_var.lower().split()):\n",
    "                    new_tags.append(\"B-DAT\")\n",
    "                    idx += 1\n",
    "                    for i in range(1, len(date_var.split())):\n",
    "                        new_tags.append(\"I-DAT\")\n",
    "                        idx += 1\n",
    "                    date_found = True\n",
    "                    break\n",
    "            # If the text matches with none of the date_vars, we need to append \"O\"\n",
    "            if not date_found:\n",
    "                new_tags.append(\"O\")\n",
    "                prev_item_tag = False\n",
    "                idx += 1\n",
    "        \n",
    "        else:\n",
    "            new_tags.append(\"O\")\n",
    "            prev_item_tag = False\n",
    "            idx += 1\n",
    "\n",
    "    return list(zip(words, new_tags))\n",
    "\n",
    "df['tagged_entities'] = df.apply(assign_entity_types, axis =1)\n",
    "# assign_entity_types(df.iloc[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences\n",
    "# df['text_split'][13] = ['I', 'have', 'two', 'hundred', 'and', 'twenty', 'dogs', 'and', 'two', 'hundred', 'and', 'fifty-seven', \n",
    "#                         'cats', 'ugly', 'two', 'hundred', 'and', 'twenty-one', 'on', '11', 'Jan', '2020', '01-11-2020']\n",
    "# assign_entity_types(df.iloc[13])\n",
    "# print(df['text_split'][13][22] in df['date_vars_first_token'][13])\n",
    "# print('01-11-2020'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bodziony B-LOC\n",
      ", I-LOC\n",
      "Mount I-LOC\n",
      "120 B-WEI\n",
      "pounds B-UNT\n",
      "trash B-ITM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Memphis Air National Guard Dining Club Inc. made a huge impact at Bodziony, Mount today - 120 pounds of trash were collected during our beach cleanup! #memphisairnationalguard #BeachCleanup #saveouroceans #protectourenvironment'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review newly assigned non-\"O\" tags\n",
    "SAMPLE_NO = 5\n",
    "for i in df.iloc[SAMPLE_NO]['tagged_entities']:\n",
    "    if i[1] != \"O\":\n",
    "        print(i[0], i[1])\n",
    "\n",
    "# print(df['tagged_entities'][SAMPLE_NO])\n",
    "# print(df['item1'][SAMPLE_NO])\n",
    "df['text'][SAMPLE_NO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 21:10:44 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba1e5f1ec3147b1949b86822f356e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 21:10:44 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2023-02-23 21:10:44 INFO: Use device: cpu\n",
      "2023-02-23 21:10:44 INFO: Loading: tokenize\n",
      "2023-02-23 21:10:44 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Use stanza tokenizer to determine sentence chunks \n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "df['text_stanza_tokenize'] = df['text'].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences tagged:  4132\n"
     ]
    }
   ],
   "source": [
    "# Compiles all sentences into a single list of lists (sentences) of word-pairs (word, NER tag)\n",
    "def get_all_sentences1(df):\n",
    "    all_sentences = []\n",
    "    for i in range(len(df)):\n",
    "        idx = 0\n",
    "        for sentence in df.iloc[i]['text_stanza_tokenize'].sentences:\n",
    "            # Check for first word in stanza-tokenized sentence and adjust index within small range to correct\n",
    "            # starting word (Problem: may result in 1 or 2 tokens being truncated from front or end of sentences, \n",
    "            # though this adjustment doesn't happen in every document and only 2-3 times per document when it does)\n",
    "            first_word = sentence.tokens[0].text\n",
    "            try:\n",
    "                if (first_word != df.iloc[i]['tagged_entities'][idx][0]):\n",
    "                    for adj in [-2, -1, 1, 2]:\n",
    "                        if (first_word == df.iloc[i]['tagged_entities'][idx + adj][0]):\n",
    "                            idx = idx + adj\n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "            end_sentence_limit = min(idx+len(sentence.words), len(df.iloc[i]['tagged_entities'])-1)\n",
    "            new_sentence = list(df.iloc[i]['tagged_entities'][idx:end_sentence_limit])\n",
    "            all_sentences.append(new_sentence)\n",
    "            idx += len(sentence.words)\n",
    "    return all_sentences\n",
    "\n",
    "end_sentence = set(['.', '!', '?', '\\n'])\n",
    "\n",
    "# Method to split sentences based on punctuation marks, not based on Stanza-chunked sentences. \n",
    "# Splits text into fewer, longer sentences than get_all_sentences1, but is less likely to truncate a sentence.\n",
    "def get_all_sentences2(df):\n",
    "    all_sentences = []\n",
    "    for i in range(len(df)):\n",
    "        idx = 0\n",
    "        text_length = len(df.iloc[i]['tagged_entities'])\n",
    "        # print(\"text length:\", text_length)\n",
    "        while idx < text_length:\n",
    "            end = text_length - 1\n",
    "            for j in range(idx, text_length):\n",
    "                if df.iloc[i]['tagged_entities'][j][0] in end_sentence:\n",
    "                    end = j\n",
    "                    # print(j)\n",
    "                    break\n",
    "            \n",
    "            # print(\"end\", end)\n",
    "            new_sentence = list(df.iloc[i]['tagged_entities'][idx : end + 1])\n",
    "            all_sentences.append(new_sentence)\n",
    "            idx = end + 1\n",
    "    return all_sentences\n",
    "\n",
    "# print(get_all_sentences2(df.iloc[5:7])[2])\n",
    "all_sentences = get_all_sentences2(df)\n",
    "\n",
    "print(\"# of sentences tagged: \", len(all_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into datasets = (train_sentences, dev_sentences, test_sentences)\n",
    "\n",
    "DEV_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "random.seed(1234)\n",
    "random.shuffle(all_sentences)\n",
    "\n",
    "train_sentences = all_sentences[ : int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT))]\n",
    "dev_sentences = all_sentences[int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT)) : int(len(all_sentences)*(1-TEST_SPLIT))]\n",
    "test_sentences = all_sentences[int(len(all_sentences)*(1-TEST_SPLIT)) : ]\n",
    "\n",
    "# print(len(train_sentences))\n",
    "# print(len(dev_sentences))\n",
    "# print(len(test_sentences))\n",
    "# print(len(all_sentences))\n",
    "\n",
    "datasets = (train_sentences, dev_sentences, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.train.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.train.json\n",
      "3305 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.train.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.train.json\n",
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.dev.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.dev.json\n",
      "413 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.dev.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.dev.json\n",
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.test.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.test.json\n",
      "414 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.test.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth1.test.json\n"
     ]
    }
   ],
   "source": [
    "# Convert file and write to JSON file needed for Stanza modelling\n",
    "out_directory = os.getcwd() + '/Processed_Data'\n",
    "write_dataset(datasets, out_directory, DATA_SELECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
