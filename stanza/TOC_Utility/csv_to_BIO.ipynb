{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "from stanza.utils.datasets.ner.utils import write_dataset\n",
    "from transform_weight_date import number_to_words, date_to_formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in synthetic data\n",
    "df = pd.read_csv('../../data/data/generated/data_230124-172021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use re to replace any instances of \"####kg\" with \"#### kg\" where #### is any continuous \n",
    "# sequence of numbers and unit is one of those listed below\n",
    "def separate_weight_unit(row):\n",
    "    return re.sub(r'([0-9]+)(kgs|kg|lbs|lb|pounds|kilograms)', r\"\\1 \\2\", row)\n",
    "\n",
    "# Function to remove spaces (e.g. \"Take 3\" -> \"Take3\")\n",
    "def remove_spaces(row):\n",
    "    return row.replace(\" \", \"\")\n",
    "\n",
    "def add_loc_comma_token(row):\n",
    "    return row.replace(\",\", \" ,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021-05-25',\n",
       " 'May 25, 2021',\n",
       " '25 May 2021',\n",
       " '05/25/21',\n",
       " '05/25/2021',\n",
       " '05-25-2021',\n",
       " '25 May 2021']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider using Stanza sentence tokenizer instead of nltk word_tokenizer? See csv_to_BIO_stanzaTokenize.ipynb file\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: separate_weight_unit(x))\n",
    "df['text_split'] = df['text'].apply(lambda x: x.strip())\n",
    "df['text_split'] = df['text_split'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Preprocess orgs and locations \n",
    "df['org_no_space'] = df['organization'].apply(lambda x: remove_spaces(x))\n",
    "df['loc_no_space'] = df['location'].apply(lambda x: remove_spaces(x))\n",
    "df['location'] = df['location'].apply(lambda x: add_loc_comma_token(x))\n",
    "\n",
    "# Assign appropriate types\n",
    "string_cols = [\"item1\", \"item2\", \"location\"]\n",
    "df[string_cols] = df[string_cols].astype(str)\n",
    "df['weight2'] = df['weight2'].astype('Int64')\n",
    "\n",
    "# Compute variations of date and weight formats\n",
    "df['date_vars'] = df['date'].apply(lambda x: date_to_formats(x))\n",
    "df['weight1_text'] = df['weight1'].apply(lambda x: number_to_words(x)[1])\n",
    "df['weight2_text'] = df['weight2'].apply(lambda x: number_to_words(x)[1] if pd.notnull(x) else str(x))\n",
    "\n",
    "# Make string columns lowercase for comparisons\n",
    "lowercase_cols = string_cols + ['organization', 'location', 'org_no_space', 'loc_no_space', 'weight1_text', 'weight2_text']\n",
    "for i in lowercase_cols:\n",
    "    df[i] = df[i].apply(lambda x: x.lower())\n",
    "\n",
    "# df.head(15)\n",
    "# df.info()\n",
    "# print(df.iloc[12])\n",
    "df['date_vars'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into list of words with associated 'B - entity', 'I - entity' or 'O'\n",
    "# Look at other preprocessing steps in read_datasets function in convert_bn_daffodil\n",
    "\n",
    "units = [\"kilograms\", \"kilogram\", \"kgs\", \"kg\", \"lb\", \"lbs\", \"pounds\", \"pound\"]\n",
    "\n",
    "def assign_entity_types(row):\n",
    "    words = row['text_split']\n",
    "    new_tags = []\n",
    "    prev_item_tag = False\n",
    "\n",
    "    idx = 0\n",
    "    while (idx < len(words)):\n",
    "        loc_length = len(row['location'].split())\n",
    "        org_length = len(row['organization'].split())\n",
    "        weight1_text_length = len(row['weight1_text'].split())\n",
    "        if row['weight2_text'] != None:\n",
    "            weight2_text_length = len(row['weight2_text'].split())\n",
    "        else:\n",
    "            weight2_text_length = -1\n",
    "        \n",
    "        # Assign location labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        # Does not handle extraneous locations not provided in prompt\n",
    "        if ((idx < len(words) - loc_length) and \n",
    "            [x.lower() for x in words[idx : idx + loc_length]] == row['location'].split()):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "            for i in range(1, loc_length):\n",
    "                new_tags.append(\"I-LOC\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['loc_no_space']):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "\n",
    "        # Assign organization labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        elif ((idx < len(words) - org_length) and \n",
    "            [x.lower() for x in words[idx : idx + org_length]] == (row['organization'].lower().split())):\n",
    "            new_tags.append(\"B-ORG\")            # idea for later: tag acronyms for Orgs?\n",
    "            idx += 1                            \n",
    "            for i in range(1, org_length):\n",
    "                new_tags.append(\"I-ORG\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['org_no_space']):\n",
    "            new_tags.append(\"B-ORG\")      \n",
    "            idx += 1\n",
    "            \n",
    "        # Assign unit labels\n",
    "        elif any(words[idx] == word for word in units):   \n",
    "            new_tags.append(\"B-UNT\")\n",
    "            idx += 1\n",
    "        \n",
    "        # Assign weight labels for numeric and text numbers (consider '-' and non- '-' versions of written numbers?)\n",
    "        elif (words[idx] == str(row['weight1']) or (row['weight2'] != None and words[idx] == str(row['weight2']))): \n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "        elif ((idx < len(words) - weight1_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight1_text_length]] == row['weight1_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "        elif ((weight2_text_length > 0) and (idx < len(words) - weight2_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight2_text_length]] == row['weight2_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "\n",
    "        # Assign item labels (dont look for consecutive matches here)\n",
    "        elif (any(words[idx] == word for word in row['item1'].split()) or \n",
    "                                (row['item2'] != None and any(words[idx] == word for word in row['item2'].split()))):\n",
    "            if prev_item_tag: \n",
    "                new_tags.append(\"I-ITM\")\n",
    "            else:\n",
    "                new_tags.append(\"B-ITM\")\n",
    "                prev_item_tag = True\n",
    "            idx += 1\n",
    "        # Open question: How to assign dates? Need to capture all possible date formats?\n",
    "        # Solution: convert golden value dates to datetime objects then use strftime package to generate \n",
    "        # possible text versions of it\n",
    "        # Assign date labels\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            new_tags.append(\"O\")\n",
    "            prev_item_tag = False\n",
    "            idx += 1\n",
    "\n",
    "    return list(zip(words, new_tags))\n",
    "\n",
    "df['tagged_entities'] = df.apply(assign_entity_types, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences\n",
    "# df['text_split'][13] = ['I', 'have', 'two', 'hundred', 'and', 'twenty', 'dogs', 'and', 'two', 'hundred', 'and', 'fifty-seven', \n",
    "#                         'cats', 'ugly', 'two', 'hundred', 'and', 'twenty-one']\n",
    "# assign_entity_types(df.iloc[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unite B-ORG\n",
      "BVI I-ORG\n",
      "Vauréal B-LOC\n",
      ", I-LOC\n",
      "pico I-LOC\n",
      "Vauréal B-LOC\n",
      ", I-LOC\n",
      "pico I-LOC\n",
      "Unite B-ORG\n",
      "BVI I-ORG\n",
      "Vauréal B-LOC\n",
      ", I-LOC\n",
      "pico I-LOC\n",
      "100 B-WEI\n",
      "pounds B-UNT\n",
      "plastic B-ITM\n",
      "Unite B-ORG\n",
      "BVI I-ORG\n",
      "100 B-WEI\n",
      "pounds B-UNT\n",
      "plastic B-ITM\n",
      "plastic B-ITM\n",
      "Unite B-ORG\n",
      "BVI I-ORG\n",
      "Vauréal B-LOC\n",
      ", I-LOC\n",
      "pico I-LOC\n",
      "Unite B-ORG\n",
      "BVI I-ORG\n",
      "Vauréal B-LOC\n",
      ", I-LOC\n",
      "pico I-LOC\n",
      "[('Unite', 'B-ORG'), ('BVI', 'I-ORG'), ('Achieves', 'O'), ('Incredible', 'O'), ('100-Pound', 'O'), ('Plastic', 'O'), ('Cleanup', 'O'), ('in', 'O'), ('Vauréal', 'B-LOC'), (',', 'I-LOC'), ('pico', 'I-LOC'), ('Vauréal', 'B-LOC'), (',', 'I-LOC'), ('pico', 'I-LOC'), ('–', 'O'), ('On', 'O'), ('Sunday', 'O'), (',', 'O'), ('December', 'O'), ('11th', 'O'), (',', 'O'), ('Unite', 'B-ORG'), ('BVI', 'I-ORG'), ('successfully', 'O'), ('completed', 'O'), ('a', 'O'), ('beach', 'O'), ('cleanup', 'O'), ('project', 'O'), ('in', 'O'), ('Vauréal', 'B-LOC'), (',', 'I-LOC'), ('pico', 'I-LOC'), (',', 'O'), ('resulting', 'O'), ('in', 'O'), ('the', 'O'), ('removal', 'O'), ('of', 'O'), ('an', 'O'), ('incredible', 'O'), ('100', 'B-WEI'), ('pounds', 'B-UNT'), ('of', 'O'), ('plastic', 'B-ITM'), ('from', 'O'), ('the', 'O'), ('beach', 'O'), ('.', 'O'), ('The', 'O'), ('cleanup', 'O'), ('project', 'O'), (',', 'O'), ('which', 'O'), ('was', 'O'), ('spearheaded', 'O'), ('by', 'O'), ('Unite', 'B-ORG'), ('BVI', 'I-ORG'), (',', 'O'), ('was', 'O'), ('supported', 'O'), ('by', 'O'), ('a', 'O'), ('group', 'O'), ('of', 'O'), ('dedicated', 'O'), ('volunteers', 'O'), ('.', 'O'), ('After', 'O'), ('several', 'O'), ('hours', 'O'), ('of', 'O'), ('hard', 'O'), ('work', 'O'), (',', 'O'), ('the', 'O'), ('volunteers', 'O'), ('were', 'O'), ('able', 'O'), ('to', 'O'), ('remove', 'O'), ('a', 'O'), ('total', 'O'), ('of', 'O'), ('100', 'B-WEI'), ('pounds', 'B-UNT'), ('of', 'O'), ('plastic', 'B-ITM'), ('from', 'O'), ('the', 'O'), ('beach', 'O'), ('.', 'O'), ('The', 'O'), ('volunteers', 'O'), ('worked', 'O'), ('tirelessly', 'O'), ('throughout', 'O'), ('the', 'O'), ('day', 'O'), (',', 'O'), ('collecting', 'O'), ('plastic', 'B-ITM'), ('bottles', 'O'), (',', 'O'), ('straws', 'O'), (',', 'O'), ('and', 'O'), ('other', 'O'), ('debris', 'O'), ('in', 'O'), ('order', 'O'), ('to', 'O'), ('remove', 'O'), ('it', 'O'), ('from', 'O'), ('the', 'O'), ('beach', 'O'), ('and', 'O'), ('prevent', 'O'), ('it', 'O'), ('from', 'O'), ('harming', 'O'), ('the', 'O'), ('local', 'O'), ('environment', 'O'), ('.', 'O'), ('Unite', 'B-ORG'), ('BVI', 'I-ORG'), ('would', 'O'), ('like', 'O'), ('to', 'O'), ('thank', 'O'), ('all', 'O'), ('of', 'O'), ('the', 'O'), ('volunteers', 'O'), ('who', 'O'), ('participated', 'O'), ('in', 'O'), ('the', 'O'), ('cleanup', 'O'), ('project', 'O'), ('.', 'O'), ('Their', 'O'), ('hard', 'O'), ('work', 'O'), ('and', 'O'), ('dedication', 'O'), ('have', 'O'), ('made', 'O'), ('a', 'O'), ('significant', 'O'), ('impact', 'O'), ('on', 'O'), ('the', 'O'), ('local', 'O'), ('environment', 'O'), ('and', 'O'), ('have', 'O'), ('made', 'O'), ('Vauréal', 'B-LOC'), (',', 'I-LOC'), ('pico', 'I-LOC'), ('a', 'O'), ('cleaner', 'O'), ('and', 'O'), ('healthier', 'O'), ('place', 'O'), ('.', 'O'), ('Unite', 'B-ORG'), ('BVI', 'I-ORG'), ('will', 'O'), ('continue', 'O'), ('to', 'O'), ('work', 'O'), ('to', 'O'), ('make', 'O'), ('Vauréal', 'B-LOC'), (',', 'I-LOC'), ('pico', 'I-LOC'), ('and', 'O'), ('other', 'O'), ('beaches', 'O'), ('in', 'O'), ('the', 'O'), ('region', 'O'), ('a', 'O'), ('more', 'O'), ('sustainable', 'O'), ('and', 'O'), ('environmentally-friendly', 'O'), ('place', 'O'), ('.', 'O'), ('We', 'O'), ('encourage', 'O'), ('everyone', 'O'), ('to', 'O'), ('do', 'O'), ('their', 'O'), ('part', 'O'), ('to', 'O'), ('help', 'O'), ('preserve', 'O'), ('and', 'O'), ('protect', 'O')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Unite BVI Achieves Incredible 100-Pound Plastic Cleanup in Vauréal, pico\\n\\nVauréal, pico – On Sunday, December 11th, Unite BVI successfully completed a beach cleanup project in Vauréal, pico, resulting in the removal of an incredible 100 pounds of plastic from the beach.\\n\\nThe cleanup project, which was spearheaded by Unite BVI, was supported by a group of dedicated volunteers. After several hours of hard work, the volunteers were able to remove a total of 100 pounds of plastic from the beach.\\n\\nThe volunteers worked tirelessly throughout the day, collecting plastic bottles, straws, and other debris in order to remove it from the beach and prevent it from harming the local environment.\\n\\nUnite BVI would like to thank all of the volunteers who participated in the cleanup project. Their hard work and dedication have made a significant impact on the local environment and have made Vauréal, pico a cleaner and healthier place.\\n\\nUnite BVI will continue to work to make Vauréal, pico and other beaches in the region a more sustainable and environmentally-friendly place. We encourage everyone to do their part to help preserve and protect'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review newly assigned non-\"O\" tags\n",
    "SAMPLE_NO = 12\n",
    "for i in df.iloc[SAMPLE_NO]['tagged_entities']:\n",
    "    if i[1] != \"O\":\n",
    "        print(i[0], i[1])\n",
    "\n",
    "print(df['tagged_entities'][SAMPLE_NO])\n",
    "df['text'][SAMPLE_NO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all sentences into a single list of lists (sentences) of word-pairs (word, NER tag)\n",
    "\n",
    "# Open question: this method is not very robust (cross-references stanza tokenizer sentence lengths \n",
    "# against list of original sentence text words, which might not be 1-1).\n",
    "#   Solution: use a find() to search for first word in each sentence, not just blind indexing into paragraph text\n",
    "\n",
    "# Method is not efficient. Maybe could be vectorized (?), but we only have to run this script once\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "df['text_stanza_tokenize'] = df['text'].apply(lambda x: nlp(x))\n",
    "\n",
    "all_sentences = []\n",
    "for i in range(len(df)):\n",
    "    idx = 0\n",
    "    for sentence in df.iloc[i]['text_stanza_tokenize'].sentences:\n",
    "        new_sentence = list(df.iloc[i]['tagged_entities'][idx:idx+len(sentence.words)])\n",
    "        all_sentences.append(new_sentence)\n",
    "        idx += len(sentence.words)\n",
    "\n",
    "# print(all_sentences[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into datasets = (train_sentences, dev_sentences, test_sentences)\n",
    "\n",
    "DEV_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "random.seed(1234)\n",
    "random.shuffle(all_sentences)\n",
    "\n",
    "train_sentences = all_sentences[ : int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT))]\n",
    "dev_sentences = all_sentences[int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT)) : int(len(all_sentences)*(1-TEST_SPLIT))]\n",
    "test_sentences = all_sentences[int(len(all_sentences)*(1-TEST_SPLIT)) : ]\n",
    "\n",
    "# print(len(train_sentences))\n",
    "# print(len(dev_sentences))\n",
    "# print(len(test_sentences))\n",
    "# print(len(all_sentences))\n",
    "\n",
    "datasets = (train_sentences, dev_sentences, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert file and write to JSON file needed for Stanza modelling\n",
    "out_directory = os.getcwd()\n",
    "write_dataset(datasets, out_directory, \"TOC_Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to JSON file needed by Stanza model\n",
    "# There is a conversion script called several times in prepare_ner_dataset.py which converts IOB format to our internal NER format:\n",
    "# import stanza.utils.datasets.ner.prepare_ner_file as prepare_ner_file\n",
    "\n",
    "# prepare_ner_file.process_dataset(input_iob, output_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
