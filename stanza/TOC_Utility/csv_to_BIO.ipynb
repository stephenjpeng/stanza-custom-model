{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "from stanza.utils.datasets.ner.utils import write_dataset\n",
    "from transform_weight_date import number_to_words, date_to_formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in synthetic data\n",
    "df = pd.read_csv('../../data/data/generated/data_230124-172021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use re to replace any instances of \"####kg\" with \"#### kg\" where #### is any continuous \n",
    "# sequence of numbers and unit is one of those listed below\n",
    "def separate_weight_unit(row):\n",
    "    return re.sub(r'([0-9]+)(kgs|kg|lbs|lb|pounds|kilograms)', r\"\\1 \\2\", row)\n",
    "\n",
    "# Function to remove spaces (e.g. \"Take 3\" -> \"Take3\")\n",
    "def remove_spaces(row):\n",
    "    return row.replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021-05-25',\n",
       " 'May 25, 2021',\n",
       " '25 May 2021',\n",
       " '05/25/21',\n",
       " '05/25/2021',\n",
       " '05-25-2021',\n",
       " '25 May 2021']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider using Stanza sentence tokenizer instead of nltk word_tokenizer? See csv_to_BIO_stanzaTokenize.ipynb file\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: separate_weight_unit(x))\n",
    "df['text_split'] = df['text'].apply(lambda x: x.strip())\n",
    "df['text_split'] = df['text_split'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Remove spaces from orgs and locations \n",
    "df['org_no_space'] = df['organization'].apply(lambda x: remove_spaces(x))\n",
    "df['loc_no_space'] = df['location'].apply(lambda x: remove_spaces(x))\n",
    "\n",
    "# Assign appropriate types\n",
    "string_cols = [\"item1\", \"item2\", \"location\"]\n",
    "df[string_cols] = df[string_cols].astype(str)\n",
    "df['weight2'] = df['weight2'].astype('Int64')\n",
    "\n",
    "# Compute variations of date and weight formats\n",
    "df['date_vars'] = df['date'].apply(lambda x: date_to_formats(x))\n",
    "df['weight1_text'] = df['weight1'].apply(lambda x: number_to_words(x)[1])\n",
    "df['weight2_text'] = df['weight2'].apply(lambda x: number_to_words(x)[1] if pd.notnull(x) else str(x))\n",
    "\n",
    "# Make string columns lowercase for comparisons\n",
    "lowercase_cols = string_cols + ['organization', 'location', 'org_no_space', 'loc_no_space', 'weight1_text', 'weight2_text']\n",
    "for i in lowercase_cols:\n",
    "    df[i] = df[i].apply(lambda x: x.lower())\n",
    "\n",
    "df.head(15)\n",
    "# df.info()\n",
    "# print(df.iloc[12])\n",
    "df['date_vars'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into list of words with associated 'B - entity', 'I - entity' or 'O'\n",
    "# Look at other preprocessing steps in read_datasets function in convert_bn_daffodil\n",
    "\n",
    "units = [\"kilograms\", \"kilogram\", \"kgs\", \"kg\", \"lb\", \"lbs\", \"pounds\", \"pound\"]\n",
    "\n",
    "def assign_entity_types(row):\n",
    "    words = row['text_split']\n",
    "    new_tags = []\n",
    "    prev_item_tag = False\n",
    "\n",
    "    idx = 0\n",
    "    while (idx < len(words)):\n",
    "        loc_length = len(row['location'].split())\n",
    "        org_length = len(row['organization'].split())\n",
    "        weight1_text_length = len(row['weight1_text'].split())\n",
    "        if row['weight2_text'] != None:\n",
    "            weight2_text_length = len(row['weight2_text'].split())\n",
    "        else:\n",
    "            weight2_text_length = -1\n",
    "        \n",
    "        # Assign location labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        # Does not handle extraneous locations not provided in prompt\n",
    "        if ((idx < len(words) - loc_length) and \n",
    "            [x.lower() for x in words[idx : idx + loc_length]] == row['location'].split()):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "            for i in range(1, loc_length):\n",
    "                new_tags.append(\"I-LOC\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['loc_no_space']):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "\n",
    "        # Assign organization labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        elif ((idx < len(words) - org_length) and \n",
    "            [x.lower() for x in words[idx : idx + org_length]] == (row['organization'].lower().split())):\n",
    "            new_tags.append(\"B-ORG\")            # idea for later: tag acronyms for Orgs?\n",
    "            idx += 1                            \n",
    "            for i in range(1, org_length):\n",
    "                new_tags.append(\"I-ORG\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['org_no_space']):\n",
    "            new_tags.append(\"B-ORG\")      \n",
    "            idx += 1\n",
    "            \n",
    "        # Assign unit labels\n",
    "        elif any(words[idx] == word for word in units):   \n",
    "            new_tags.append(\"B-UNT\")\n",
    "            idx += 1\n",
    "        \n",
    "        # Assign weight labels for numeric and text numbers\n",
    "        elif (words[idx] == str(row['weight1']) or (row['weight2'] != None and words[idx] == str(row['weight2']))): \n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "        elif ((idx < len(words) - weight1_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight1_text_length]] == row['weight1_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "        elif ((weight2_text_length > 0) and (idx < len(words) - weight2_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight2_text_length]] == row['weight2_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "\n",
    "        # Assign item labels (dont look for consecutive matches here)\n",
    "        elif (any(words[idx] == word for word in row['item1'].split()) or \n",
    "                                (row['item2'] != None and any(words[idx] == word for word in row['item2'].split()))):\n",
    "            if prev_item_tag: \n",
    "                new_tags.append(\"I-ITM\")\n",
    "            else:\n",
    "                new_tags.append(\"B-ITM\")\n",
    "                prev_item_tag = True\n",
    "            idx += 1\n",
    "        # Open question: How to assign dates? Need to capture all possible date formats?\n",
    "        # Solution: convert golden value dates to datetime objects then use strftime package to generate \n",
    "        # possible text versions of it\n",
    "        # Assign date labels\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            new_tags.append(\"O\")\n",
    "            prev_item_tag = False\n",
    "            idx += 1\n",
    "\n",
    "    return list(zip(words, new_tags))\n",
    "\n",
    "df['tagged_entities'] = df.apply(assign_entity_types, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences\n",
    "# df['text_split'][13] = ['I', 'have', 'two', 'hundred', 'and', 'twenty', 'dogs', 'and', 'two', 'hundred', 'and', 'fifty-seven', \n",
    "#                         'cats', 'ugly', 'two', 'hundred', 'and', 'twenty-one']\n",
    "# assign_entity_types(df.iloc[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reef B-ORG\n",
      "Relief I-ORG\n",
      "Juno B-LOC\n",
      "Peaks I-LOC\n",
      "Reef B-ORG\n",
      "Relief I-ORG\n",
      "Juno B-LOC\n",
      "Peaks I-LOC\n",
      "220 B-WEI\n",
      "pounds B-UNT\n",
      "257 B-WEI\n",
      "pounds B-UNT\n",
      "pizza B-ITM\n",
      "boxes I-ITM\n",
      "Reef B-ORG\n",
      "Relief I-ORG\n",
      "Reef B-ORG\n",
      "Relief I-ORG\n",
      "Reef B-ORG\n",
      "Relief I-ORG\n",
      "Reef B-ORG\n",
      "Relief I-ORG\n",
      "Reef B-ORG\n",
      "Relief I-ORG\n",
      "[('Reef', 'B-ORG'), ('Relief', 'I-ORG'), ('Hosts', 'O'), ('Successful', 'O'), ('Beach', 'O'), ('Cleanup', 'O'), ('at', 'O'), ('Juno', 'B-LOC'), ('Peaks', 'I-LOC'), ('Key', 'O'), ('West', 'O'), (',', 'O'), ('FL', 'O'), ('–', 'O'), ('On', 'O'), ('January', 'O'), ('11', 'O'), (',', 'O'), ('2020', 'O'), (',', 'O'), ('Reef', 'B-ORG'), ('Relief', 'I-ORG'), ('hosted', 'O'), ('a', 'O'), ('successful', 'O'), ('beach', 'O'), ('cleanup', 'O'), ('at', 'O'), ('Juno', 'B-LOC'), ('Peaks', 'I-LOC'), ('.', 'O'), ('After', 'O'), ('an', 'O'), ('hour', 'O'), ('and', 'O'), ('a', 'O'), ('half', 'O'), ('of', 'O'), ('cleaning', 'O'), (',', 'O'), ('a', 'O'), ('total', 'O'), ('of', 'O'), ('220', 'B-WEI'), ('pounds', 'B-UNT'), ('of', 'O'), ('Styrofoam', 'O'), ('containers', 'O'), ('and', 'O'), ('257', 'B-WEI'), ('pounds', 'B-UNT'), ('of', 'O'), ('pizza', 'B-ITM'), ('boxes', 'I-ITM'), ('were', 'O'), ('collected', 'O'), ('.', 'O'), ('“', 'O'), ('We', 'O'), ('’', 'O'), ('re', 'O'), ('so', 'O'), ('proud', 'O'), ('of', 'O'), ('the', 'O'), ('hard', 'O'), ('work', 'O'), ('and', 'O'), ('dedication', 'O'), ('that', 'O'), ('our', 'O'), ('volunteers', 'O'), ('put', 'O'), ('in', 'O'), ('to', 'O'), ('make', 'O'), ('this', 'O'), ('event', 'O'), ('a', 'O'), ('success', 'O'), (',', 'O'), ('”', 'O'), ('said', 'O'), ('Reef', 'B-ORG'), ('Relief', 'I-ORG'), ('Executive', 'O'), ('Director', 'O'), ('DeeVon', 'O'), ('Quirolo', 'O'), ('.', 'O'), ('“', 'O'), ('It', 'O'), ('’', 'O'), ('s', 'O'), ('inspiring', 'O'), ('to', 'O'), ('see', 'O'), ('the', 'O'), ('community', 'O'), ('come', 'O'), ('together', 'O'), ('and', 'O'), ('take', 'O'), ('action', 'O'), ('to', 'O'), ('protect', 'O'), ('and', 'O'), ('preserve', 'O'), ('our', 'O'), ('environment.', 'O'), ('”', 'O'), ('Reef', 'B-ORG'), ('Relief', 'I-ORG'), ('is', 'O'), ('a', 'O'), ('non-profit', 'O'), ('organization', 'O'), ('dedicated', 'O'), ('to', 'O'), ('the', 'O'), ('conservation', 'O'), ('and', 'O'), ('protection', 'O'), ('of', 'O'), ('coral', 'O'), ('reef', 'O'), ('ecosystems', 'O'), ('.', 'O'), ('Reef', 'B-ORG'), ('Relief', 'I-ORG'), ('’', 'O'), ('s', 'O'), ('mission', 'O'), ('is', 'O'), ('to', 'O'), ('be', 'O'), ('a', 'O'), ('voice', 'O'), ('for', 'O'), ('the', 'O'), ('reefs', 'O'), ('.', 'O'), ('The', 'O'), ('organization', 'O'), ('hosts', 'O'), ('beach', 'O'), ('cleanup', 'O'), ('events', 'O'), ('throughout', 'O'), ('the', 'O'), ('year', 'O'), ('to', 'O'), ('remove', 'O'), ('debris', 'O'), ('from', 'O'), ('the', 'O'), ('shoreline', 'O'), ('and', 'O'), ('keep', 'O'), ('the', 'O'), ('ocean', 'O'), ('clean', 'O'), ('.', 'O'), ('Reef', 'B-ORG'), ('Relief', 'I-ORG'), ('encourages', 'O'), ('the', 'O'), ('community', 'O'), ('to', 'O'), ('get', 'O'), ('involved', 'O'), ('and', 'O'), ('help', 'O'), ('preserve', 'O'), ('our', 'O'), ('oceans', 'O'), ('by', 'O'), ('participating', 'O'), ('in', 'O'), ('their', 'O'), ('beach', 'O'), ('cleanup', 'O'), ('events', 'O'), ('.', 'O'), ('To', 'O'), ('learn', 'O'), ('more', 'O'), ('about', 'O'), ('Reef', 'B-ORG'), ('Relief', 'I-ORG'), ('and', 'O'), ('their', 'O'), ('efforts', 'O'), (',', 'O'), ('please', 'O'), ('visit', 'O'), ('their', 'O'), ('website', 'O'), ('at', 'O'), ('www.reefrelief.org', 'O'), ('.', 'O')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nReef Relief Hosts Successful Beach Cleanup at Juno Peaks \\n\\nKey West, FL – On January 11, 2020, Reef Relief hosted a successful beach cleanup at Juno Peaks. After an hour and a half of cleaning, a total of 220 pounds of Styrofoam containers and 257 pounds of pizza boxes were collected.\\n\\n“We’re so proud of the hard work and dedication that our volunteers put in to make this event a success,” said Reef Relief Executive Director DeeVon Quirolo. “It’s inspiring to see the community come together and take action to protect and preserve our environment.”\\n\\nReef Relief is a non-profit organization dedicated to the conservation and protection of coral reef ecosystems. Reef Relief’s mission is to be a voice for the reefs.\\n\\nThe organization hosts beach cleanup events throughout the year to remove debris from the shoreline and keep the ocean clean.\\n\\nReef Relief encourages the community to get involved and help preserve our oceans by participating in their beach cleanup events. To learn more about Reef Relief and their efforts, please visit their website at www.reefrelief.org.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review newly assigned non-\"O\" tags\n",
    "SAMPLE_NO = 13\n",
    "for i in df.iloc[SAMPLE_NO]['tagged_entities']:\n",
    "    if i[1] != \"O\":\n",
    "        print(i[0], i[1])\n",
    "\n",
    "print(df['tagged_entities'][SAMPLE_NO])\n",
    "df['text'][SAMPLE_NO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all sentences into a single list of lists (sentences) of word-pairs (word, NER tag)\n",
    "\n",
    "# Open question: this method is not very robust (cross-references stanza tokenizer sentence lengths \n",
    "# against list of original sentence text words, which might not be 1-1).\n",
    "#   Solution: use a find() to search for first word in each sentence, not just blind indexing into paragraph text\n",
    "\n",
    "# Method is not efficient. Maybe could be vectorized (?), but we only have to run this script once\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "df['text_stanza_tokenize'] = df['text'].apply(lambda x: nlp(x))\n",
    "\n",
    "all_sentences = []\n",
    "for i in range(len(df)):\n",
    "    idx = 0\n",
    "    for sentence in df.iloc[i]['text_stanza_tokenize'].sentences:\n",
    "        new_sentence = list(df.iloc[i]['tagged_entities'][idx:idx+len(sentence.words)])\n",
    "        all_sentences.append(new_sentence)\n",
    "        idx += len(sentence.words)\n",
    "\n",
    "# print(all_sentences[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into datasets = (train_sentences, dev_sentences, test_sentences)\n",
    "\n",
    "DEV_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "random.seed(1234)\n",
    "random.shuffle(all_sentences)\n",
    "\n",
    "train_sentences = all_sentences[ : int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT))]\n",
    "dev_sentences = all_sentences[int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT)) : int(len(all_sentences)*(1-TEST_SPLIT))]\n",
    "test_sentences = all_sentences[int(len(all_sentences)*(1-TEST_SPLIT)) : ]\n",
    "\n",
    "# print(len(train_sentences))\n",
    "# print(len(dev_sentences))\n",
    "# print(len(test_sentences))\n",
    "# print(len(all_sentences))\n",
    "\n",
    "datasets = (train_sentences, dev_sentences, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert file and write to JSON file needed for Stanza modelling\n",
    "out_directory = os.getcwd()\n",
    "write_dataset(datasets, out_directory, \"TOC_Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to JSON file needed by Stanza model\n",
    "# There is a conversion script called several times in prepare_ner_dataset.py which converts IOB format to our internal NER format:\n",
    "# import stanza.utils.datasets.ner.prepare_ner_file as prepare_ner_file\n",
    "\n",
    "# prepare_ner_file.process_dataset(input_iob, output_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
