{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "from stanza.utils.datasets.ner.utils import write_dataset\n",
    "from transform_weight_date import number_to_words, date_to_formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data to be read in from CSV using output\n",
    "toy_data_path = '../../data/data/generated/data_230124-172021.csv'\n",
    "synthetic_data_1_path = '../../../xplore-the-ocean-cleanup/data-generation/data/generated/data_230221-205202.csv'\n",
    "synthetic_data_2_path = '../../../xplore-the-ocean-cleanup/data-generation/data/generated/data_230222-092039.csv'\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "DATA_SELECTION = \"synth2\"\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "if DATA_SELECTION == \"toy\":\n",
    "    data_path = toy_data_path\n",
    "if DATA_SELECTION == \"synth1\":\n",
    "    data_path = synthetic_data_1_path\n",
    "if DATA_SELECTION == \"synth2\":\n",
    "    data_path = synthetic_data_2_path\n",
    "\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use re to replace any instances of \"####kg\" with \"#### kg\" where #### is any continuous \n",
    "# sequence of numbers and unit is one of those listed below\n",
    "def separate_weight_unit(row):\n",
    "    return re.sub(r'([0-9]+)(kgs|kg|lbs|lb|pounds|kilograms)', r\"\\1 \\2\", row)\n",
    "\n",
    "# Function to remove spaces (e.g. \"Take 3\" -> \"Take3\")\n",
    "def remove_spaces(text):\n",
    "    return text.replace(\" \", \"\")\n",
    "\n",
    "# Function to replace long hyphen ASCII code with short hyphen '-' ASCII code\n",
    "def character_norm(text):\n",
    "    return text.replace(chr(8211), \"-\")\n",
    "\n",
    "# Word tokenizer splits ',' into separate token, so we have this function to do the same\n",
    "def add_comma_token(text):\n",
    "    return text.replace(\",\", \" ,\")\n",
    "\n",
    "# Split '/' into its own token   JOE TO UPDATE THIS TINY EDGE CASE\n",
    "def add_slash_token(text):\n",
    "    return text.replace(chr(47), \" / \")\n",
    "\n",
    "# Word tokenizer splits ',' into separate token, so we have this function to do the same for our dates list\n",
    "def add_date_var_comma_token(list):\n",
    "    new_list = []\n",
    "    for i in list:\n",
    "        new_list.append(add_comma_token(i))\n",
    "    return new_list\n",
    "\n",
    "# Gets the first token of each date variation, to allow for faster downstream computation \n",
    "def get_first_token_set(list):\n",
    "    new_set = set()\n",
    "    for i in list:\n",
    "        new_set.add(i.split()[0])\n",
    "    return new_set\n",
    "\n",
    "def get_item_set(row):\n",
    "    item_set = set([])\n",
    "    for i in row['item1'].split():\n",
    "        item_set.add(i)\n",
    "    for j in row['item2'].split():\n",
    "        item_set.add(j)\n",
    "    if 'nan' in item_set:\n",
    "        item_set.remove('nan')\n",
    "    return item_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>organization</th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>unit</th>\n",
       "      <th>weight1</th>\n",
       "      <th>item1</th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "      <th>weight2</th>\n",
       "      <th>item2</th>\n",
       "      <th>text_split</th>\n",
       "      <th>org_no_space</th>\n",
       "      <th>loc_no_space</th>\n",
       "      <th>item_set</th>\n",
       "      <th>date_vars</th>\n",
       "      <th>weight1_text</th>\n",
       "      <th>weight2_text</th>\n",
       "      <th>date_vars_first_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>playa del cocal</td>\n",
       "      <td>beach warriors</td>\n",
       "      <td>press release</td>\n",
       "      <td>2021-08-07</td>\n",
       "      <td>pounds</td>\n",
       "      <td>500</td>\n",
       "      <td>plastic</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>BEACH WARRIORS MAKE A MAJOR IMPACT WITH 500 PO...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[BEACH, WARRIORS, MAKE, A, MAJOR, IMPACT, WITH...</td>\n",
       "      <td>beachwarriors</td>\n",
       "      <td>playadelcocal</td>\n",
       "      <td>{plastic}</td>\n",
       "      <td>[2021-08-07, August 7 , 2021, august 7 , 2021,...</td>\n",
       "      <td>five hundred</td>\n",
       "      <td></td>\n",
       "      <td>{08/07/21, 2021-08-07, 7th, Aug, august, 7, au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>playa palma real</td>\n",
       "      <td>solid waste management and development corp</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>pounds</td>\n",
       "      <td>40</td>\n",
       "      <td>light sticks</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>We just spent the day at Playa Palma Real and ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[We, just, spent, the, day, at, Playa, Palma, ...</td>\n",
       "      <td>solidwastemanagementanddevelopmentcorp</td>\n",
       "      <td>playapalmareal</td>\n",
       "      <td>{light, sticks}</td>\n",
       "      <td>[2019-09-01, September 1 , 2019, september 1 ,...</td>\n",
       "      <td>forty</td>\n",
       "      <td></td>\n",
       "      <td>{09/01/19, sep, september, 09-01-2019, 09-01, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>isla playa grande</td>\n",
       "      <td>community builders exchange inc</td>\n",
       "      <td>press release</td>\n",
       "      <td>2017-10-11</td>\n",
       "      <td>units</td>\n",
       "      <td>200</td>\n",
       "      <td>single-use carrier bags</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>Isla Playa Grande, October 11th, 2017 – Commun...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Isla, Playa, Grande, ,, October, 11th, ,, 201...</td>\n",
       "      <td>communitybuildersexchangeinc</td>\n",
       "      <td>islaplayagrande</td>\n",
       "      <td>{bags, carrier, single-use}</td>\n",
       "      <td>[2017-10-11, October 11 , 2017, october 11 , 2...</td>\n",
       "      <td>two hundred</td>\n",
       "      <td></td>\n",
       "      <td>{10/11, Oct, october, oct, 10-11, 11th, 10/11/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beach ridge</td>\n",
       "      <td>black oaks center for sustainable renewable li...</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2022-10-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>trash</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>We had an amazing time today at Beach Ridge fo...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[We, had, an, amazing, time, today, at, Beach,...</td>\n",
       "      <td>blackoakscenterforsustainablerenewablelivingnfp</td>\n",
       "      <td>beachridge</td>\n",
       "      <td>{trash}</td>\n",
       "      <td>[2022-10-14, October 14 , 2022, october 14 , 2...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>{14, 14th, october, 10/14, oct, 10-14, 10/14/2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nan</td>\n",
       "      <td>wolf river preservation association</td>\n",
       "      <td>press release</td>\n",
       "      <td>2019-05-02</td>\n",
       "      <td>kgs</td>\n",
       "      <td>200</td>\n",
       "      <td>trash</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>FOR IMMEDIATE RELEASE\\n\\nThe Wolf River Preser...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[FOR, IMMEDIATE, RELEASE, The, Wolf, River, Pr...</td>\n",
       "      <td>wolfriverpreservationassociation</td>\n",
       "      <td>nan</td>\n",
       "      <td>{trash}</td>\n",
       "      <td>[2019-05-02, May 2 , 2019, may 2 , 2019, May 2...</td>\n",
       "      <td>two hundred</td>\n",
       "      <td></td>\n",
       "      <td>{2019-05-02, 05-02-2019, 05/02, 2, 05-02, 05/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>manama</td>\n",
       "      <td>summit downtown inc</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>lbs</td>\n",
       "      <td>420</td>\n",
       "      <td>plastic</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>This #WorldAnimalDay, @SummitDowntownInc joine...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[This, #, WorldAnimalDay, ,, @, SummitDowntown...</td>\n",
       "      <td>summitdowntowninc</td>\n",
       "      <td>manama</td>\n",
       "      <td>{plastic}</td>\n",
       "      <td>[2022-10-04, October 4 , 2022, october 4 , 202...</td>\n",
       "      <td>four hundred and twenty</td>\n",
       "      <td></td>\n",
       "      <td>{4, 10-04, 2022-10-04, 10/04, october, 10-04-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>playa del rey</td>\n",
       "      <td>environmental recycling inc</td>\n",
       "      <td>press release</td>\n",
       "      <td>nan</td>\n",
       "      <td>pounds</td>\n",
       "      <td>190</td>\n",
       "      <td>plastic</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>Environmental Recycling Inc Makes Major Impact...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Environmental, Recycling, Inc, Makes, Major, ...</td>\n",
       "      <td>environmentalrecyclinginc</td>\n",
       "      <td>playadelrey</td>\n",
       "      <td>{plastic}</td>\n",
       "      <td>[n, a, n]</td>\n",
       "      <td>one hundred and ninety</td>\n",
       "      <td></td>\n",
       "      <td>{a, n}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>carolinas heights</td>\n",
       "      <td>clear blue skies inc</td>\n",
       "      <td>instagram caption</td>\n",
       "      <td>2021-08-30</td>\n",
       "      <td>pounds</td>\n",
       "      <td>190</td>\n",
       "      <td>plastic</td>\n",
       "      <td>Generate an instagram caption for a beach clea...</td>\n",
       "      <td>We just made a difference at Carolinas Heights...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[We, just, made, a, difference, at, Carolinas,...</td>\n",
       "      <td>clearblueskiesinc</td>\n",
       "      <td>carolinasheights</td>\n",
       "      <td>{plastic}</td>\n",
       "      <td>[2021-08-30, August 30 , 2021, august 30 , 202...</td>\n",
       "      <td>one hundred and ninety</td>\n",
       "      <td></td>\n",
       "      <td>{08/30/21, Aug, 08/30, 2021-08-30, 08-30, augu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>onitsha</td>\n",
       "      <td>union de residentes para la proteccion ambient...</td>\n",
       "      <td>press release</td>\n",
       "      <td>2016-05-16</td>\n",
       "      <td>units</td>\n",
       "      <td>490</td>\n",
       "      <td>plastified paper bags and diapers</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>For Immediate Release\\n\\nOn Sunday, May 16th, ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[For, Immediate, Release, On, Sunday, ,, May, ...</td>\n",
       "      <td>unionderesidentesparalaproteccionambientaldeva...</td>\n",
       "      <td>onitsha</td>\n",
       "      <td>{bags, plastified, and, paper, diapers}</td>\n",
       "      <td>[2016-05-16, May 16 , 2016, may 16 , 2016, May...</td>\n",
       "      <td>four hundred and ninety</td>\n",
       "      <td></td>\n",
       "      <td>{2016-05-16, 16th, 05-16-2016, 05/16, 05/16/16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>katsina</td>\n",
       "      <td>trout unlimited</td>\n",
       "      <td>press release</td>\n",
       "      <td>nan</td>\n",
       "      <td>kilograms</td>\n",
       "      <td>400</td>\n",
       "      <td>normal papers</td>\n",
       "      <td>Generate a press release for a beach cleanup w...</td>\n",
       "      <td>Katsina, Nigeria – On Saturday, Trout Unlimite...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>nan</td>\n",
       "      <td>[Katsina, ,, Nigeria, –, On, Saturday, ,, Trou...</td>\n",
       "      <td>troutunlimited</td>\n",
       "      <td>katsina</td>\n",
       "      <td>{normal, papers}</td>\n",
       "      <td>[n, a, n]</td>\n",
       "      <td>four hundred</td>\n",
       "      <td></td>\n",
       "      <td>{a, n}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            location                                       organization  \\\n",
       "0    playa del cocal                                     beach warriors   \n",
       "1   playa palma real        solid waste management and development corp   \n",
       "2  isla playa grande                    community builders exchange inc   \n",
       "3        beach ridge  black oaks center for sustainable renewable li...   \n",
       "4                nan                wolf river preservation association   \n",
       "5             manama                                summit downtown inc   \n",
       "6      playa del rey                        environmental recycling inc   \n",
       "7  carolinas heights                               clear blue skies inc   \n",
       "8            onitsha  union de residentes para la proteccion ambient...   \n",
       "9            katsina                                    trout unlimited   \n",
       "\n",
       "                type        date       unit  weight1  \\\n",
       "0      press release  2021-08-07     pounds      500   \n",
       "1  instagram caption  2019-09-01     pounds       40   \n",
       "2      press release  2017-10-11      units      200   \n",
       "3  instagram caption  2022-10-14        NaN     <NA>   \n",
       "4      press release  2019-05-02        kgs      200   \n",
       "5  instagram caption  2022-10-04        lbs      420   \n",
       "6      press release         nan     pounds      190   \n",
       "7  instagram caption  2021-08-30     pounds      190   \n",
       "8      press release  2016-05-16      units      490   \n",
       "9      press release         nan  kilograms      400   \n",
       "\n",
       "                               item1  \\\n",
       "0                            plastic   \n",
       "1                       light sticks   \n",
       "2            single-use carrier bags   \n",
       "3                              trash   \n",
       "4                              trash   \n",
       "5                            plastic   \n",
       "6                            plastic   \n",
       "7                            plastic   \n",
       "8  plastified paper bags and diapers   \n",
       "9                      normal papers   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Generate a press release for a beach cleanup w...   \n",
       "1  Generate an instagram caption for a beach clea...   \n",
       "2  Generate a press release for a beach cleanup w...   \n",
       "3  Generate an instagram caption for a beach clea...   \n",
       "4  Generate a press release for a beach cleanup w...   \n",
       "5  Generate an instagram caption for a beach clea...   \n",
       "6  Generate a press release for a beach cleanup w...   \n",
       "7  Generate an instagram caption for a beach clea...   \n",
       "8  Generate a press release for a beach cleanup w...   \n",
       "9  Generate a press release for a beach cleanup w...   \n",
       "\n",
       "                                                text  weight2 item2  \\\n",
       "0  BEACH WARRIORS MAKE A MAJOR IMPACT WITH 500 PO...     <NA>   nan   \n",
       "1  We just spent the day at Playa Palma Real and ...     <NA>   nan   \n",
       "2  Isla Playa Grande, October 11th, 2017 – Commun...     <NA>   nan   \n",
       "3  We had an amazing time today at Beach Ridge fo...     <NA>   nan   \n",
       "4  FOR IMMEDIATE RELEASE\\n\\nThe Wolf River Preser...     <NA>   nan   \n",
       "5  This #WorldAnimalDay, @SummitDowntownInc joine...     <NA>   nan   \n",
       "6  Environmental Recycling Inc Makes Major Impact...     <NA>   nan   \n",
       "7  We just made a difference at Carolinas Heights...     <NA>   nan   \n",
       "8  For Immediate Release\\n\\nOn Sunday, May 16th, ...     <NA>   nan   \n",
       "9  Katsina, Nigeria – On Saturday, Trout Unlimite...     <NA>   nan   \n",
       "\n",
       "                                          text_split  \\\n",
       "0  [BEACH, WARRIORS, MAKE, A, MAJOR, IMPACT, WITH...   \n",
       "1  [We, just, spent, the, day, at, Playa, Palma, ...   \n",
       "2  [Isla, Playa, Grande, ,, October, 11th, ,, 201...   \n",
       "3  [We, had, an, amazing, time, today, at, Beach,...   \n",
       "4  [FOR, IMMEDIATE, RELEASE, The, Wolf, River, Pr...   \n",
       "5  [This, #, WorldAnimalDay, ,, @, SummitDowntown...   \n",
       "6  [Environmental, Recycling, Inc, Makes, Major, ...   \n",
       "7  [We, just, made, a, difference, at, Carolinas,...   \n",
       "8  [For, Immediate, Release, On, Sunday, ,, May, ...   \n",
       "9  [Katsina, ,, Nigeria, –, On, Saturday, ,, Trou...   \n",
       "\n",
       "                                        org_no_space      loc_no_space  \\\n",
       "0                                      beachwarriors     playadelcocal   \n",
       "1             solidwastemanagementanddevelopmentcorp    playapalmareal   \n",
       "2                       communitybuildersexchangeinc   islaplayagrande   \n",
       "3    blackoakscenterforsustainablerenewablelivingnfp        beachridge   \n",
       "4                   wolfriverpreservationassociation               nan   \n",
       "5                                  summitdowntowninc            manama   \n",
       "6                          environmentalrecyclinginc       playadelrey   \n",
       "7                                  clearblueskiesinc  carolinasheights   \n",
       "8  unionderesidentesparalaproteccionambientaldeva...           onitsha   \n",
       "9                                     troutunlimited           katsina   \n",
       "\n",
       "                                  item_set  \\\n",
       "0                                {plastic}   \n",
       "1                          {light, sticks}   \n",
       "2              {bags, carrier, single-use}   \n",
       "3                                  {trash}   \n",
       "4                                  {trash}   \n",
       "5                                {plastic}   \n",
       "6                                {plastic}   \n",
       "7                                {plastic}   \n",
       "8  {bags, plastified, and, paper, diapers}   \n",
       "9                         {normal, papers}   \n",
       "\n",
       "                                           date_vars             weight1_text  \\\n",
       "0  [2021-08-07, August 7 , 2021, august 7 , 2021,...             five hundred   \n",
       "1  [2019-09-01, September 1 , 2019, september 1 ,...                    forty   \n",
       "2  [2017-10-11, October 11 , 2017, october 11 , 2...              two hundred   \n",
       "3  [2022-10-14, October 14 , 2022, october 14 , 2...                            \n",
       "4  [2019-05-02, May 2 , 2019, may 2 , 2019, May 2...              two hundred   \n",
       "5  [2022-10-04, October 4 , 2022, october 4 , 202...  four hundred and twenty   \n",
       "6                                          [n, a, n]   one hundred and ninety   \n",
       "7  [2021-08-30, August 30 , 2021, august 30 , 202...   one hundred and ninety   \n",
       "8  [2016-05-16, May 16 , 2016, may 16 , 2016, May...  four hundred and ninety   \n",
       "9                                          [n, a, n]             four hundred   \n",
       "\n",
       "  weight2_text                              date_vars_first_token  \n",
       "0               {08/07/21, 2021-08-07, 7th, Aug, august, 7, au...  \n",
       "1               {09/01/19, sep, september, 09-01-2019, 09-01, ...  \n",
       "2               {10/11, Oct, october, oct, 10-11, 11th, 10/11/...  \n",
       "3               {14, 14th, october, 10/14, oct, 10-14, 10/14/2...  \n",
       "4               {2019-05-02, 05-02-2019, 05/02, 2, 05-02, 05/0...  \n",
       "5               {4, 10-04, 2022-10-04, 10/04, october, 10-04-2...  \n",
       "6                                                          {a, n}  \n",
       "7               {08/30/21, Aug, 08/30, 2021-08-30, 08-30, augu...  \n",
       "8               {2016-05-16, 16th, 05-16-2016, 05/16, 05/16/16...  \n",
       "9                                                          {a, n}  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign appropriate types\n",
    "string_cols = [\"item1\", \"item2\", \"location\", \"organization\", \"date\"]\n",
    "df[string_cols] = df[string_cols].astype(str)\n",
    "int_cols = [\"weight1\", \"weight2\"]\n",
    "for i in int_cols:\n",
    "    df[i] = df[i].astype('Int64')\n",
    "\n",
    "# Normalize text columns to match tokenizer \n",
    "df['text'] = df['text'].apply(lambda x: separate_weight_unit(x))\n",
    "for i in string_cols:\n",
    "    df[i] = df[i].apply(lambda x: character_norm(x))\n",
    "df['text'] = df['text'].apply(lambda x: x.strip())\n",
    "\n",
    "# Tokenize text\n",
    "df['text_split'] = df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Preprocess orgs and locations \n",
    "df['org_no_space'] = df['organization'].apply(lambda x: remove_spaces(x))\n",
    "df['loc_no_space'] = df['location'].apply(lambda x: remove_spaces(x))\n",
    "\n",
    "# Preprocess ',' and '/' tokens\n",
    "for i in string_cols:\n",
    "    df[i] = df[i].apply(lambda x: add_comma_token(x))\n",
    "for i in [\"item1\", \"item2\"]:\n",
    "    df[i] = df[i].apply(lambda x: add_slash_token(x))\n",
    "\n",
    "# Create set of trash items of interest for each text\n",
    "df['item_set'] = df.apply(get_item_set, axis = 1)\n",
    "\n",
    "# Compute variations of date and weight formats and preprocess into desired formats\n",
    "df['date_vars'] = df['date'].apply(lambda x: date_to_formats(x) if x != 'nan' else str(x))\n",
    "df['weight1_text'] = df['weight1'].apply(lambda x: number_to_words(x)[1] if pd.notnull(x) else \"\")\n",
    "df['weight2_text'] = df['weight2'].apply(lambda x: number_to_words(x)[1] if pd.notnull(x) else \"\")\n",
    "df['date_vars'] = df['date_vars'].apply(lambda x: add_date_var_comma_token(x))\n",
    "df['date_vars_first_token'] = df['date_vars'].apply(lambda x: get_first_token_set(x))\n",
    "\n",
    "# Make string columns lowercase for downstream comparisons\n",
    "lowercase_cols = string_cols + ['organization', 'org_no_space', 'loc_no_space', 'weight1_text', 'weight2_text']\n",
    "for i in lowercase_cols:\n",
    "    df[i] = df[i].apply(lambda x: x.lower())\n",
    "\n",
    "df.head(10)\n",
    "# df.info()\n",
    "# print(df.iloc[12]['date_vars'])\n",
    "# df[(df['item1'].str.contains(\"cigarette butts\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into list of words with associated 'B - entity', 'I - entity' or 'O'\n",
    "\n",
    "units = set([\"kilograms\", \"kilogram\", \"kgs\", \"kg\", \"lb\", \"lbs\", \"pounds\", \"pound\"])\n",
    "filler_words = set([\"and\", \"the\", \"a\", \"an\", \",\", \"/\"])\n",
    "\n",
    "def assign_entity_types(row):\n",
    "    words = row['text_split']\n",
    "    new_tags = []\n",
    "    prev_item_tag = False\n",
    "\n",
    "    idx = 0\n",
    "    while (idx < len(words)):\n",
    "        loc_length = len(row['location'].split())\n",
    "        org_length = len(row['organization'].split())\n",
    "        weight1_text_length = len(row['weight1_text'].split())\n",
    "        if row['weight2_text'] != None:\n",
    "            weight2_text_length = len(row['weight2_text'].split())\n",
    "        else:\n",
    "            weight2_text_length = -1\n",
    "        \n",
    "        # Assign location labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        # Does not handle extraneous locations not provided in prompt!\n",
    "        if ((idx <= len(words) - loc_length) and \n",
    "            [x.lower() for x in words[idx : idx + loc_length]] == row['location'].split()):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "            for i in range(1, loc_length):\n",
    "                new_tags.append(\"I-LOC\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['loc_no_space']):\n",
    "            new_tags.append(\"B-LOC\")\n",
    "            idx += 1\n",
    "\n",
    "        # Assign organization labels\n",
    "        # Checks for consecutive word matching for full location name (normalizing all words to lowercase)\n",
    "        elif ((idx <= len(words) - org_length) and \n",
    "            [x.lower() for x in words[idx : idx + org_length]] == (row['organization'].lower().split())):\n",
    "            new_tags.append(\"B-ORG\")            # idea for later: tag acronyms for Orgs?\n",
    "            idx += 1                            \n",
    "            for i in range(1, org_length):\n",
    "                new_tags.append(\"I-ORG\")\n",
    "                idx += 1\n",
    "        elif (words[idx].lower() == row['org_no_space']):\n",
    "            new_tags.append(\"B-ORG\")      \n",
    "            idx += 1\n",
    "            \n",
    "        # Assign unit labels\n",
    "        elif words[idx] in units:   \n",
    "            new_tags.append(\"B-UNT\")\n",
    "            idx += 1\n",
    "        \n",
    "        # Assign weight labels for numeric and text numbers (consider '-' and non- '-' versions of written numbers?)\n",
    "        elif (words[idx] == str(row['weight1']) or \n",
    "            (not pd.isna(row['weight2']) and words[idx] == str(row['weight2']))): \n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "        elif (not pd.isna(row['weight1']) and (idx <= len(words) - weight1_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight1_text_length]] == row['weight1_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "        elif ((weight2_text_length > 0) and (idx <= len(words) - weight2_text_length) and \n",
    "                [x.lower() for x in words[idx : idx + weight2_text_length]] == row['weight2_text'].split()):\n",
    "            new_tags.append(\"B-WEI\")\n",
    "            idx += 1\n",
    "            for i in range(1, weight1_text_length):\n",
    "                new_tags.append(\"I-WEI\")\n",
    "                idx += 1\n",
    "\n",
    "        # Assign item labels (dont look for consecutive matches here)\n",
    "        # Does not handle extraneous trash items not provided in prompt!\n",
    "        elif (words[idx] in row['item_set'] and words[idx] not in filler_words):\n",
    "            if prev_item_tag: \n",
    "                new_tags.append(\"I-ITM\")\n",
    "            else:\n",
    "                new_tags.append(\"B-ITM\")\n",
    "                prev_item_tag = True\n",
    "            idx += 1\n",
    "        # Assign date labels (check only first token to minimize computation on each word)\n",
    "        elif (words[idx] in row['date_vars_first_token']):\n",
    "            # Check for complete consecutive match with any of the possible date variations \n",
    "            date_found = False\n",
    "            for date_var in row['date_vars']:\n",
    "                if ((idx <= len(words) - len(date_var.split())) and \n",
    "                    [x.lower() for x in words[idx : idx + len(date_var.split())]] == date_var.lower().split()):\n",
    "                    new_tags.append(\"B-DAT\")\n",
    "                    idx += 1\n",
    "                    for i in range(1, len(date_var.split())):\n",
    "                        new_tags.append(\"I-DAT\")\n",
    "                        idx += 1\n",
    "                    date_found = True\n",
    "                    break\n",
    "            # If the text matches with none of the date_vars, we need to append \"O\"\n",
    "            if not date_found:\n",
    "                new_tags.append(\"O\")\n",
    "                prev_item_tag = False\n",
    "                idx += 1\n",
    "        \n",
    "        else:\n",
    "            new_tags.append(\"O\")\n",
    "            prev_item_tag = False\n",
    "            idx += 1\n",
    "\n",
    "    return list(zip(words, new_tags))\n",
    "\n",
    "df['tagged_entities'] = df.apply(assign_entity_types, axis =1)\n",
    "# assign_entity_types(df.iloc[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences\n",
    "# df['text_split'][13] = ['I', 'have', 'two', 'hundred', 'and', 'twenty', 'dogs', 'and', 'two', 'hundred', 'and', 'fifty-seven', \n",
    "#                         'cats', 'ugly', 'two', 'hundred', 'and', 'twenty-one', 'on', '11', 'Jan', '2020', '01-11-2020']\n",
    "# assign_entity_types(df.iloc[13])\n",
    "# print(df['text_split'][13][22] in df['date_vars_first_token'][13])\n",
    "# print('01-11-2020'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SummitDowntownInc B-ORG\n",
      "Manama B-LOC\n",
      "420 B-WEI\n",
      "lbs B-UNT\n",
      "plastic B-ITM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This #WorldAnimalDay, @SummitDowntownInc joined forces with Manama for an epic beach cleanup! We’re proud to announce that 420 lbs of plastic was collected and will be recycled! #ManamaCares #CleanBeaches #MoveTheNeedle #PlasticFreeWorld'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review newly assigned non-\"O\" tags\n",
    "SAMPLE_NO = 5\n",
    "for i in df.iloc[SAMPLE_NO]['tagged_entities']:\n",
    "    if i[1] != \"O\":\n",
    "        print(i[0], i[1])\n",
    "\n",
    "# print(df['tagged_entities'][SAMPLE_NO])\n",
    "# print(df['item1'][SAMPLE_NO])\n",
    "df['text'][SAMPLE_NO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use stanza tokenizer to determine sentence chunks \n",
    "\n",
    "# nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "# df['text_stanza_tokenize'] = df['text'].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences tagged:  4990\n"
     ]
    }
   ],
   "source": [
    "# Compiles all sentences into a single list of lists (sentences) of word-pairs (word, NER tag)\n",
    "def get_all_sentences1(df):\n",
    "    all_sentences = []\n",
    "    for i in range(len(df)):\n",
    "        idx = 0\n",
    "        for sentence in df.iloc[i]['text_stanza_tokenize'].sentences:\n",
    "            # Check for first word in stanza-tokenized sentence and adjust index within small range to correct\n",
    "            # starting word (Problem: may result in 1 or 2 tokens being truncated from front or end of sentences, \n",
    "            # though this adjustment doesn't happen in every document and only 2-3 times per document when it does)\n",
    "            first_word = sentence.tokens[0].text\n",
    "            try:\n",
    "                if (first_word != df.iloc[i]['tagged_entities'][idx][0]):\n",
    "                    for adj in [-2, -1, 1, 2]:\n",
    "                        if (first_word == df.iloc[i]['tagged_entities'][idx + adj][0]):\n",
    "                            idx = idx + adj\n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "            end_sentence_limit = min(idx+len(sentence.words), len(df.iloc[i]['tagged_entities'])-1)\n",
    "            new_sentence = list(df.iloc[i]['tagged_entities'][idx:end_sentence_limit])\n",
    "            all_sentences.append(new_sentence)\n",
    "            idx += len(sentence.words)\n",
    "    return all_sentences\n",
    "\n",
    "end_sentence = set(['.', '!', '?', '\\n'])\n",
    "\n",
    "# Method to split sentences based on punctuation marks, not based on Stanza-chunked sentences. \n",
    "# Splits text into fewer, longer sentences than get_all_sentences1, but is less likely to truncate a sentence.\n",
    "def get_all_sentences2(df):\n",
    "    all_sentences = []\n",
    "    for i in range(len(df)):\n",
    "        idx = 0\n",
    "        text_length = len(df.iloc[i]['tagged_entities'])\n",
    "        # print(\"text length:\", text_length)\n",
    "        while idx < text_length:\n",
    "            end = text_length - 1\n",
    "            for j in range(idx, text_length):\n",
    "                if df.iloc[i]['tagged_entities'][j][0] in end_sentence:\n",
    "                    end = j\n",
    "                    # print(j)\n",
    "                    break\n",
    "            \n",
    "            # print(\"end\", end)\n",
    "            new_sentence = list(df.iloc[i]['tagged_entities'][idx : end + 1])\n",
    "            all_sentences.append(new_sentence)\n",
    "            idx = end + 1\n",
    "    return all_sentences\n",
    "\n",
    "# print(get_all_sentences2(df.iloc[5:7])[2])\n",
    "all_sentences = get_all_sentences2(df)\n",
    "\n",
    "print(\"# of sentences tagged: \", len(all_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into datasets = (train_sentences, dev_sentences, test_sentences)\n",
    "\n",
    "DEV_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "random.seed(1234)\n",
    "random.shuffle(all_sentences)\n",
    "\n",
    "train_sentences = all_sentences[ : int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT))]\n",
    "dev_sentences = all_sentences[int(len(all_sentences)*(1-DEV_SPLIT-TEST_SPLIT)) : int(len(all_sentences)*(1-TEST_SPLIT))]\n",
    "test_sentences = all_sentences[int(len(all_sentences)*(1-TEST_SPLIT)) : ]\n",
    "\n",
    "# print(len(train_sentences))\n",
    "# print(len(dev_sentences))\n",
    "# print(len(test_sentences))\n",
    "# print(len(all_sentences))\n",
    "\n",
    "datasets = (train_sentences, dev_sentences, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth2.train.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth2.train.json\n",
      "3992 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth2.train.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth2.train.json\n",
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth2.dev.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth2.dev.json\n",
      "499 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth2.dev.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth2.dev.json\n",
      "Converting /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth2.test.bio to /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth2.test.json\n",
      "499 examples loaded from /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth2.test.bio\n",
      "Generated json file /Users/josephjamison/Documents/Joe_Documents/Stanford/CME291/stanza-custom-model/stanza/TOC_Utility/Processed_Data/synth2.test.json\n"
     ]
    }
   ],
   "source": [
    "# Convert file and write to JSON file needed for Stanza modelling\n",
    "out_directory = os.getcwd() + '/Processed_Data'\n",
    "write_dataset(datasets, out_directory, DATA_SELECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
