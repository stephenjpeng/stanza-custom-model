{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from stanza.models.ner import scorer\n",
    "from stanza.utils.confusion import format_confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Assigned</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>an</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awesome</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>day</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>with</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Token Assigned Predicted\n",
       "0     What        O         O\n",
       "1       an        O         O\n",
       "2  awesome        O         O\n",
       "3      day        O         O\n",
       "4     with        O         O"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in manually annotated instagram captions and model's predictions\n",
    "file_loc = '/manual_annotated_feb_ig_subset.tsv'\n",
    "file_path = os.getcwd() + file_loc\n",
    "model_output = pd.read_table(file_path)\n",
    "model_output.columns=[\"Token\", \"Assigned\", \"Predicted\"]\n",
    "model_output.reset_index()\n",
    "model_output.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 17:26:11 INFO: Score by entity:\n",
      "Prec.\tRec.\tF1\n",
      "76.19\t62.34\t68.57\n",
      "2023-03-13 17:26:11 INFO: Score by token:\n",
      "Prec.\tRec.\tF1\n",
      "79.05\t64.84\t71.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     t/p      O B-DAT E-DAT I-DAT S-DAT B-ITM E-ITM I-ITM S-ITM B-LOC E-LOC I-LOC S-LOC B-ORG E-ORG I-ORG S-ORG S-UNT S-WEI\n",
      "        O  1115     0     0     0     1     2     0     0     1     0     1     0     1     2     2     1     0     0     4\n",
      "    B-DAT     1     4     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    E-DAT     3     0     3     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    I-DAT     0     0     1     1     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    S-DAT     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    B-ITM     2     0     0     0     0     3     0     0     1     0     0     0     0     0     0     0     0     0     0\n",
      "    E-ITM     3     0     0     0     0     0     3     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    I-ITM     0     0     0     0     0     0     0     1     0     0     0     0     0     0     0     0     0     0     0\n",
      "    S-ITM     7     0     0     0     0     0     2     0     8     0     0     0     0     0     0     0     0     0     0\n",
      "    B-LOC     2     0     0     0     0     0     0     0     0     7     0     0     0     0     0     0     0     0     0\n",
      "    E-LOC     2     0     0     0     0     0     0     0     0     0     7     0     0     0     0     0     0     0     0\n",
      "    I-LOC     2     0     0     0     0     0     0     0     0     0     0     1     0     0     0     0     0     0     0\n",
      "    S-LOC     3     0     0     0     0     0     0     0     0     1     0     0     3     0     0     0     0     0     0\n",
      "    B-ORG     2     0     0     0     0     0     0     0     0     1     0     0     0     6     0     0     0     0     0\n",
      "    E-ORG     3     0     0     0     0     0     0     0     0     0     0     0     0     0     6     0     0     0     0\n",
      "    I-ORG     3     0     0     0     0     0     0     0     0     0     1     0     0     0     0    12     0     0     0\n",
      "    S-ORG     4     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     1     0     0\n",
      "    S-UNT     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     9     0\n",
      "    S-WEI     1     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     8\n"
     ]
    }
   ],
   "source": [
    "# Print evaluation results\n",
    "preds = [[]]\n",
    "gold_tags = [[]]\n",
    "for i in range(len(model_output)):\n",
    "    preds[0].append(model_output.iloc[i]['Predicted'])\n",
    "    gold_tags[0].append(model_output.iloc[i]['Assigned'])\n",
    "\n",
    "_, _, score = scorer.score_by_entity(preds, gold_tags)\n",
    "_, _, _, confusion = scorer.score_by_token(preds, gold_tags)\n",
    "\n",
    "print(format_confusion(confusion))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          What\n",
      "1            an\n",
      "2       awesome\n",
      "3           day\n",
      "4          with\n",
      "5       amazing\n",
      "6        people\n",
      "7           and\n",
      "8       stellar\n",
      "9         dives\n",
      "10            ,\n",
      "11          and\n",
      "12           we\n",
      "13         left\n",
      "14          the\n",
      "15        ocean\n",
      "16       better\n",
      "17         than\n",
      "18           we\n",
      "19        found\n",
      "20           it\n",
      "21            !\n",
      "22            ðŸŒŽ\n",
      "23           In\n",
      "24        total\n",
      "25           we\n",
      "26     gathered\n",
      "27        about\n",
      "28           50\n",
      "29          lbs\n",
      "30           of\n",
      "31      debris-\n",
      "32          the\n",
      "33      biggest\n",
      "34       weight\n",
      "35       factor\n",
      "36        being\n",
      "37            a\n",
      "38       anchor\n",
      "39          and\n",
      "40        chain\n",
      "41         that\n",
      "42          was\n",
      "43         left\n",
      "44       behind\n",
      "45            ,\n",
      "46          but\n",
      "47          the\n",
      "48    remaining\n",
      "49           10\n",
      "Name: Token, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Feed all the sentences into GPT-3 and let it predict which ones are Orgs, location, etc.\n",
    "post_delimiters = []\n",
    "print(model_output['Token'][0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"api_key.txt\"\n",
    "with open(fname, \"r\") as f:\n",
    "    OPENAI_KEY = f.readline().strip()\n",
    "\n",
    "openai.api_key = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I don't understand the question.\n"
     ]
    }
   ],
   "source": [
    "def get_prompt(row):\n",
    "    prompt = f\"Please extract the organization, location, date, type of trash, and weight of trash from this post: {row['text']}\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "                engine=\"text-davinci-003\",\n",
    "                prompt= \"Who is your daddy?\",\n",
    "                max_tokens=75,\n",
    "                temperature=0.7\n",
    "            )\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
